{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from platform import python_version\n",
    "\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "from os import listdir, makedirs, getcwd, remove\n",
    "from os.path import isfile, join, abspath, exists, isdir, expanduser\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets, models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len('chili-dog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_images(path=\"./hw4/train_kaggle\"):\n",
    "    imgs = os.listdir(path)\n",
    "    X, y = [], []\n",
    "    for i in imgs:\n",
    "        X.append(i)\n",
    "#         'frankfurter', 'chili-dog' or 'hotdog'\n",
    "        y.append(1 if len(i) > 6 and \n",
    "                 (i[:6]=='hotdog' or i[:11] == 'frankfurter' or i[:9] == 'chili-dog')\n",
    "                 else 0)\n",
    "    return np.array(X), np.array(y, dtype=np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = read_images()\n",
    "# X = X[:60]\n",
    "# y = y[:60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, 0.3137084510102107)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available(), np.mean(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = models.resnet152(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in resnet.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (6): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (7): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (6): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (7): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (8): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (9): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (10): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (11): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (12): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (13): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (14): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (15): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (16): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (17): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (18): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (19): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (20): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (21): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (22): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (23): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (24): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (25): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (26): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (27): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (28): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (29): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (30): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (31): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (32): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (33): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (34): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (35): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(resnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resnet.fc.out_features = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (6): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (7): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (6): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (7): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (8): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (9): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (10): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (11): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (12): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (13): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (14): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (15): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (16): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (17): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (18): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (19): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (20): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (21): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (22): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (23): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (24): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (25): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (26): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (27): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (28): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (29): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (30): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (31): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (32): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (33): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (34): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (35): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=2048, out_features=128, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.4, inplace=False)\n",
       "    (3): Linear(in_features=128, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet.fc = nn.Sequential(\n",
    "   nn.Linear(2048, 128),\n",
    "   nn.ReLU(inplace=True),\n",
    "   nn.Dropout(0.4),\n",
    "   nn.Linear(128, 2))\n",
    "#     nn.Linear(fc_inputs, 256),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Dropout(0.4),\n",
    "#     nn.Linear(fc_inputs, 2), \n",
    "#     nn.Softmax() # For using NLLLoss()\n",
    "# )\n",
    "resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in resnet.fc.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kjdbkvj\n",
      "kjdbkvj\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nastya/.local/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Linear') != -1:\n",
    "        print('kjdbkvj')\n",
    "        nn.init.kaiming_normal(m.weight.data)\n",
    "        \n",
    "resnet.fc.apply(weights_init);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'batch_size': 24,\n",
    "    'shuffle': True,\n",
    "    'num_workers': 8\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nastya/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torchvision/transforms/transforms.py:220: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
      "  \"please use transforms.Resize instead.\")\n"
     ]
    }
   ],
   "source": [
    "ds_trans = transforms.Compose([transforms.Scale(224),\n",
    "                               transforms.CenterCrop(224),\n",
    "                               transforms.RandomHorizontalFlip(),\n",
    "                               transforms.RandomAffine(8, shear=5, scale=(0.85,1.15)),\n",
    "                               transforms.ToTensor()])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    'Characterizes a dataset for PyTorch'\n",
    "    def __init__(self, list_IDs, labels, transform, datadir='./hw4/train_kaggle'):\n",
    "        'Initialization'\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.transform = transform\n",
    "        self.datadir = datadir\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.list_IDs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        ID = self.list_IDs[index]       \n",
    "        img = Image.open(self.datadir + '/' + ID)\n",
    "        X = self.transform(img)\n",
    "        y = self.labels[index]\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(y)/y.shape[0]\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fc_inputs = resnet.fc.in_features\n",
    " \n",
    "# resnet.fc = nn.Sequential(\n",
    "# #    nn.Linear(2048, 128),\n",
    "# #    nn.ReLU(inplace=True),\n",
    "# #    nn.Dropout(0.4),\n",
    "#    nn.Linear(2048, 2))\n",
    "# #     nn.Linear(fc_inputs, 256),\n",
    "# #     nn.ReLU(),\n",
    "# #     nn.Dropout(0.4),\n",
    "# #     nn.Linear(fc_inputs, 2), \n",
    "# #     nn.Softmax() # For using NLLLoss()\n",
    "# # )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = resnet.cuda()\n",
    "criterion = nn.CrossEntropyLoss(weight=torch.tensor([0.3137, 1 - 0.3137]).cuda())\n",
    "optimizer = torch.optim.Adam(resnet.parameters(),lr=0.001, weight_decay=1e-6)\n",
    "# optimizer = torch.optim.SGD(resnet.parameters(),lr=0.001, momentum=0.9, nesterov=True, weight_decay=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/150\n",
      "Batch number: 000, Training: Loss: 0.8226, Accuracy: 0.3750\n",
      "Batch number: 001, Training: Loss: 0.6430, Accuracy: 0.6250\n",
      "Batch number: 002, Training: Loss: 1.6575, Accuracy: 0.6667\n",
      "Batch number: 003, Training: Loss: 0.8720, Accuracy: 0.6667\n",
      "Batch number: 004, Training: Loss: 0.5812, Accuracy: 0.5833\n",
      "Batch number: 005, Training: Loss: 0.3887, Accuracy: 0.8333\n",
      "Batch number: 006, Training: Loss: 0.5980, Accuracy: 0.7083\n",
      "Batch number: 007, Training: Loss: 0.2306, Accuracy: 0.8750\n",
      "Batch number: 008, Training: Loss: 0.1672, Accuracy: 0.9583\n",
      "Batch number: 009, Training: Loss: 0.3371, Accuracy: 0.8333\n",
      "Batch number: 010, Training: Loss: 0.1426, Accuracy: 0.9167\n",
      "Batch number: 011, Training: Loss: 0.7562, Accuracy: 0.7083\n",
      "Batch number: 012, Training: Loss: 0.0793, Accuracy: 0.9167\n",
      "Batch number: 013, Training: Loss: 0.2004, Accuracy: 0.9167\n",
      "Batch number: 014, Training: Loss: 0.1035, Accuracy: 0.9583\n",
      "Batch number: 015, Training: Loss: 0.2906, Accuracy: 0.8333\n",
      "Batch number: 016, Training: Loss: 0.4023, Accuracy: 0.8333\n",
      "Batch number: 017, Training: Loss: 0.1500, Accuracy: 0.9583\n",
      "Batch number: 018, Training: Loss: 0.3558, Accuracy: 0.7917\n",
      "Batch number: 019, Training: Loss: 0.4105, Accuracy: 0.8750\n",
      "Batch number: 020, Training: Loss: 0.3797, Accuracy: 0.7917\n",
      "Batch number: 021, Training: Loss: 0.2733, Accuracy: 0.9167\n",
      "Batch number: 022, Training: Loss: 0.6268, Accuracy: 0.7917\n",
      "Batch number: 023, Training: Loss: 0.2147, Accuracy: 0.9583\n",
      "Batch number: 024, Training: Loss: 0.1278, Accuracy: 0.9583\n",
      "Batch number: 025, Training: Loss: 0.1479, Accuracy: 0.9583\n",
      "Batch number: 026, Training: Loss: 0.4761, Accuracy: 0.8750\n",
      "Batch number: 027, Training: Loss: 0.2016, Accuracy: 0.9583\n",
      "Batch number: 028, Training: Loss: 0.2223, Accuracy: 0.8750\n",
      "Batch number: 029, Training: Loss: 0.4060, Accuracy: 0.7917\n",
      "Batch number: 030, Training: Loss: 0.2474, Accuracy: 0.8333\n",
      "Batch number: 031, Training: Loss: 0.8097, Accuracy: 0.7917\n",
      "Batch number: 032, Training: Loss: 0.1046, Accuracy: 1.0000\n",
      "Batch number: 033, Training: Loss: 0.1964, Accuracy: 0.9583\n",
      "Batch number: 034, Training: Loss: 0.1095, Accuracy: 0.9583\n",
      "Batch number: 035, Training: Loss: 0.5949, Accuracy: 0.8750\n",
      "Batch number: 036, Training: Loss: 0.4074, Accuracy: 0.7500\n",
      "Batch number: 037, Training: Loss: 0.1135, Accuracy: 0.9167\n",
      "Batch number: 038, Training: Loss: 0.2768, Accuracy: 0.8750\n",
      "Batch number: 039, Training: Loss: 0.0643, Accuracy: 1.0000\n",
      "Batch number: 040, Training: Loss: 0.2799, Accuracy: 0.8750\n",
      "Batch number: 041, Training: Loss: 0.2706, Accuracy: 0.9167\n",
      "Batch number: 042, Training: Loss: 0.3801, Accuracy: 0.9167\n",
      "Batch number: 043, Training: Loss: 0.3261, Accuracy: 0.9583\n",
      "Batch number: 044, Training: Loss: 0.2375, Accuracy: 0.9167\n",
      "Batch number: 045, Training: Loss: 0.2394, Accuracy: 0.8333\n",
      "Batch number: 046, Training: Loss: 0.2659, Accuracy: 0.8333\n",
      "Batch number: 047, Training: Loss: 0.3337, Accuracy: 0.7917\n",
      "Batch number: 048, Training: Loss: 0.1348, Accuracy: 0.9167\n",
      "Batch number: 049, Training: Loss: 0.3639, Accuracy: 0.7917\n",
      "Batch number: 050, Training: Loss: 0.2607, Accuracy: 0.9167\n",
      "Batch number: 051, Training: Loss: 0.0958, Accuracy: 1.0000\n",
      "Batch number: 052, Training: Loss: 0.1165, Accuracy: 0.9583\n",
      "Batch number: 053, Training: Loss: 0.3744, Accuracy: 0.9167\n",
      "Batch number: 054, Training: Loss: 1.0124, Accuracy: 0.7917\n",
      "Batch number: 055, Training: Loss: 0.1083, Accuracy: 0.9583\n",
      "Batch number: 056, Training: Loss: 0.2014, Accuracy: 0.9167\n",
      "Batch number: 057, Training: Loss: 0.3351, Accuracy: 0.7917\n",
      "Batch number: 058, Training: Loss: 0.1886, Accuracy: 0.8333\n",
      "Batch number: 059, Training: Loss: 0.0714, Accuracy: 0.9583\n",
      "Batch number: 060, Training: Loss: 0.4501, Accuracy: 0.7500\n",
      "Batch number: 061, Training: Loss: 0.2163, Accuracy: 0.8333\n",
      "Batch number: 062, Training: Loss: 0.5162, Accuracy: 0.6667\n",
      "Batch number: 063, Training: Loss: 0.3648, Accuracy: 0.8333\n",
      "Batch number: 064, Training: Loss: 0.3572, Accuracy: 0.9167\n",
      "Batch number: 065, Training: Loss: 0.1772, Accuracy: 0.9167\n",
      "Batch number: 066, Training: Loss: 0.2183, Accuracy: 0.9583\n",
      "Batch number: 067, Training: Loss: 0.2033, Accuracy: 0.9583\n",
      "Batch number: 068, Training: Loss: 0.3202, Accuracy: 0.8750\n",
      "Batch number: 069, Training: Loss: 0.1607, Accuracy: 0.9167\n",
      "Batch number: 070, Training: Loss: 0.6047, Accuracy: 0.7500\n",
      "Batch number: 071, Training: Loss: 0.1527, Accuracy: 0.8750\n",
      "Batch number: 072, Training: Loss: 0.2969, Accuracy: 0.9167\n",
      "Batch number: 073, Training: Loss: 0.5326, Accuracy: 0.7917\n",
      "Batch number: 074, Training: Loss: 0.3819, Accuracy: 0.7917\n",
      "Batch number: 075, Training: Loss: 0.1807, Accuracy: 0.8750\n",
      "Batch number: 076, Training: Loss: 0.1910, Accuracy: 0.9167\n",
      "Batch number: 077, Training: Loss: 0.3098, Accuracy: 0.8750\n",
      "Batch number: 078, Training: Loss: 0.5024, Accuracy: 0.8750\n",
      "Batch number: 079, Training: Loss: 0.1900, Accuracy: 0.9167\n",
      "Batch number: 080, Training: Loss: 0.2083, Accuracy: 0.9583\n",
      "Batch number: 081, Training: Loss: 0.0891, Accuracy: 0.9583\n",
      "Batch number: 082, Training: Loss: 0.1758, Accuracy: 0.9167\n",
      "Batch number: 083, Training: Loss: 0.0619, Accuracy: 1.0000\n",
      "Batch number: 084, Training: Loss: 0.3138, Accuracy: 0.7917\n",
      "Batch number: 085, Training: Loss: 0.0651, Accuracy: 1.0000\n",
      "Batch number: 086, Training: Loss: 0.1749, Accuracy: 0.9583\n",
      "Batch number: 087, Training: Loss: 0.2524, Accuracy: 0.9167\n",
      "Batch number: 088, Training: Loss: 0.2595, Accuracy: 0.9583\n",
      "Batch number: 089, Training: Loss: 0.2239, Accuracy: 0.9167\n",
      "Batch number: 090, Training: Loss: 0.1711, Accuracy: 0.9583\n",
      "Batch number: 091, Training: Loss: 0.1557, Accuracy: 0.9167\n",
      "Batch number: 092, Training: Loss: 0.1336, Accuracy: 0.9583\n",
      "Batch number: 093, Training: Loss: 0.2565, Accuracy: 0.9167\n",
      "Batch number: 094, Training: Loss: 0.4177, Accuracy: 0.8333\n",
      "Batch number: 095, Training: Loss: 0.5559, Accuracy: 0.8333\n",
      "Batch number: 096, Training: Loss: 0.1888, Accuracy: 0.8333\n",
      "Batch number: 097, Training: Loss: 0.1449, Accuracy: 0.9583\n",
      "Batch number: 098, Training: Loss: 0.3786, Accuracy: 0.7500\n",
      "Batch number: 099, Training: Loss: 0.1538, Accuracy: 0.9167\n",
      "Batch number: 100, Training: Loss: 0.3602, Accuracy: 0.7917\n",
      "Batch number: 101, Training: Loss: 0.1664, Accuracy: 0.9167\n",
      "Batch number: 102, Training: Loss: 0.1386, Accuracy: 0.9167\n",
      "Batch number: 103, Training: Loss: 0.2376, Accuracy: 0.9583\n",
      "Batch number: 104, Training: Loss: 0.2210, Accuracy: 0.9583\n",
      "Batch number: 105, Training: Loss: 0.5613, Accuracy: 0.7917\n",
      "Batch number: 106, Training: Loss: 0.3314, Accuracy: 0.8750\n",
      "Batch number: 107, Training: Loss: 0.1370, Accuracy: 0.9583\n",
      "Batch number: 108, Training: Loss: 0.5155, Accuracy: 0.8333\n",
      "Batch number: 109, Training: Loss: 0.3568, Accuracy: 0.8333\n",
      "Batch number: 110, Training: Loss: 0.3095, Accuracy: 0.8333\n",
      "Batch number: 111, Training: Loss: 0.2601, Accuracy: 0.8333\n",
      "Batch number: 112, Training: Loss: 0.4361, Accuracy: 0.8750\n",
      "Batch number: 113, Training: Loss: 0.1760, Accuracy: 0.8750\n",
      "Batch number: 114, Training: Loss: 0.5802, Accuracy: 0.6667\n",
      "Batch number: 115, Training: Loss: 0.1962, Accuracy: 0.9583\n",
      "Batch number: 116, Training: Loss: 0.1030, Accuracy: 0.9583\n",
      "Batch number: 117, Training: Loss: 0.5610, Accuracy: 0.8750\n",
      "Batch number: 118, Training: Loss: 0.3175, Accuracy: 0.9167\n",
      "Batch number: 119, Training: Loss: 0.0638, Accuracy: 1.0000\n",
      "Batch number: 120, Training: Loss: 0.2252, Accuracy: 0.9167\n",
      "Batch number: 121, Training: Loss: 0.1934, Accuracy: 0.8750\n",
      "Batch number: 122, Training: Loss: 0.1378, Accuracy: 0.9583\n",
      "Batch number: 123, Training: Loss: 0.0537, Accuracy: 1.0000\n",
      "Batch number: 124, Training: Loss: 0.2581, Accuracy: 0.9167\n",
      "Batch number: 125, Training: Loss: 0.2938, Accuracy: 0.9167\n",
      "Batch number: 126, Training: Loss: 0.2545, Accuracy: 0.9167\n",
      "Batch number: 127, Training: Loss: 0.1435, Accuracy: 0.9583\n",
      "Batch number: 128, Training: Loss: 0.2627, Accuracy: 0.8750\n",
      "Batch number: 129, Training: Loss: 0.1087, Accuracy: 1.0000\n",
      "Batch number: 130, Training: Loss: 0.1916, Accuracy: 0.9167\n",
      "Batch number: 131, Training: Loss: 0.0633, Accuracy: 0.9583\n",
      "Batch number: 132, Training: Loss: 0.1630, Accuracy: 1.0000\n",
      "Batch number: 133, Training: Loss: 0.1261, Accuracy: 1.0000\n",
      "Batch number: 134, Training: Loss: 0.4178, Accuracy: 0.8333\n",
      "Batch number: 135, Training: Loss: 0.1173, Accuracy: 0.8750\n",
      "Batch number: 136, Training: Loss: 0.4061, Accuracy: 0.7917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number: 137, Training: Loss: 0.2470, Accuracy: 0.8750\n",
      "Batch number: 138, Training: Loss: 0.0760, Accuracy: 1.0000\n",
      "Batch number: 139, Training: Loss: 0.1654, Accuracy: 0.8750\n",
      "Batch number: 140, Training: Loss: 0.6106, Accuracy: 0.8333\n",
      "Batch number: 141, Training: Loss: 0.1976, Accuracy: 0.9583\n",
      "Batch number: 142, Training: Loss: 0.1157, Accuracy: 0.9583\n",
      "Batch number: 143, Training: Loss: 0.1603, Accuracy: 0.9583\n",
      "Batch number: 144, Training: Loss: 0.0339, Accuracy: 1.0000\n",
      "Batch number: 145, Training: Loss: 0.3467, Accuracy: 0.8750\n",
      "Batch number: 146, Training: Loss: 0.1524, Accuracy: 0.9167\n",
      "Batch number: 147, Training: Loss: 0.5737, Accuracy: 0.7500\n",
      "Batch number: 148, Training: Loss: 0.1032, Accuracy: 1.0000\n",
      "Batch number: 149, Training: Loss: 0.4635, Accuracy: 0.7083\n",
      "Batch number: 150, Training: Loss: 0.1396, Accuracy: 0.9583\n",
      "Batch number: 151, Training: Loss: 0.1307, Accuracy: 0.9583\n",
      "Batch number: 152, Training: Loss: 0.2681, Accuracy: 0.8750\n",
      "Batch number: 153, Training: Loss: 0.2441, Accuracy: 0.9583\n",
      "Batch number: 154, Training: Loss: 0.2141, Accuracy: 0.9167\n",
      "Batch number: 155, Training: Loss: 0.2749, Accuracy: 0.9583\n",
      "Batch number: 156, Training: Loss: 0.1952, Accuracy: 0.9167\n",
      "Batch number: 157, Training: Loss: 0.0894, Accuracy: 0.9583\n",
      "Batch number: 158, Training: Loss: 0.3982, Accuracy: 0.8333\n",
      "Batch number: 159, Training: Loss: 0.1586, Accuracy: 0.9583\n",
      "Batch number: 160, Training: Loss: 0.1801, Accuracy: 0.9167\n",
      "Batch number: 161, Training: Loss: 0.1520, Accuracy: 0.9167\n",
      "Batch number: 162, Training: Loss: 0.3072, Accuracy: 0.8750\n",
      "Batch number: 163, Training: Loss: 0.2755, Accuracy: 0.8750\n",
      "Batch number: 164, Training: Loss: 0.1699, Accuracy: 0.9167\n",
      "Batch number: 165, Training: Loss: 0.1635, Accuracy: 0.8333\n",
      "Batch number: 166, Training: Loss: 0.1280, Accuracy: 0.9583\n",
      "Batch number: 167, Training: Loss: 0.3298, Accuracy: 0.8333\n",
      "Batch number: 168, Training: Loss: 0.1584, Accuracy: 0.8750\n",
      "Batch number: 169, Training: Loss: 0.2899, Accuracy: 0.9167\n",
      "Batch number: 170, Training: Loss: 0.3480, Accuracy: 0.8333\n",
      "Batch number: 171, Training: Loss: 0.0886, Accuracy: 1.0000\n",
      "Batch number: 172, Training: Loss: 0.1889, Accuracy: 0.9167\n",
      "Batch number: 173, Training: Loss: 0.1417, Accuracy: 1.0000\n",
      "Batch number: 174, Training: Loss: 0.1248, Accuracy: 0.9583\n",
      "Batch number: 175, Training: Loss: 0.1002, Accuracy: 0.9583\n",
      "Batch number: 176, Training: Loss: 0.1145, Accuracy: 0.9583\n",
      "Batch number: 177, Training: Loss: 0.3738, Accuracy: 0.8750\n",
      "Batch number: 178, Training: Loss: 0.2013, Accuracy: 0.9583\n",
      "Batch number: 179, Training: Loss: 0.1550, Accuracy: 0.9167\n",
      "Batch number: 180, Training: Loss: 0.0971, Accuracy: 0.9583\n",
      "Batch number: 181, Training: Loss: 0.2760, Accuracy: 0.9167\n",
      "Batch number: 182, Training: Loss: 0.1878, Accuracy: 0.8750\n",
      "Batch number: 183, Training: Loss: 0.2384, Accuracy: 0.8750\n",
      "Batch number: 184, Training: Loss: 0.2309, Accuracy: 0.9167\n",
      "Batch number: 185, Training: Loss: 0.2062, Accuracy: 0.8750\n",
      "Batch number: 186, Training: Loss: 0.1908, Accuracy: 0.9583\n",
      "Batch number: 187, Training: Loss: 0.2602, Accuracy: 0.8750\n",
      "Batch number: 188, Training: Loss: 0.0664, Accuracy: 1.0000\n",
      "Batch number: 189, Training: Loss: 0.3396, Accuracy: 0.8750\n",
      "Batch number: 190, Training: Loss: 0.1625, Accuracy: 0.9167\n",
      "Batch number: 191, Training: Loss: 0.1897, Accuracy: 0.9474\n",
      "Epoch: 2/150\n",
      "Batch number: 000, Training: Loss: 0.2145, Accuracy: 0.9583\n",
      "Batch number: 001, Training: Loss: 0.1370, Accuracy: 0.9583\n",
      "Batch number: 002, Training: Loss: 0.2492, Accuracy: 0.9167\n",
      "Batch number: 003, Training: Loss: 0.1156, Accuracy: 0.9583\n",
      "Batch number: 004, Training: Loss: 0.0836, Accuracy: 1.0000\n",
      "Batch number: 005, Training: Loss: 0.2954, Accuracy: 0.9167\n",
      "Batch number: 006, Training: Loss: 0.1723, Accuracy: 0.8750\n",
      "Batch number: 007, Training: Loss: 0.1208, Accuracy: 0.9583\n",
      "Batch number: 008, Training: Loss: 0.1934, Accuracy: 0.9167\n",
      "Batch number: 009, Training: Loss: 0.0623, Accuracy: 1.0000\n",
      "Batch number: 010, Training: Loss: 0.0926, Accuracy: 0.9583\n",
      "Batch number: 011, Training: Loss: 0.1974, Accuracy: 0.9167\n",
      "Batch number: 012, Training: Loss: 0.1237, Accuracy: 1.0000\n",
      "Batch number: 013, Training: Loss: 0.1240, Accuracy: 1.0000\n",
      "Batch number: 014, Training: Loss: 0.1599, Accuracy: 0.9583\n",
      "Batch number: 015, Training: Loss: 0.1887, Accuracy: 0.9167\n",
      "Batch number: 016, Training: Loss: 0.1196, Accuracy: 0.9583\n",
      "Batch number: 017, Training: Loss: 0.2179, Accuracy: 0.9167\n",
      "Batch number: 018, Training: Loss: 0.2561, Accuracy: 0.8750\n",
      "Batch number: 019, Training: Loss: 0.3229, Accuracy: 0.8750\n",
      "Batch number: 020, Training: Loss: 0.1214, Accuracy: 0.9583\n",
      "Batch number: 021, Training: Loss: 0.2153, Accuracy: 0.9583\n",
      "Batch number: 022, Training: Loss: 0.1699, Accuracy: 0.9167\n",
      "Batch number: 023, Training: Loss: 0.1870, Accuracy: 0.8750\n",
      "Batch number: 024, Training: Loss: 0.2716, Accuracy: 0.8750\n",
      "Batch number: 025, Training: Loss: 0.2980, Accuracy: 0.8333\n",
      "Batch number: 026, Training: Loss: 0.0810, Accuracy: 1.0000\n",
      "Batch number: 027, Training: Loss: 0.1578, Accuracy: 0.9167\n",
      "Batch number: 028, Training: Loss: 0.2308, Accuracy: 0.9167\n",
      "Batch number: 029, Training: Loss: 0.2350, Accuracy: 0.8750\n",
      "Batch number: 030, Training: Loss: 0.1888, Accuracy: 0.9167\n",
      "Batch number: 031, Training: Loss: 0.1369, Accuracy: 0.8750\n",
      "Batch number: 032, Training: Loss: 0.2062, Accuracy: 0.8750\n",
      "Batch number: 033, Training: Loss: 0.3156, Accuracy: 0.8750\n",
      "Batch number: 034, Training: Loss: 0.1394, Accuracy: 0.9583\n",
      "Batch number: 035, Training: Loss: 0.3937, Accuracy: 0.7917\n",
      "Batch number: 036, Training: Loss: 0.1273, Accuracy: 0.9583\n",
      "Batch number: 037, Training: Loss: 0.1911, Accuracy: 0.9583\n",
      "Batch number: 038, Training: Loss: 0.1835, Accuracy: 0.9167\n",
      "Batch number: 039, Training: Loss: 0.4497, Accuracy: 0.7500\n",
      "Batch number: 040, Training: Loss: 0.3040, Accuracy: 0.8333\n",
      "Batch number: 041, Training: Loss: 0.0612, Accuracy: 1.0000\n",
      "Batch number: 042, Training: Loss: 0.1922, Accuracy: 0.9583\n",
      "Batch number: 043, Training: Loss: 0.6042, Accuracy: 0.8333\n",
      "Batch number: 044, Training: Loss: 0.1898, Accuracy: 0.9583\n",
      "Batch number: 045, Training: Loss: 0.2992, Accuracy: 0.8750\n",
      "Batch number: 046, Training: Loss: 0.1396, Accuracy: 0.9583\n",
      "Batch number: 047, Training: Loss: 0.0641, Accuracy: 1.0000\n",
      "Batch number: 048, Training: Loss: 0.0984, Accuracy: 0.9167\n",
      "Batch number: 049, Training: Loss: 0.0649, Accuracy: 1.0000\n",
      "Batch number: 050, Training: Loss: 0.2650, Accuracy: 0.8750\n",
      "Batch number: 051, Training: Loss: 0.0772, Accuracy: 1.0000\n",
      "Batch number: 052, Training: Loss: 0.2435, Accuracy: 0.7917\n",
      "Batch number: 053, Training: Loss: 0.5717, Accuracy: 0.6667\n",
      "Batch number: 054, Training: Loss: 0.3768, Accuracy: 0.7917\n",
      "Batch number: 055, Training: Loss: 0.1049, Accuracy: 1.0000\n",
      "Batch number: 056, Training: Loss: 0.6937, Accuracy: 0.7917\n",
      "Batch number: 057, Training: Loss: 0.3809, Accuracy: 0.9167\n",
      "Batch number: 058, Training: Loss: 0.2901, Accuracy: 0.8750\n",
      "Batch number: 059, Training: Loss: 0.8082, Accuracy: 0.7917\n",
      "Batch number: 060, Training: Loss: 0.1226, Accuracy: 0.9583\n",
      "Batch number: 061, Training: Loss: 0.2435, Accuracy: 0.9167\n",
      "Batch number: 062, Training: Loss: 0.2743, Accuracy: 0.8333\n",
      "Batch number: 063, Training: Loss: 0.0929, Accuracy: 1.0000\n",
      "Batch number: 064, Training: Loss: 0.1386, Accuracy: 0.9583\n",
      "Batch number: 065, Training: Loss: 0.5539, Accuracy: 0.8333\n",
      "Batch number: 066, Training: Loss: 0.2401, Accuracy: 0.8750\n",
      "Batch number: 067, Training: Loss: 0.2604, Accuracy: 0.8333\n",
      "Batch number: 068, Training: Loss: 0.1976, Accuracy: 0.8333\n",
      "Batch number: 069, Training: Loss: 0.1616, Accuracy: 0.9167\n",
      "Batch number: 070, Training: Loss: 0.2056, Accuracy: 0.9583\n",
      "Batch number: 071, Training: Loss: 0.1840, Accuracy: 0.9583\n",
      "Batch number: 072, Training: Loss: 0.3386, Accuracy: 0.8333\n",
      "Batch number: 073, Training: Loss: 0.0518, Accuracy: 1.0000\n",
      "Batch number: 074, Training: Loss: 0.2996, Accuracy: 0.8750\n",
      "Batch number: 075, Training: Loss: 0.1067, Accuracy: 0.9583\n",
      "Batch number: 076, Training: Loss: 0.1834, Accuracy: 0.9583\n",
      "Batch number: 077, Training: Loss: 0.2278, Accuracy: 0.9167\n",
      "Batch number: 078, Training: Loss: 0.3173, Accuracy: 0.9583\n",
      "Batch number: 079, Training: Loss: 0.0656, Accuracy: 1.0000\n",
      "Batch number: 080, Training: Loss: 0.0707, Accuracy: 1.0000\n",
      "Batch number: 081, Training: Loss: 0.1686, Accuracy: 0.8333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number: 082, Training: Loss: 0.1309, Accuracy: 0.8750\n",
      "Batch number: 083, Training: Loss: 0.1201, Accuracy: 0.9167\n",
      "Batch number: 084, Training: Loss: 0.1510, Accuracy: 0.9167\n",
      "Batch number: 085, Training: Loss: 0.1219, Accuracy: 0.9583\n",
      "Batch number: 086, Training: Loss: 0.0982, Accuracy: 0.9583\n",
      "Batch number: 087, Training: Loss: 0.2859, Accuracy: 0.9167\n",
      "Batch number: 088, Training: Loss: 0.1206, Accuracy: 0.9583\n",
      "Batch number: 089, Training: Loss: 0.2261, Accuracy: 0.8750\n",
      "Batch number: 090, Training: Loss: 0.1374, Accuracy: 0.9167\n",
      "Batch number: 091, Training: Loss: 0.2164, Accuracy: 0.8750\n",
      "Batch number: 092, Training: Loss: 0.1732, Accuracy: 0.9167\n",
      "Batch number: 093, Training: Loss: 0.0937, Accuracy: 0.9583\n",
      "Batch number: 094, Training: Loss: 0.1335, Accuracy: 0.9167\n",
      "Batch number: 095, Training: Loss: 0.2881, Accuracy: 0.9167\n",
      "Batch number: 096, Training: Loss: 0.0428, Accuracy: 1.0000\n",
      "Batch number: 097, Training: Loss: 0.1233, Accuracy: 0.9583\n",
      "Batch number: 098, Training: Loss: 0.0781, Accuracy: 1.0000\n",
      "Batch number: 099, Training: Loss: 0.1559, Accuracy: 0.9167\n",
      "Batch number: 100, Training: Loss: 0.0784, Accuracy: 0.9583\n",
      "Batch number: 101, Training: Loss: 0.0360, Accuracy: 1.0000\n",
      "Batch number: 102, Training: Loss: 0.0867, Accuracy: 0.9583\n",
      "Batch number: 103, Training: Loss: 0.0131, Accuracy: 1.0000\n",
      "Batch number: 104, Training: Loss: 0.2665, Accuracy: 0.9583\n",
      "Batch number: 105, Training: Loss: 0.5019, Accuracy: 0.9167\n",
      "Batch number: 106, Training: Loss: 0.1016, Accuracy: 0.9583\n",
      "Batch number: 107, Training: Loss: 0.0768, Accuracy: 0.9583\n",
      "Batch number: 108, Training: Loss: 0.0842, Accuracy: 0.9583\n",
      "Batch number: 109, Training: Loss: 0.2620, Accuracy: 0.8333\n",
      "Batch number: 110, Training: Loss: 0.1945, Accuracy: 0.9167\n",
      "Batch number: 111, Training: Loss: 0.7127, Accuracy: 0.7083\n",
      "Batch number: 112, Training: Loss: 0.1734, Accuracy: 0.9583\n",
      "Batch number: 113, Training: Loss: 0.0985, Accuracy: 0.9583\n",
      "Batch number: 114, Training: Loss: 0.1873, Accuracy: 0.8750\n",
      "Batch number: 115, Training: Loss: 0.2809, Accuracy: 0.8333\n",
      "Batch number: 116, Training: Loss: 0.1290, Accuracy: 0.9583\n",
      "Batch number: 117, Training: Loss: 0.0964, Accuracy: 0.9583\n",
      "Batch number: 118, Training: Loss: 0.3161, Accuracy: 0.8333\n",
      "Batch number: 119, Training: Loss: 0.3958, Accuracy: 0.8750\n",
      "Batch number: 120, Training: Loss: 0.3556, Accuracy: 0.9583\n",
      "Batch number: 121, Training: Loss: 0.0439, Accuracy: 1.0000\n",
      "Batch number: 122, Training: Loss: 0.4786, Accuracy: 0.8750\n",
      "Batch number: 123, Training: Loss: 0.0370, Accuracy: 1.0000\n",
      "Batch number: 124, Training: Loss: 0.2119, Accuracy: 0.9167\n",
      "Batch number: 125, Training: Loss: 0.1558, Accuracy: 0.9583\n",
      "Batch number: 126, Training: Loss: 0.3698, Accuracy: 0.8333\n",
      "Batch number: 127, Training: Loss: 0.1351, Accuracy: 1.0000\n",
      "Batch number: 128, Training: Loss: 0.0969, Accuracy: 0.9583\n",
      "Batch number: 129, Training: Loss: 0.0792, Accuracy: 0.9583\n",
      "Batch number: 130, Training: Loss: 0.9621, Accuracy: 0.7917\n",
      "Batch number: 131, Training: Loss: 0.1044, Accuracy: 1.0000\n",
      "Batch number: 132, Training: Loss: 0.0897, Accuracy: 0.9583\n",
      "Batch number: 133, Training: Loss: 0.2195, Accuracy: 0.8750\n",
      "Batch number: 134, Training: Loss: 0.2644, Accuracy: 0.8750\n",
      "Batch number: 135, Training: Loss: 0.0802, Accuracy: 1.0000\n",
      "Batch number: 136, Training: Loss: 0.6191, Accuracy: 0.7500\n",
      "Batch number: 137, Training: Loss: 0.3380, Accuracy: 0.9167\n",
      "Batch number: 138, Training: Loss: 0.2207, Accuracy: 0.8750\n",
      "Batch number: 139, Training: Loss: 0.1655, Accuracy: 0.9167\n",
      "Batch number: 140, Training: Loss: 0.1451, Accuracy: 0.9583\n",
      "Batch number: 141, Training: Loss: 0.1958, Accuracy: 0.9583\n",
      "Batch number: 142, Training: Loss: 0.1478, Accuracy: 0.9167\n",
      "Batch number: 143, Training: Loss: 0.0985, Accuracy: 0.9583\n",
      "Batch number: 144, Training: Loss: 0.1245, Accuracy: 0.9583\n",
      "Batch number: 145, Training: Loss: 0.0857, Accuracy: 1.0000\n",
      "Batch number: 146, Training: Loss: 0.0735, Accuracy: 1.0000\n",
      "Batch number: 147, Training: Loss: 0.0578, Accuracy: 1.0000\n",
      "Batch number: 148, Training: Loss: 0.2134, Accuracy: 0.9583\n",
      "Batch number: 149, Training: Loss: 0.3371, Accuracy: 0.9167\n",
      "Batch number: 150, Training: Loss: 0.2703, Accuracy: 0.8750\n",
      "Batch number: 151, Training: Loss: 0.3192, Accuracy: 0.9583\n",
      "Batch number: 152, Training: Loss: 0.1922, Accuracy: 0.8750\n",
      "Batch number: 153, Training: Loss: 0.2553, Accuracy: 0.8750\n",
      "Batch number: 154, Training: Loss: 0.0591, Accuracy: 1.0000\n",
      "Batch number: 155, Training: Loss: 0.2542, Accuracy: 0.9167\n",
      "Batch number: 156, Training: Loss: 0.1020, Accuracy: 0.9583\n",
      "Batch number: 157, Training: Loss: 0.2837, Accuracy: 0.7500\n",
      "Batch number: 158, Training: Loss: 0.1993, Accuracy: 0.8750\n",
      "Batch number: 159, Training: Loss: 0.2913, Accuracy: 0.8333\n",
      "Batch number: 160, Training: Loss: 0.3207, Accuracy: 0.8750\n",
      "Batch number: 161, Training: Loss: 0.1747, Accuracy: 0.9583\n",
      "Batch number: 162, Training: Loss: 0.2761, Accuracy: 0.9583\n",
      "Batch number: 163, Training: Loss: 0.0668, Accuracy: 0.9583\n",
      "Batch number: 164, Training: Loss: 0.0988, Accuracy: 0.9583\n",
      "Batch number: 165, Training: Loss: 0.0575, Accuracy: 1.0000\n",
      "Batch number: 166, Training: Loss: 0.1184, Accuracy: 1.0000\n",
      "Batch number: 167, Training: Loss: 0.3864, Accuracy: 0.8333\n",
      "Batch number: 168, Training: Loss: 0.2478, Accuracy: 0.9167\n",
      "Batch number: 169, Training: Loss: 0.2729, Accuracy: 0.8333\n",
      "Batch number: 170, Training: Loss: 0.0771, Accuracy: 0.9583\n",
      "Batch number: 171, Training: Loss: 0.2203, Accuracy: 0.9167\n",
      "Batch number: 172, Training: Loss: 0.0552, Accuracy: 1.0000\n",
      "Batch number: 173, Training: Loss: 0.1697, Accuracy: 0.9167\n",
      "Batch number: 174, Training: Loss: 0.1690, Accuracy: 0.9167\n",
      "Batch number: 175, Training: Loss: 0.2851, Accuracy: 0.8333\n",
      "Batch number: 176, Training: Loss: 0.1229, Accuracy: 0.9167\n",
      "Batch number: 177, Training: Loss: 0.1661, Accuracy: 0.9583\n",
      "Batch number: 178, Training: Loss: 0.1128, Accuracy: 0.9583\n",
      "Batch number: 179, Training: Loss: 0.1425, Accuracy: 0.9167\n",
      "Batch number: 180, Training: Loss: 0.1400, Accuracy: 0.9167\n",
      "Batch number: 181, Training: Loss: 0.0829, Accuracy: 1.0000\n",
      "Batch number: 182, Training: Loss: 0.3247, Accuracy: 0.8333\n",
      "Batch number: 183, Training: Loss: 0.2172, Accuracy: 0.9167\n",
      "Batch number: 184, Training: Loss: 0.0968, Accuracy: 0.9583\n",
      "Batch number: 185, Training: Loss: 0.2111, Accuracy: 0.8750\n",
      "Batch number: 186, Training: Loss: 0.0699, Accuracy: 1.0000\n",
      "Batch number: 187, Training: Loss: 0.0965, Accuracy: 0.9583\n",
      "Batch number: 188, Training: Loss: 0.1557, Accuracy: 0.9583\n",
      "Batch number: 189, Training: Loss: 0.1337, Accuracy: 0.9167\n",
      "Batch number: 190, Training: Loss: 0.0778, Accuracy: 1.0000\n",
      "Batch number: 191, Training: Loss: 0.0751, Accuracy: 0.9474\n",
      "Epoch: 3/150\n",
      "Batch number: 000, Training: Loss: 0.3842, Accuracy: 0.9167\n",
      "Batch number: 001, Training: Loss: 0.1344, Accuracy: 0.9583\n",
      "Batch number: 002, Training: Loss: 0.1292, Accuracy: 0.9583\n",
      "Batch number: 003, Training: Loss: 0.2172, Accuracy: 0.9167\n",
      "Batch number: 004, Training: Loss: 0.1568, Accuracy: 0.9583\n",
      "Batch number: 005, Training: Loss: 0.1983, Accuracy: 0.8333\n",
      "Batch number: 006, Training: Loss: 0.0454, Accuracy: 0.9583\n",
      "Batch number: 007, Training: Loss: 0.1033, Accuracy: 0.9583\n",
      "Batch number: 008, Training: Loss: 0.1081, Accuracy: 0.9583\n",
      "Batch number: 009, Training: Loss: 0.1657, Accuracy: 0.9167\n",
      "Batch number: 010, Training: Loss: 0.1046, Accuracy: 0.9583\n",
      "Batch number: 011, Training: Loss: 0.0997, Accuracy: 1.0000\n",
      "Batch number: 012, Training: Loss: 0.1341, Accuracy: 0.9583\n",
      "Batch number: 013, Training: Loss: 0.0979, Accuracy: 0.9583\n",
      "Batch number: 014, Training: Loss: 0.1404, Accuracy: 0.9167\n",
      "Batch number: 015, Training: Loss: 0.3039, Accuracy: 0.9167\n",
      "Batch number: 016, Training: Loss: 0.0680, Accuracy: 1.0000\n",
      "Batch number: 017, Training: Loss: 0.2901, Accuracy: 0.8333\n",
      "Batch number: 018, Training: Loss: 0.2352, Accuracy: 0.8750\n",
      "Batch number: 019, Training: Loss: 0.1208, Accuracy: 0.9167\n",
      "Batch number: 020, Training: Loss: 0.1407, Accuracy: 0.9167\n",
      "Batch number: 021, Training: Loss: 0.1316, Accuracy: 0.9167\n",
      "Batch number: 022, Training: Loss: 0.0857, Accuracy: 0.9583\n",
      "Batch number: 023, Training: Loss: 0.3332, Accuracy: 0.8750\n",
      "Batch number: 024, Training: Loss: 0.1856, Accuracy: 0.8750\n",
      "Batch number: 025, Training: Loss: 0.0497, Accuracy: 1.0000\n",
      "Batch number: 026, Training: Loss: 0.0313, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number: 027, Training: Loss: 0.0610, Accuracy: 1.0000\n",
      "Batch number: 028, Training: Loss: 0.1578, Accuracy: 0.9167\n",
      "Batch number: 029, Training: Loss: 0.1187, Accuracy: 0.9167\n",
      "Batch number: 030, Training: Loss: 0.1841, Accuracy: 0.9583\n",
      "Batch number: 031, Training: Loss: 0.1286, Accuracy: 1.0000\n",
      "Batch number: 032, Training: Loss: 0.6899, Accuracy: 0.7500\n",
      "Batch number: 033, Training: Loss: 0.2284, Accuracy: 0.9583\n",
      "Batch number: 034, Training: Loss: 0.1038, Accuracy: 0.9583\n",
      "Batch number: 035, Training: Loss: 0.0885, Accuracy: 0.9583\n",
      "Batch number: 036, Training: Loss: 0.4162, Accuracy: 0.8333\n",
      "Batch number: 037, Training: Loss: 0.1079, Accuracy: 0.9167\n",
      "Batch number: 038, Training: Loss: 0.1662, Accuracy: 0.8750\n",
      "Batch number: 039, Training: Loss: 0.5673, Accuracy: 0.6667\n",
      "Batch number: 040, Training: Loss: 0.0991, Accuracy: 0.9583\n",
      "Batch number: 041, Training: Loss: 0.0810, Accuracy: 0.9583\n",
      "Batch number: 042, Training: Loss: 0.2164, Accuracy: 0.8750\n",
      "Batch number: 043, Training: Loss: 0.0447, Accuracy: 1.0000\n",
      "Batch number: 044, Training: Loss: 0.0551, Accuracy: 1.0000\n",
      "Batch number: 045, Training: Loss: 0.0489, Accuracy: 1.0000\n",
      "Batch number: 046, Training: Loss: 0.1044, Accuracy: 0.9583\n",
      "Batch number: 047, Training: Loss: 0.1405, Accuracy: 0.9583\n",
      "Batch number: 048, Training: Loss: 0.6072, Accuracy: 0.8750\n",
      "Batch number: 049, Training: Loss: 0.3118, Accuracy: 0.8750\n",
      "Batch number: 050, Training: Loss: 0.2441, Accuracy: 0.9583\n",
      "Batch number: 051, Training: Loss: 0.2376, Accuracy: 0.9583\n",
      "Batch number: 052, Training: Loss: 0.2464, Accuracy: 0.9167\n",
      "Batch number: 053, Training: Loss: 0.1363, Accuracy: 0.9583\n",
      "Batch number: 054, Training: Loss: 0.1571, Accuracy: 1.0000\n",
      "Batch number: 055, Training: Loss: 0.1175, Accuracy: 1.0000\n",
      "Batch number: 056, Training: Loss: 0.1470, Accuracy: 0.9167\n",
      "Batch number: 057, Training: Loss: 0.1841, Accuracy: 0.8750\n",
      "Batch number: 058, Training: Loss: 0.3386, Accuracy: 0.8333\n",
      "Batch number: 059, Training: Loss: 0.1911, Accuracy: 0.9167\n",
      "Batch number: 060, Training: Loss: 0.2803, Accuracy: 0.8333\n",
      "Batch number: 061, Training: Loss: 0.1681, Accuracy: 0.9583\n",
      "Batch number: 062, Training: Loss: 0.1825, Accuracy: 0.8750\n",
      "Batch number: 063, Training: Loss: 0.0686, Accuracy: 1.0000\n",
      "Batch number: 064, Training: Loss: 0.5840, Accuracy: 0.7917\n",
      "Batch number: 065, Training: Loss: 0.1364, Accuracy: 0.9167\n",
      "Batch number: 066, Training: Loss: 0.2373, Accuracy: 0.8750\n",
      "Batch number: 067, Training: Loss: 0.1579, Accuracy: 0.9167\n",
      "Batch number: 068, Training: Loss: 0.2852, Accuracy: 0.8750\n",
      "Batch number: 069, Training: Loss: 0.2747, Accuracy: 0.8333\n",
      "Batch number: 070, Training: Loss: 0.0595, Accuracy: 1.0000\n",
      "Batch number: 071, Training: Loss: 0.1830, Accuracy: 0.9167\n",
      "Batch number: 072, Training: Loss: 0.2077, Accuracy: 0.9167\n",
      "Batch number: 073, Training: Loss: 0.0954, Accuracy: 0.9583\n",
      "Batch number: 074, Training: Loss: 0.1678, Accuracy: 0.9167\n",
      "Batch number: 075, Training: Loss: 0.2592, Accuracy: 0.9167\n",
      "Batch number: 076, Training: Loss: 0.2618, Accuracy: 0.9167\n",
      "Batch number: 077, Training: Loss: 0.2133, Accuracy: 0.9167\n",
      "Batch number: 078, Training: Loss: 0.0955, Accuracy: 1.0000\n",
      "Batch number: 079, Training: Loss: 0.0908, Accuracy: 0.9583\n",
      "Batch number: 080, Training: Loss: 0.1157, Accuracy: 0.9583\n",
      "Batch number: 081, Training: Loss: 0.1225, Accuracy: 0.9583\n",
      "Batch number: 082, Training: Loss: 0.3580, Accuracy: 0.8333\n",
      "Batch number: 083, Training: Loss: 0.1064, Accuracy: 0.9583\n",
      "Batch number: 084, Training: Loss: 0.2021, Accuracy: 0.8750\n",
      "Batch number: 085, Training: Loss: 0.0748, Accuracy: 1.0000\n",
      "Batch number: 086, Training: Loss: 0.0670, Accuracy: 0.9583\n",
      "Batch number: 087, Training: Loss: 0.3517, Accuracy: 0.7917\n",
      "Batch number: 088, Training: Loss: 0.2407, Accuracy: 0.8750\n",
      "Batch number: 089, Training: Loss: 0.0945, Accuracy: 1.0000\n",
      "Batch number: 090, Training: Loss: 0.1801, Accuracy: 0.9167\n",
      "Batch number: 091, Training: Loss: 0.0674, Accuracy: 1.0000\n",
      "Batch number: 092, Training: Loss: 0.0569, Accuracy: 1.0000\n",
      "Batch number: 093, Training: Loss: 0.2606, Accuracy: 0.9167\n",
      "Batch number: 094, Training: Loss: 0.1908, Accuracy: 0.9167\n",
      "Batch number: 095, Training: Loss: 0.5291, Accuracy: 0.8333\n",
      "Batch number: 096, Training: Loss: 0.0313, Accuracy: 1.0000\n",
      "Batch number: 097, Training: Loss: 0.2406, Accuracy: 0.8750\n",
      "Batch number: 098, Training: Loss: 0.1367, Accuracy: 0.9583\n",
      "Batch number: 099, Training: Loss: 0.1561, Accuracy: 0.9167\n",
      "Batch number: 100, Training: Loss: 0.3151, Accuracy: 0.8333\n",
      "Batch number: 101, Training: Loss: 0.0865, Accuracy: 0.9583\n",
      "Batch number: 102, Training: Loss: 0.1265, Accuracy: 0.9583\n",
      "Batch number: 103, Training: Loss: 0.1906, Accuracy: 0.9167\n",
      "Batch number: 104, Training: Loss: 0.0753, Accuracy: 0.9167\n",
      "Batch number: 105, Training: Loss: 0.1658, Accuracy: 0.8750\n",
      "Batch number: 106, Training: Loss: 0.4071, Accuracy: 0.9167\n",
      "Batch number: 107, Training: Loss: 0.0611, Accuracy: 0.9583\n",
      "Batch number: 108, Training: Loss: 0.1672, Accuracy: 0.9583\n",
      "Batch number: 109, Training: Loss: 0.0583, Accuracy: 1.0000\n",
      "Batch number: 110, Training: Loss: 0.1134, Accuracy: 1.0000\n",
      "Batch number: 111, Training: Loss: 0.2415, Accuracy: 0.9583\n",
      "Batch number: 112, Training: Loss: 0.2440, Accuracy: 0.9167\n",
      "Batch number: 113, Training: Loss: 0.3930, Accuracy: 0.9167\n",
      "Batch number: 114, Training: Loss: 0.1730, Accuracy: 0.9583\n",
      "Batch number: 115, Training: Loss: 0.2140, Accuracy: 0.9583\n",
      "Batch number: 116, Training: Loss: 0.2128, Accuracy: 0.9583\n",
      "Batch number: 117, Training: Loss: 0.0564, Accuracy: 1.0000\n",
      "Batch number: 118, Training: Loss: 0.1333, Accuracy: 0.9167\n",
      "Batch number: 119, Training: Loss: 0.0666, Accuracy: 1.0000\n",
      "Batch number: 120, Training: Loss: 0.0990, Accuracy: 0.9583\n",
      "Batch number: 121, Training: Loss: 0.1014, Accuracy: 0.9167\n",
      "Batch number: 122, Training: Loss: 0.1054, Accuracy: 0.9583\n",
      "Batch number: 123, Training: Loss: 0.1159, Accuracy: 0.9167\n",
      "Batch number: 124, Training: Loss: 0.1599, Accuracy: 0.9167\n",
      "Batch number: 125, Training: Loss: 0.3516, Accuracy: 0.8750\n",
      "Batch number: 126, Training: Loss: 0.3643, Accuracy: 0.8750\n",
      "Batch number: 127, Training: Loss: 0.0776, Accuracy: 0.9583\n",
      "Batch number: 128, Training: Loss: 0.1235, Accuracy: 1.0000\n",
      "Batch number: 129, Training: Loss: 0.1947, Accuracy: 0.9167\n",
      "Batch number: 130, Training: Loss: 0.1065, Accuracy: 0.9583\n",
      "Batch number: 131, Training: Loss: 0.2172, Accuracy: 0.9167\n",
      "Batch number: 132, Training: Loss: 0.4182, Accuracy: 0.8750\n",
      "Batch number: 133, Training: Loss: 0.1036, Accuracy: 0.9583\n",
      "Batch number: 134, Training: Loss: 0.3287, Accuracy: 0.8750\n",
      "Batch number: 135, Training: Loss: 0.2185, Accuracy: 0.8750\n",
      "Batch number: 136, Training: Loss: 0.1633, Accuracy: 0.9583\n",
      "Batch number: 137, Training: Loss: 0.3186, Accuracy: 0.8333\n",
      "Batch number: 138, Training: Loss: 0.3826, Accuracy: 0.8333\n",
      "Batch number: 139, Training: Loss: 0.5079, Accuracy: 0.9167\n",
      "Batch number: 140, Training: Loss: 0.1031, Accuracy: 0.9583\n",
      "Batch number: 141, Training: Loss: 0.0993, Accuracy: 1.0000\n",
      "Batch number: 142, Training: Loss: 0.3593, Accuracy: 0.7917\n",
      "Batch number: 143, Training: Loss: 0.1123, Accuracy: 0.9583\n",
      "Batch number: 144, Training: Loss: 0.2538, Accuracy: 0.8333\n",
      "Batch number: 145, Training: Loss: 0.1407, Accuracy: 0.9583\n",
      "Batch number: 146, Training: Loss: 0.2437, Accuracy: 0.9167\n",
      "Batch number: 147, Training: Loss: 0.0783, Accuracy: 0.9583\n",
      "Batch number: 148, Training: Loss: 0.2024, Accuracy: 0.9167\n",
      "Batch number: 149, Training: Loss: 0.1094, Accuracy: 0.9583\n",
      "Batch number: 150, Training: Loss: 0.0425, Accuracy: 1.0000\n",
      "Batch number: 151, Training: Loss: 0.1091, Accuracy: 0.9583\n",
      "Batch number: 152, Training: Loss: 0.1976, Accuracy: 0.8750\n",
      "Batch number: 153, Training: Loss: 0.0680, Accuracy: 1.0000\n",
      "Batch number: 154, Training: Loss: 0.1770, Accuracy: 0.8750\n",
      "Batch number: 155, Training: Loss: 0.2670, Accuracy: 0.9167\n",
      "Batch number: 156, Training: Loss: 0.0950, Accuracy: 0.9583\n",
      "Batch number: 157, Training: Loss: 0.1263, Accuracy: 0.9583\n",
      "Batch number: 158, Training: Loss: 0.2873, Accuracy: 0.8750\n",
      "Batch number: 159, Training: Loss: 0.1028, Accuracy: 0.9583\n",
      "Batch number: 160, Training: Loss: 0.1324, Accuracy: 0.9583\n",
      "Batch number: 161, Training: Loss: 0.0562, Accuracy: 1.0000\n",
      "Batch number: 162, Training: Loss: 0.2262, Accuracy: 0.9167\n",
      "Batch number: 163, Training: Loss: 0.1095, Accuracy: 0.9583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number: 164, Training: Loss: 0.1384, Accuracy: 0.9583\n",
      "Batch number: 165, Training: Loss: 0.2048, Accuracy: 0.9583\n",
      "Batch number: 166, Training: Loss: 0.0819, Accuracy: 0.9583\n",
      "Batch number: 167, Training: Loss: 0.3174, Accuracy: 0.8750\n",
      "Batch number: 168, Training: Loss: 0.1420, Accuracy: 0.9167\n",
      "Batch number: 169, Training: Loss: 0.1685, Accuracy: 0.9167\n",
      "Batch number: 170, Training: Loss: 0.1719, Accuracy: 0.9583\n",
      "Batch number: 171, Training: Loss: 0.0921, Accuracy: 1.0000\n",
      "Batch number: 172, Training: Loss: 0.0601, Accuracy: 0.9583\n",
      "Batch number: 173, Training: Loss: 0.0307, Accuracy: 1.0000\n",
      "Batch number: 174, Training: Loss: 0.1371, Accuracy: 0.9583\n",
      "Batch number: 175, Training: Loss: 0.0533, Accuracy: 1.0000\n",
      "Batch number: 176, Training: Loss: 0.0295, Accuracy: 1.0000\n",
      "Batch number: 177, Training: Loss: 0.0550, Accuracy: 0.9583\n",
      "Batch number: 178, Training: Loss: 0.0582, Accuracy: 1.0000\n",
      "Batch number: 179, Training: Loss: 0.0496, Accuracy: 1.0000\n",
      "Batch number: 180, Training: Loss: 0.1720, Accuracy: 0.9167\n",
      "Batch number: 181, Training: Loss: 0.1074, Accuracy: 0.9167\n",
      "Batch number: 182, Training: Loss: 0.3043, Accuracy: 0.8750\n",
      "Batch number: 183, Training: Loss: 0.3300, Accuracy: 0.8333\n",
      "Batch number: 184, Training: Loss: 0.0845, Accuracy: 1.0000\n",
      "Batch number: 185, Training: Loss: 0.4281, Accuracy: 0.9167\n",
      "Batch number: 186, Training: Loss: 0.0663, Accuracy: 1.0000\n",
      "Batch number: 187, Training: Loss: 0.1056, Accuracy: 1.0000\n",
      "Batch number: 188, Training: Loss: 0.0809, Accuracy: 0.9583\n",
      "Batch number: 189, Training: Loss: 0.0561, Accuracy: 1.0000\n",
      "Batch number: 190, Training: Loss: 0.1047, Accuracy: 0.9583\n",
      "Batch number: 191, Training: Loss: 0.0573, Accuracy: 1.0000\n",
      "Epoch: 4/150\n",
      "Batch number: 000, Training: Loss: 0.3681, Accuracy: 0.9167\n",
      "Batch number: 001, Training: Loss: 0.0265, Accuracy: 1.0000\n",
      "Batch number: 002, Training: Loss: 0.0633, Accuracy: 1.0000\n",
      "Batch number: 003, Training: Loss: 0.0929, Accuracy: 1.0000\n",
      "Batch number: 004, Training: Loss: 0.0781, Accuracy: 0.9583\n",
      "Batch number: 005, Training: Loss: 0.1470, Accuracy: 0.9583\n",
      "Batch number: 006, Training: Loss: 0.1662, Accuracy: 0.8750\n",
      "Batch number: 007, Training: Loss: 0.0740, Accuracy: 0.9583\n",
      "Batch number: 008, Training: Loss: 0.2204, Accuracy: 0.8750\n",
      "Batch number: 009, Training: Loss: 0.3657, Accuracy: 0.7917\n",
      "Batch number: 010, Training: Loss: 0.1415, Accuracy: 0.9583\n",
      "Batch number: 011, Training: Loss: 0.4804, Accuracy: 0.7500\n",
      "Batch number: 012, Training: Loss: 0.2418, Accuracy: 0.8750\n",
      "Batch number: 013, Training: Loss: 0.2272, Accuracy: 0.9167\n",
      "Batch number: 014, Training: Loss: 0.0882, Accuracy: 0.9583\n",
      "Batch number: 015, Training: Loss: 0.7091, Accuracy: 0.9167\n",
      "Batch number: 016, Training: Loss: 1.2022, Accuracy: 0.7500\n",
      "Batch number: 017, Training: Loss: 0.4328, Accuracy: 0.9167\n",
      "Batch number: 018, Training: Loss: 0.3530, Accuracy: 0.9167\n",
      "Batch number: 019, Training: Loss: 0.3155, Accuracy: 0.8750\n",
      "Batch number: 020, Training: Loss: 0.1485, Accuracy: 0.9583\n",
      "Batch number: 021, Training: Loss: 0.0750, Accuracy: 1.0000\n",
      "Batch number: 022, Training: Loss: 0.4071, Accuracy: 0.8333\n",
      "Batch number: 023, Training: Loss: 0.1976, Accuracy: 0.8333\n",
      "Batch number: 024, Training: Loss: 0.0715, Accuracy: 1.0000\n",
      "Batch number: 025, Training: Loss: 0.2643, Accuracy: 0.8333\n",
      "Batch number: 026, Training: Loss: 0.2105, Accuracy: 0.8333\n",
      "Batch number: 027, Training: Loss: 0.3156, Accuracy: 0.7917\n",
      "Batch number: 028, Training: Loss: 0.1933, Accuracy: 0.9167\n",
      "Batch number: 029, Training: Loss: 0.1221, Accuracy: 1.0000\n",
      "Batch number: 030, Training: Loss: 0.1342, Accuracy: 0.9583\n",
      "Batch number: 031, Training: Loss: 0.1909, Accuracy: 0.9583\n",
      "Batch number: 032, Training: Loss: 0.3390, Accuracy: 0.8750\n",
      "Batch number: 033, Training: Loss: 0.1572, Accuracy: 0.9167\n",
      "Batch number: 034, Training: Loss: 0.1951, Accuracy: 0.8750\n",
      "Batch number: 035, Training: Loss: 0.1616, Accuracy: 0.9583\n",
      "Batch number: 036, Training: Loss: 0.3167, Accuracy: 0.8333\n",
      "Batch number: 037, Training: Loss: 0.0848, Accuracy: 1.0000\n",
      "Batch number: 038, Training: Loss: 0.0889, Accuracy: 1.0000\n",
      "Batch number: 039, Training: Loss: 0.1932, Accuracy: 0.9583\n",
      "Batch number: 040, Training: Loss: 0.1815, Accuracy: 0.9167\n",
      "Batch number: 041, Training: Loss: 0.0310, Accuracy: 1.0000\n",
      "Batch number: 042, Training: Loss: 0.1338, Accuracy: 0.9583\n",
      "Batch number: 043, Training: Loss: 0.1326, Accuracy: 0.9583\n",
      "Batch number: 044, Training: Loss: 0.1142, Accuracy: 0.9167\n",
      "Batch number: 045, Training: Loss: 0.1482, Accuracy: 0.9167\n",
      "Batch number: 046, Training: Loss: 0.1808, Accuracy: 0.9583\n",
      "Batch number: 047, Training: Loss: 0.0407, Accuracy: 1.0000\n",
      "Batch number: 048, Training: Loss: 0.2622, Accuracy: 0.8750\n",
      "Batch number: 049, Training: Loss: 0.2091, Accuracy: 0.8750\n",
      "Batch number: 050, Training: Loss: 0.4059, Accuracy: 0.8333\n",
      "Batch number: 051, Training: Loss: 0.3168, Accuracy: 0.8333\n",
      "Batch number: 052, Training: Loss: 0.1863, Accuracy: 0.9167\n",
      "Batch number: 053, Training: Loss: 0.1120, Accuracy: 0.9583\n",
      "Batch number: 054, Training: Loss: 0.0396, Accuracy: 1.0000\n",
      "Batch number: 055, Training: Loss: 0.2049, Accuracy: 0.9167\n",
      "Batch number: 056, Training: Loss: 0.1782, Accuracy: 0.8750\n",
      "Batch number: 057, Training: Loss: 0.2942, Accuracy: 0.8750\n",
      "Batch number: 058, Training: Loss: 0.1637, Accuracy: 0.9167\n",
      "Batch number: 059, Training: Loss: 0.3882, Accuracy: 0.8750\n",
      "Batch number: 060, Training: Loss: 0.1879, Accuracy: 0.9583\n",
      "Batch number: 061, Training: Loss: 0.0738, Accuracy: 1.0000\n",
      "Batch number: 062, Training: Loss: 0.2955, Accuracy: 0.9167\n",
      "Batch number: 063, Training: Loss: 0.4377, Accuracy: 0.8750\n",
      "Batch number: 064, Training: Loss: 0.0809, Accuracy: 0.9583\n",
      "Batch number: 065, Training: Loss: 0.0450, Accuracy: 1.0000\n",
      "Batch number: 066, Training: Loss: 0.1860, Accuracy: 0.8750\n",
      "Batch number: 067, Training: Loss: 0.1902, Accuracy: 0.8750\n",
      "Batch number: 068, Training: Loss: 0.1378, Accuracy: 0.9583\n",
      "Batch number: 069, Training: Loss: 0.2604, Accuracy: 0.7500\n",
      "Batch number: 070, Training: Loss: 0.1790, Accuracy: 0.9167\n",
      "Batch number: 071, Training: Loss: 0.1664, Accuracy: 0.9167\n",
      "Batch number: 072, Training: Loss: 0.1909, Accuracy: 0.9167\n",
      "Batch number: 073, Training: Loss: 0.1240, Accuracy: 0.9583\n",
      "Batch number: 074, Training: Loss: 0.1136, Accuracy: 0.9583\n",
      "Batch number: 075, Training: Loss: 0.1527, Accuracy: 0.9167\n",
      "Batch number: 076, Training: Loss: 0.0616, Accuracy: 1.0000\n",
      "Batch number: 077, Training: Loss: 0.2040, Accuracy: 0.9167\n",
      "Batch number: 078, Training: Loss: 0.1409, Accuracy: 0.9167\n",
      "Batch number: 079, Training: Loss: 0.1310, Accuracy: 0.9167\n",
      "Batch number: 080, Training: Loss: 0.1789, Accuracy: 0.9583\n",
      "Batch number: 081, Training: Loss: 0.1166, Accuracy: 0.9583\n",
      "Batch number: 082, Training: Loss: 0.1842, Accuracy: 0.9167\n",
      "Batch number: 083, Training: Loss: 0.2090, Accuracy: 0.9167\n",
      "Batch number: 084, Training: Loss: 0.0394, Accuracy: 1.0000\n",
      "Batch number: 085, Training: Loss: 0.1146, Accuracy: 0.9583\n",
      "Batch number: 086, Training: Loss: 0.1546, Accuracy: 0.9167\n",
      "Batch number: 087, Training: Loss: 0.2043, Accuracy: 0.9167\n",
      "Batch number: 088, Training: Loss: 0.0336, Accuracy: 1.0000\n",
      "Batch number: 089, Training: Loss: 0.2531, Accuracy: 0.8750\n",
      "Batch number: 090, Training: Loss: 0.1271, Accuracy: 0.9167\n",
      "Batch number: 091, Training: Loss: 0.2763, Accuracy: 0.9167\n",
      "Batch number: 092, Training: Loss: 0.1050, Accuracy: 1.0000\n",
      "Batch number: 093, Training: Loss: 0.3524, Accuracy: 0.9167\n",
      "Batch number: 094, Training: Loss: 0.3443, Accuracy: 0.9167\n",
      "Batch number: 095, Training: Loss: 0.0663, Accuracy: 1.0000\n",
      "Batch number: 096, Training: Loss: 0.2562, Accuracy: 0.8750\n",
      "Batch number: 097, Training: Loss: 0.0823, Accuracy: 1.0000\n",
      "Batch number: 098, Training: Loss: 0.0981, Accuracy: 0.9583\n",
      "Batch number: 099, Training: Loss: 0.1993, Accuracy: 0.8750\n",
      "Batch number: 100, Training: Loss: 0.1022, Accuracy: 0.9583\n",
      "Batch number: 101, Training: Loss: 0.0688, Accuracy: 1.0000\n",
      "Batch number: 102, Training: Loss: 0.0972, Accuracy: 1.0000\n",
      "Batch number: 103, Training: Loss: 0.1287, Accuracy: 0.9167\n",
      "Batch number: 104, Training: Loss: 0.1987, Accuracy: 0.9167\n",
      "Batch number: 105, Training: Loss: 0.0391, Accuracy: 1.0000\n",
      "Batch number: 106, Training: Loss: 0.0433, Accuracy: 1.0000\n",
      "Batch number: 107, Training: Loss: 0.0643, Accuracy: 0.9583\n",
      "Batch number: 108, Training: Loss: 0.1610, Accuracy: 0.9167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number: 109, Training: Loss: 0.1295, Accuracy: 0.9167\n",
      "Batch number: 110, Training: Loss: 0.1334, Accuracy: 0.9583\n",
      "Batch number: 111, Training: Loss: 0.0912, Accuracy: 0.9167\n",
      "Batch number: 112, Training: Loss: 0.1440, Accuracy: 0.8750\n",
      "Batch number: 113, Training: Loss: 0.0472, Accuracy: 1.0000\n",
      "Batch number: 114, Training: Loss: 0.0330, Accuracy: 1.0000\n",
      "Batch number: 115, Training: Loss: 0.0573, Accuracy: 1.0000\n",
      "Batch number: 116, Training: Loss: 0.1773, Accuracy: 0.9167\n",
      "Batch number: 117, Training: Loss: 0.0881, Accuracy: 0.9583\n",
      "Batch number: 118, Training: Loss: 0.2128, Accuracy: 0.9583\n",
      "Batch number: 119, Training: Loss: 0.1863, Accuracy: 0.9583\n",
      "Batch number: 120, Training: Loss: 0.1376, Accuracy: 0.9583\n",
      "Batch number: 121, Training: Loss: 0.4722, Accuracy: 0.8333\n",
      "Batch number: 122, Training: Loss: 0.1348, Accuracy: 0.9583\n",
      "Batch number: 123, Training: Loss: 0.4569, Accuracy: 0.7917\n",
      "Batch number: 124, Training: Loss: 0.1866, Accuracy: 0.8750\n",
      "Batch number: 125, Training: Loss: 0.0961, Accuracy: 0.9583\n",
      "Batch number: 126, Training: Loss: 0.1089, Accuracy: 0.9583\n",
      "Batch number: 127, Training: Loss: 0.0791, Accuracy: 0.9583\n",
      "Batch number: 128, Training: Loss: 0.1195, Accuracy: 1.0000\n",
      "Batch number: 129, Training: Loss: 0.1182, Accuracy: 0.9583\n",
      "Batch number: 130, Training: Loss: 0.2939, Accuracy: 0.9583\n",
      "Batch number: 131, Training: Loss: 0.1927, Accuracy: 0.9583\n",
      "Batch number: 132, Training: Loss: 0.0454, Accuracy: 1.0000\n",
      "Batch number: 133, Training: Loss: 0.0543, Accuracy: 1.0000\n",
      "Batch number: 134, Training: Loss: 0.1445, Accuracy: 0.9167\n",
      "Batch number: 135, Training: Loss: 0.1582, Accuracy: 0.9167\n",
      "Batch number: 136, Training: Loss: 0.1819, Accuracy: 0.9583\n",
      "Batch number: 137, Training: Loss: 0.0646, Accuracy: 1.0000\n",
      "Batch number: 138, Training: Loss: 0.1798, Accuracy: 0.9167\n",
      "Batch number: 139, Training: Loss: 0.1355, Accuracy: 0.9583\n",
      "Batch number: 140, Training: Loss: 0.1528, Accuracy: 0.9167\n",
      "Batch number: 141, Training: Loss: 0.1529, Accuracy: 0.9167\n",
      "Batch number: 142, Training: Loss: 0.0659, Accuracy: 0.9583\n",
      "Batch number: 143, Training: Loss: 0.1063, Accuracy: 0.9167\n",
      "Batch number: 144, Training: Loss: 0.3614, Accuracy: 0.8333\n",
      "Batch number: 145, Training: Loss: 0.2769, Accuracy: 0.8333\n",
      "Batch number: 146, Training: Loss: 0.0631, Accuracy: 0.9583\n",
      "Batch number: 147, Training: Loss: 0.3486, Accuracy: 0.8333\n",
      "Batch number: 148, Training: Loss: 0.1351, Accuracy: 0.8750\n",
      "Batch number: 149, Training: Loss: 0.0999, Accuracy: 1.0000\n",
      "Batch number: 150, Training: Loss: 0.0452, Accuracy: 1.0000\n",
      "Batch number: 151, Training: Loss: 0.0467, Accuracy: 1.0000\n",
      "Batch number: 152, Training: Loss: 0.6442, Accuracy: 0.8750\n",
      "Batch number: 153, Training: Loss: 0.5907, Accuracy: 0.9167\n",
      "Batch number: 154, Training: Loss: 0.1259, Accuracy: 1.0000\n",
      "Batch number: 155, Training: Loss: 0.0508, Accuracy: 1.0000\n",
      "Batch number: 156, Training: Loss: 0.1124, Accuracy: 0.9583\n",
      "Batch number: 157, Training: Loss: 0.1694, Accuracy: 0.9167\n",
      "Batch number: 158, Training: Loss: 0.1674, Accuracy: 0.9167\n",
      "Batch number: 159, Training: Loss: 0.1293, Accuracy: 0.9583\n",
      "Batch number: 160, Training: Loss: 0.2457, Accuracy: 0.9167\n",
      "Batch number: 161, Training: Loss: 0.0983, Accuracy: 0.9583\n",
      "Batch number: 162, Training: Loss: 0.2787, Accuracy: 0.9167\n",
      "Batch number: 163, Training: Loss: 0.3239, Accuracy: 0.7500\n",
      "Batch number: 164, Training: Loss: 0.1564, Accuracy: 0.9167\n",
      "Batch number: 165, Training: Loss: 0.2200, Accuracy: 0.8750\n",
      "Batch number: 166, Training: Loss: 0.2433, Accuracy: 0.9167\n",
      "Batch number: 167, Training: Loss: 0.1449, Accuracy: 0.9583\n",
      "Batch number: 168, Training: Loss: 0.2353, Accuracy: 0.9583\n",
      "Batch number: 169, Training: Loss: 0.3597, Accuracy: 0.8750\n",
      "Batch number: 170, Training: Loss: 0.3928, Accuracy: 0.8333\n",
      "Batch number: 171, Training: Loss: 0.0485, Accuracy: 1.0000\n",
      "Batch number: 172, Training: Loss: 0.2264, Accuracy: 0.9167\n",
      "Batch number: 173, Training: Loss: 0.2726, Accuracy: 0.8333\n",
      "Batch number: 174, Training: Loss: 0.1091, Accuracy: 0.9583\n",
      "Batch number: 175, Training: Loss: 0.0652, Accuracy: 1.0000\n",
      "Batch number: 176, Training: Loss: 0.2302, Accuracy: 0.8750\n",
      "Batch number: 177, Training: Loss: 0.1637, Accuracy: 0.8750\n",
      "Batch number: 178, Training: Loss: 0.1016, Accuracy: 1.0000\n",
      "Batch number: 179, Training: Loss: 0.1495, Accuracy: 0.9167\n",
      "Batch number: 180, Training: Loss: 0.1513, Accuracy: 0.9167\n",
      "Batch number: 181, Training: Loss: 0.1757, Accuracy: 0.9583\n",
      "Batch number: 182, Training: Loss: 0.0596, Accuracy: 0.9583\n",
      "Batch number: 183, Training: Loss: 0.3706, Accuracy: 0.8750\n",
      "Batch number: 184, Training: Loss: 0.1337, Accuracy: 0.9167\n",
      "Batch number: 185, Training: Loss: 0.5864, Accuracy: 0.8750\n",
      "Batch number: 186, Training: Loss: 0.1852, Accuracy: 0.9167\n",
      "Batch number: 187, Training: Loss: 0.1384, Accuracy: 0.9167\n",
      "Batch number: 188, Training: Loss: 0.2266, Accuracy: 0.8750\n",
      "Batch number: 189, Training: Loss: 0.0650, Accuracy: 1.0000\n",
      "Batch number: 190, Training: Loss: 0.2606, Accuracy: 0.9167\n",
      "Batch number: 191, Training: Loss: 0.1215, Accuracy: 0.9474\n",
      "Epoch: 5/150\n",
      "Batch number: 000, Training: Loss: 0.1213, Accuracy: 0.9583\n",
      "Batch number: 001, Training: Loss: 0.0815, Accuracy: 1.0000\n",
      "Batch number: 002, Training: Loss: 0.0906, Accuracy: 1.0000\n",
      "Batch number: 003, Training: Loss: 0.1141, Accuracy: 0.9167\n",
      "Batch number: 004, Training: Loss: 0.0727, Accuracy: 0.9583\n",
      "Batch number: 005, Training: Loss: 0.1818, Accuracy: 0.9167\n",
      "Batch number: 006, Training: Loss: 0.2472, Accuracy: 0.8750\n",
      "Batch number: 007, Training: Loss: 0.1050, Accuracy: 0.9583\n",
      "Batch number: 008, Training: Loss: 0.1522, Accuracy: 0.9167\n",
      "Batch number: 009, Training: Loss: 0.1307, Accuracy: 1.0000\n",
      "Batch number: 010, Training: Loss: 0.0974, Accuracy: 0.9167\n",
      "Batch number: 011, Training: Loss: 0.1008, Accuracy: 1.0000\n",
      "Batch number: 012, Training: Loss: 0.1372, Accuracy: 0.9583\n",
      "Batch number: 013, Training: Loss: 0.0401, Accuracy: 1.0000\n",
      "Batch number: 014, Training: Loss: 0.0540, Accuracy: 0.9583\n",
      "Batch number: 015, Training: Loss: 0.2209, Accuracy: 0.9583\n",
      "Batch number: 016, Training: Loss: 0.3305, Accuracy: 0.8333\n",
      "Batch number: 017, Training: Loss: 0.0408, Accuracy: 1.0000\n",
      "Batch number: 018, Training: Loss: 0.0566, Accuracy: 1.0000\n",
      "Batch number: 019, Training: Loss: 0.1565, Accuracy: 0.9583\n",
      "Batch number: 020, Training: Loss: 0.1729, Accuracy: 0.9167\n",
      "Batch number: 021, Training: Loss: 0.2852, Accuracy: 0.8750\n",
      "Batch number: 022, Training: Loss: 0.0653, Accuracy: 1.0000\n",
      "Batch number: 023, Training: Loss: 0.1807, Accuracy: 0.9583\n",
      "Batch number: 024, Training: Loss: 0.1110, Accuracy: 0.9583\n",
      "Batch number: 025, Training: Loss: 0.1172, Accuracy: 0.9583\n",
      "Batch number: 026, Training: Loss: 0.2393, Accuracy: 0.9167\n",
      "Batch number: 027, Training: Loss: 0.2088, Accuracy: 0.9167\n",
      "Batch number: 028, Training: Loss: 0.2352, Accuracy: 0.9167\n",
      "Batch number: 029, Training: Loss: 0.1683, Accuracy: 0.9167\n",
      "Batch number: 030, Training: Loss: 0.2556, Accuracy: 0.8333\n",
      "Batch number: 031, Training: Loss: 0.2169, Accuracy: 0.9583\n",
      "Batch number: 032, Training: Loss: 0.2935, Accuracy: 0.8750\n",
      "Batch number: 033, Training: Loss: 0.3854, Accuracy: 0.7917\n",
      "Batch number: 034, Training: Loss: 0.1963, Accuracy: 0.8333\n",
      "Batch number: 035, Training: Loss: 0.1024, Accuracy: 0.9167\n",
      "Batch number: 036, Training: Loss: 0.3404, Accuracy: 0.8750\n",
      "Batch number: 037, Training: Loss: 0.0919, Accuracy: 1.0000\n",
      "Batch number: 038, Training: Loss: 0.0700, Accuracy: 0.9583\n",
      "Batch number: 039, Training: Loss: 0.1912, Accuracy: 0.9583\n",
      "Batch number: 040, Training: Loss: 0.1077, Accuracy: 0.9583\n",
      "Batch number: 041, Training: Loss: 0.3798, Accuracy: 0.9167\n",
      "Batch number: 042, Training: Loss: 0.0430, Accuracy: 1.0000\n",
      "Batch number: 043, Training: Loss: 0.2225, Accuracy: 0.9583\n",
      "Batch number: 044, Training: Loss: 0.1821, Accuracy: 0.9583\n",
      "Batch number: 045, Training: Loss: 0.1308, Accuracy: 0.9583\n",
      "Batch number: 046, Training: Loss: 0.0819, Accuracy: 1.0000\n",
      "Batch number: 047, Training: Loss: 0.2005, Accuracy: 0.9167\n",
      "Batch number: 048, Training: Loss: 0.1001, Accuracy: 0.9583\n",
      "Batch number: 049, Training: Loss: 0.0846, Accuracy: 0.9583\n",
      "Batch number: 050, Training: Loss: 0.1761, Accuracy: 0.9167\n",
      "Batch number: 051, Training: Loss: 0.2383, Accuracy: 0.8750\n",
      "Batch number: 052, Training: Loss: 0.2141, Accuracy: 0.8750\n",
      "Batch number: 053, Training: Loss: 0.0895, Accuracy: 0.9583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number: 054, Training: Loss: 0.0335, Accuracy: 1.0000\n",
      "Batch number: 055, Training: Loss: 0.1029, Accuracy: 0.9583\n",
      "Batch number: 056, Training: Loss: 0.3236, Accuracy: 0.8750\n",
      "Batch number: 057, Training: Loss: 0.3457, Accuracy: 0.8750\n",
      "Batch number: 058, Training: Loss: 0.1209, Accuracy: 0.9167\n",
      "Batch number: 059, Training: Loss: 0.0595, Accuracy: 0.9583\n",
      "Batch number: 060, Training: Loss: 0.0914, Accuracy: 1.0000\n",
      "Batch number: 061, Training: Loss: 0.1728, Accuracy: 0.9167\n",
      "Batch number: 062, Training: Loss: 0.0862, Accuracy: 0.9583\n",
      "Batch number: 063, Training: Loss: 0.0441, Accuracy: 1.0000\n",
      "Batch number: 064, Training: Loss: 0.0738, Accuracy: 1.0000\n",
      "Batch number: 065, Training: Loss: 0.5551, Accuracy: 0.8333\n",
      "Batch number: 066, Training: Loss: 0.1646, Accuracy: 0.9167\n",
      "Batch number: 067, Training: Loss: 0.0341, Accuracy: 1.0000\n",
      "Batch number: 068, Training: Loss: 0.2231, Accuracy: 0.8750\n",
      "Batch number: 069, Training: Loss: 0.1294, Accuracy: 0.9583\n",
      "Batch number: 070, Training: Loss: 0.0377, Accuracy: 1.0000\n",
      "Batch number: 071, Training: Loss: 0.0632, Accuracy: 1.0000\n",
      "Batch number: 072, Training: Loss: 0.1231, Accuracy: 0.9167\n",
      "Batch number: 073, Training: Loss: 0.1946, Accuracy: 0.9167\n",
      "Batch number: 074, Training: Loss: 0.2169, Accuracy: 0.9167\n",
      "Batch number: 075, Training: Loss: 0.0721, Accuracy: 1.0000\n",
      "Batch number: 076, Training: Loss: 0.0208, Accuracy: 1.0000\n",
      "Batch number: 077, Training: Loss: 0.0925, Accuracy: 0.9583\n",
      "Batch number: 078, Training: Loss: 0.1369, Accuracy: 0.9167\n",
      "Batch number: 079, Training: Loss: 0.1674, Accuracy: 0.9167\n",
      "Batch number: 080, Training: Loss: 0.0770, Accuracy: 1.0000\n",
      "Batch number: 081, Training: Loss: 0.1752, Accuracy: 0.9167\n",
      "Batch number: 082, Training: Loss: 0.1535, Accuracy: 0.9167\n",
      "Batch number: 083, Training: Loss: 0.1498, Accuracy: 0.9583\n",
      "Batch number: 084, Training: Loss: 0.2120, Accuracy: 0.9167\n",
      "Batch number: 085, Training: Loss: 0.6742, Accuracy: 0.9583\n",
      "Batch number: 086, Training: Loss: 0.0787, Accuracy: 1.0000\n",
      "Batch number: 087, Training: Loss: 0.0389, Accuracy: 0.9583\n",
      "Batch number: 088, Training: Loss: 0.0871, Accuracy: 0.9583\n",
      "Batch number: 089, Training: Loss: 0.0148, Accuracy: 1.0000\n",
      "Batch number: 090, Training: Loss: 0.0914, Accuracy: 0.9583\n",
      "Batch number: 091, Training: Loss: 0.3464, Accuracy: 0.8333\n",
      "Batch number: 092, Training: Loss: 0.1822, Accuracy: 0.8333\n",
      "Batch number: 093, Training: Loss: 0.2075, Accuracy: 0.9167\n",
      "Batch number: 094, Training: Loss: 0.1064, Accuracy: 0.9583\n",
      "Batch number: 095, Training: Loss: 0.0625, Accuracy: 0.9583\n",
      "Batch number: 096, Training: Loss: 0.2156, Accuracy: 0.9167\n",
      "Batch number: 097, Training: Loss: 0.2028, Accuracy: 0.8750\n",
      "Batch number: 098, Training: Loss: 0.0531, Accuracy: 1.0000\n",
      "Batch number: 099, Training: Loss: 0.0457, Accuracy: 1.0000\n",
      "Batch number: 100, Training: Loss: 0.0126, Accuracy: 1.0000\n",
      "Batch number: 101, Training: Loss: 0.0954, Accuracy: 0.9583\n",
      "Batch number: 102, Training: Loss: 0.4659, Accuracy: 0.8333\n",
      "Batch number: 103, Training: Loss: 0.0578, Accuracy: 1.0000\n",
      "Batch number: 104, Training: Loss: 0.0177, Accuracy: 1.0000\n",
      "Batch number: 105, Training: Loss: 0.0493, Accuracy: 1.0000\n",
      "Batch number: 106, Training: Loss: 0.2746, Accuracy: 0.9167\n",
      "Batch number: 107, Training: Loss: 0.1129, Accuracy: 0.9167\n",
      "Batch number: 108, Training: Loss: 0.1295, Accuracy: 0.9167\n",
      "Batch number: 109, Training: Loss: 0.1518, Accuracy: 0.8750\n",
      "Batch number: 110, Training: Loss: 0.0459, Accuracy: 1.0000\n",
      "Batch number: 111, Training: Loss: 0.2772, Accuracy: 0.9583\n",
      "Batch number: 112, Training: Loss: 0.0544, Accuracy: 0.9583\n",
      "Batch number: 113, Training: Loss: 0.0456, Accuracy: 1.0000\n",
      "Batch number: 114, Training: Loss: 0.0478, Accuracy: 1.0000\n",
      "Batch number: 115, Training: Loss: 0.0101, Accuracy: 1.0000\n",
      "Batch number: 116, Training: Loss: 0.5666, Accuracy: 0.8333\n",
      "Batch number: 117, Training: Loss: 0.1223, Accuracy: 0.9583\n",
      "Batch number: 118, Training: Loss: 0.6380, Accuracy: 0.7917\n",
      "Batch number: 119, Training: Loss: 0.0690, Accuracy: 0.9583\n",
      "Batch number: 120, Training: Loss: 0.1145, Accuracy: 0.9167\n",
      "Batch number: 121, Training: Loss: 0.1743, Accuracy: 0.9167\n",
      "Batch number: 122, Training: Loss: 0.4338, Accuracy: 0.7917\n",
      "Batch number: 123, Training: Loss: 0.0965, Accuracy: 0.9583\n",
      "Batch number: 124, Training: Loss: 0.2004, Accuracy: 0.9167\n",
      "Batch number: 125, Training: Loss: 0.2191, Accuracy: 0.9167\n",
      "Batch number: 126, Training: Loss: 0.1974, Accuracy: 0.8333\n",
      "Batch number: 127, Training: Loss: 0.3977, Accuracy: 0.9167\n",
      "Batch number: 128, Training: Loss: 0.1176, Accuracy: 0.9583\n",
      "Batch number: 129, Training: Loss: 0.0717, Accuracy: 1.0000\n",
      "Batch number: 130, Training: Loss: 0.2211, Accuracy: 0.8750\n",
      "Batch number: 131, Training: Loss: 0.0595, Accuracy: 1.0000\n",
      "Batch number: 132, Training: Loss: 0.1622, Accuracy: 0.9167\n",
      "Batch number: 133, Training: Loss: 0.1187, Accuracy: 1.0000\n",
      "Batch number: 134, Training: Loss: 0.2045, Accuracy: 0.9167\n",
      "Batch number: 135, Training: Loss: 0.1911, Accuracy: 0.9167\n",
      "Batch number: 136, Training: Loss: 0.0575, Accuracy: 1.0000\n",
      "Batch number: 137, Training: Loss: 0.2091, Accuracy: 0.9583\n",
      "Batch number: 138, Training: Loss: 0.0927, Accuracy: 0.9583\n",
      "Batch number: 139, Training: Loss: 0.1867, Accuracy: 0.9583\n",
      "Batch number: 140, Training: Loss: 0.0697, Accuracy: 0.9583\n",
      "Batch number: 141, Training: Loss: 0.1169, Accuracy: 1.0000\n",
      "Batch number: 142, Training: Loss: 0.0749, Accuracy: 1.0000\n",
      "Batch number: 143, Training: Loss: 0.1707, Accuracy: 0.9583\n",
      "Batch number: 144, Training: Loss: 0.2125, Accuracy: 0.8750\n",
      "Batch number: 145, Training: Loss: 0.2113, Accuracy: 0.9167\n",
      "Batch number: 146, Training: Loss: 0.2173, Accuracy: 0.8750\n",
      "Batch number: 147, Training: Loss: 0.1274, Accuracy: 0.9167\n",
      "Batch number: 148, Training: Loss: 0.1005, Accuracy: 0.9583\n",
      "Batch number: 149, Training: Loss: 0.2787, Accuracy: 0.9167\n",
      "Batch number: 150, Training: Loss: 0.3106, Accuracy: 0.9583\n",
      "Batch number: 151, Training: Loss: 0.4827, Accuracy: 0.9167\n",
      "Batch number: 152, Training: Loss: 0.2774, Accuracy: 0.8750\n",
      "Batch number: 153, Training: Loss: 0.0505, Accuracy: 1.0000\n",
      "Batch number: 154, Training: Loss: 0.4849, Accuracy: 0.8750\n",
      "Batch number: 155, Training: Loss: 0.1874, Accuracy: 0.9583\n",
      "Batch number: 156, Training: Loss: 0.1407, Accuracy: 0.9583\n",
      "Batch number: 157, Training: Loss: 0.1480, Accuracy: 0.9583\n",
      "Batch number: 158, Training: Loss: 0.1600, Accuracy: 0.9167\n",
      "Batch number: 159, Training: Loss: 0.1473, Accuracy: 1.0000\n",
      "Batch number: 160, Training: Loss: 0.0791, Accuracy: 0.9583\n",
      "Batch number: 161, Training: Loss: 0.1382, Accuracy: 0.8750\n",
      "Batch number: 162, Training: Loss: 0.1900, Accuracy: 0.8333\n",
      "Batch number: 163, Training: Loss: 0.3773, Accuracy: 0.7917\n",
      "Batch number: 164, Training: Loss: 0.1034, Accuracy: 0.9583\n",
      "Batch number: 165, Training: Loss: 0.1634, Accuracy: 0.8750\n",
      "Batch number: 166, Training: Loss: 0.1076, Accuracy: 0.9167\n",
      "Batch number: 167, Training: Loss: 0.1526, Accuracy: 0.9167\n",
      "Batch number: 168, Training: Loss: 0.1330, Accuracy: 0.9167\n",
      "Batch number: 169, Training: Loss: 0.0639, Accuracy: 1.0000\n",
      "Batch number: 170, Training: Loss: 0.1696, Accuracy: 0.9583\n",
      "Batch number: 171, Training: Loss: 0.0423, Accuracy: 1.0000\n",
      "Batch number: 172, Training: Loss: 0.2188, Accuracy: 0.9167\n",
      "Batch number: 173, Training: Loss: 0.1811, Accuracy: 0.9167\n",
      "Batch number: 174, Training: Loss: 0.1932, Accuracy: 0.8750\n",
      "Batch number: 175, Training: Loss: 0.6411, Accuracy: 0.7500\n",
      "Batch number: 176, Training: Loss: 0.0806, Accuracy: 0.9583\n",
      "Batch number: 177, Training: Loss: 0.1853, Accuracy: 0.8750\n",
      "Batch number: 178, Training: Loss: 0.3722, Accuracy: 0.9167\n",
      "Batch number: 179, Training: Loss: 0.1046, Accuracy: 0.9583\n",
      "Batch number: 180, Training: Loss: 0.0860, Accuracy: 0.9583\n",
      "Batch number: 181, Training: Loss: 0.2439, Accuracy: 0.8333\n",
      "Batch number: 182, Training: Loss: 0.1143, Accuracy: 0.9583\n",
      "Batch number: 183, Training: Loss: 0.0692, Accuracy: 1.0000\n",
      "Batch number: 184, Training: Loss: 0.1454, Accuracy: 0.8750\n",
      "Batch number: 185, Training: Loss: 0.2467, Accuracy: 0.7917\n",
      "Batch number: 186, Training: Loss: 0.3340, Accuracy: 0.8750\n",
      "Batch number: 187, Training: Loss: 0.0379, Accuracy: 1.0000\n",
      "Batch number: 188, Training: Loss: 0.2203, Accuracy: 0.9167\n",
      "Batch number: 189, Training: Loss: 0.1645, Accuracy: 0.9583\n",
      "Batch number: 190, Training: Loss: 0.2810, Accuracy: 0.9167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number: 191, Training: Loss: 0.1580, Accuracy: 0.9474\n",
      "Epoch: 6/150\n",
      "Batch number: 000, Training: Loss: 0.0781, Accuracy: 0.9583\n",
      "Batch number: 001, Training: Loss: 0.4227, Accuracy: 0.7917\n",
      "Batch number: 002, Training: Loss: 0.4997, Accuracy: 0.8750\n",
      "Batch number: 003, Training: Loss: 0.1232, Accuracy: 0.9583\n",
      "Batch number: 004, Training: Loss: 0.1411, Accuracy: 0.9583\n",
      "Batch number: 005, Training: Loss: 0.1417, Accuracy: 0.9167\n",
      "Batch number: 006, Training: Loss: 0.3030, Accuracy: 0.8750\n",
      "Batch number: 007, Training: Loss: 0.1451, Accuracy: 0.9167\n",
      "Batch number: 008, Training: Loss: 0.0374, Accuracy: 1.0000\n",
      "Batch number: 009, Training: Loss: 0.1983, Accuracy: 0.9583\n",
      "Batch number: 010, Training: Loss: 0.1648, Accuracy: 0.8750\n",
      "Batch number: 011, Training: Loss: 0.0985, Accuracy: 1.0000\n",
      "Batch number: 012, Training: Loss: 0.1979, Accuracy: 0.9167\n",
      "Batch number: 013, Training: Loss: 0.0827, Accuracy: 1.0000\n",
      "Batch number: 014, Training: Loss: 0.0870, Accuracy: 1.0000\n",
      "Batch number: 015, Training: Loss: 0.2319, Accuracy: 0.8750\n",
      "Batch number: 016, Training: Loss: 0.2408, Accuracy: 0.8750\n",
      "Batch number: 017, Training: Loss: 0.0478, Accuracy: 1.0000\n",
      "Batch number: 018, Training: Loss: 0.2370, Accuracy: 0.8750\n",
      "Batch number: 019, Training: Loss: 0.1784, Accuracy: 0.9167\n",
      "Batch number: 020, Training: Loss: 0.2152, Accuracy: 0.9167\n",
      "Batch number: 021, Training: Loss: 0.0751, Accuracy: 1.0000\n",
      "Batch number: 022, Training: Loss: 0.0516, Accuracy: 1.0000\n",
      "Batch number: 023, Training: Loss: 0.1499, Accuracy: 0.9583\n",
      "Batch number: 024, Training: Loss: 0.0741, Accuracy: 1.0000\n",
      "Batch number: 025, Training: Loss: 0.2080, Accuracy: 0.9167\n",
      "Batch number: 026, Training: Loss: 0.0507, Accuracy: 1.0000\n",
      "Batch number: 027, Training: Loss: 0.0642, Accuracy: 1.0000\n",
      "Batch number: 028, Training: Loss: 0.2837, Accuracy: 0.9167\n",
      "Batch number: 029, Training: Loss: 0.1082, Accuracy: 0.9583\n",
      "Batch number: 030, Training: Loss: 0.0979, Accuracy: 0.9167\n",
      "Batch number: 031, Training: Loss: 0.1278, Accuracy: 0.9583\n",
      "Batch number: 032, Training: Loss: 0.2960, Accuracy: 0.8750\n",
      "Batch number: 033, Training: Loss: 0.2051, Accuracy: 0.8750\n",
      "Batch number: 034, Training: Loss: 0.3487, Accuracy: 0.8750\n",
      "Batch number: 035, Training: Loss: 0.0633, Accuracy: 1.0000\n",
      "Batch number: 036, Training: Loss: 0.1143, Accuracy: 0.9583\n",
      "Batch number: 037, Training: Loss: 0.1397, Accuracy: 0.9583\n",
      "Batch number: 038, Training: Loss: 0.0869, Accuracy: 1.0000\n",
      "Batch number: 039, Training: Loss: 0.0607, Accuracy: 0.9583\n",
      "Batch number: 040, Training: Loss: 0.2186, Accuracy: 0.9583\n",
      "Batch number: 041, Training: Loss: 0.0112, Accuracy: 1.0000\n",
      "Batch number: 042, Training: Loss: 0.3565, Accuracy: 0.9583\n",
      "Batch number: 043, Training: Loss: 0.0439, Accuracy: 1.0000\n",
      "Batch number: 044, Training: Loss: 0.0464, Accuracy: 1.0000\n",
      "Batch number: 045, Training: Loss: 0.0516, Accuracy: 1.0000\n",
      "Batch number: 046, Training: Loss: 0.1283, Accuracy: 0.9583\n",
      "Batch number: 047, Training: Loss: 0.1230, Accuracy: 0.9167\n",
      "Batch number: 048, Training: Loss: 0.5326, Accuracy: 0.8333\n",
      "Batch number: 049, Training: Loss: 0.1449, Accuracy: 0.9583\n",
      "Batch number: 050, Training: Loss: 0.2114, Accuracy: 0.9167\n",
      "Batch number: 051, Training: Loss: 0.0599, Accuracy: 1.0000\n",
      "Batch number: 052, Training: Loss: 0.0856, Accuracy: 0.9583\n",
      "Batch number: 053, Training: Loss: 0.0742, Accuracy: 1.0000\n",
      "Batch number: 054, Training: Loss: 0.1327, Accuracy: 0.9167\n",
      "Batch number: 055, Training: Loss: 0.0458, Accuracy: 0.9583\n",
      "Batch number: 056, Training: Loss: 0.2283, Accuracy: 0.9167\n",
      "Batch number: 057, Training: Loss: 0.0512, Accuracy: 0.9583\n",
      "Batch number: 058, Training: Loss: 0.0463, Accuracy: 1.0000\n",
      "Batch number: 059, Training: Loss: 0.1433, Accuracy: 0.9583\n",
      "Batch number: 060, Training: Loss: 0.0590, Accuracy: 1.0000\n",
      "Batch number: 061, Training: Loss: 0.1119, Accuracy: 0.9167\n",
      "Batch number: 062, Training: Loss: 0.1417, Accuracy: 0.9583\n",
      "Batch number: 063, Training: Loss: 0.4459, Accuracy: 0.8333\n",
      "Batch number: 064, Training: Loss: 0.3221, Accuracy: 0.9167\n",
      "Batch number: 065, Training: Loss: 0.1973, Accuracy: 0.9167\n",
      "Batch number: 066, Training: Loss: 0.0772, Accuracy: 0.9583\n",
      "Batch number: 067, Training: Loss: 0.3011, Accuracy: 0.9583\n",
      "Batch number: 068, Training: Loss: 0.0771, Accuracy: 0.9583\n",
      "Batch number: 069, Training: Loss: 0.0735, Accuracy: 0.9583\n",
      "Batch number: 070, Training: Loss: 0.0407, Accuracy: 1.0000\n",
      "Batch number: 071, Training: Loss: 0.1595, Accuracy: 0.9583\n",
      "Batch number: 072, Training: Loss: 0.3148, Accuracy: 0.8750\n",
      "Batch number: 073, Training: Loss: 0.1142, Accuracy: 0.9583\n",
      "Batch number: 074, Training: Loss: 0.2583, Accuracy: 0.8750\n",
      "Batch number: 075, Training: Loss: 0.1425, Accuracy: 0.9167\n",
      "Batch number: 076, Training: Loss: 0.0993, Accuracy: 1.0000\n",
      "Batch number: 077, Training: Loss: 0.1410, Accuracy: 0.9583\n",
      "Batch number: 078, Training: Loss: 0.0653, Accuracy: 1.0000\n",
      "Batch number: 079, Training: Loss: 0.1711, Accuracy: 0.8750\n",
      "Batch number: 080, Training: Loss: 0.0928, Accuracy: 0.9167\n",
      "Batch number: 081, Training: Loss: 0.1539, Accuracy: 0.9167\n",
      "Batch number: 082, Training: Loss: 0.2784, Accuracy: 0.8333\n",
      "Batch number: 083, Training: Loss: 0.1844, Accuracy: 0.9167\n",
      "Batch number: 084, Training: Loss: 0.1134, Accuracy: 0.9583\n",
      "Batch number: 085, Training: Loss: 0.2099, Accuracy: 0.9167\n",
      "Batch number: 086, Training: Loss: 0.2291, Accuracy: 0.9167\n",
      "Batch number: 087, Training: Loss: 0.1479, Accuracy: 0.9167\n",
      "Batch number: 088, Training: Loss: 0.0992, Accuracy: 0.9583\n",
      "Batch number: 089, Training: Loss: 0.0589, Accuracy: 0.9583\n",
      "Batch number: 090, Training: Loss: 0.2259, Accuracy: 0.9583\n",
      "Batch number: 091, Training: Loss: 0.0731, Accuracy: 1.0000\n",
      "Batch number: 092, Training: Loss: 0.3271, Accuracy: 0.8750\n",
      "Batch number: 093, Training: Loss: 1.0996, Accuracy: 0.7500\n",
      "Batch number: 094, Training: Loss: 0.1043, Accuracy: 0.9583\n",
      "Batch number: 095, Training: Loss: 0.1501, Accuracy: 1.0000\n",
      "Batch number: 096, Training: Loss: 0.1907, Accuracy: 0.9167\n",
      "Batch number: 097, Training: Loss: 0.0939, Accuracy: 1.0000\n",
      "Batch number: 098, Training: Loss: 0.4544, Accuracy: 0.8333\n",
      "Batch number: 099, Training: Loss: 0.1455, Accuracy: 0.9583\n",
      "Batch number: 100, Training: Loss: 0.2057, Accuracy: 0.9167\n",
      "Batch number: 101, Training: Loss: 0.0963, Accuracy: 0.9583\n",
      "Batch number: 102, Training: Loss: 0.4021, Accuracy: 0.7500\n",
      "Batch number: 103, Training: Loss: 0.1267, Accuracy: 0.9583\n",
      "Batch number: 104, Training: Loss: 0.2108, Accuracy: 0.8750\n",
      "Batch number: 105, Training: Loss: 0.1446, Accuracy: 0.9583\n",
      "Batch number: 106, Training: Loss: 0.1998, Accuracy: 0.8750\n",
      "Batch number: 107, Training: Loss: 0.1343, Accuracy: 0.9583\n",
      "Batch number: 108, Training: Loss: 0.0410, Accuracy: 1.0000\n",
      "Batch number: 109, Training: Loss: 0.1087, Accuracy: 0.9167\n",
      "Batch number: 110, Training: Loss: 0.0124, Accuracy: 1.0000\n",
      "Batch number: 111, Training: Loss: 0.4034, Accuracy: 0.9167\n",
      "Batch number: 112, Training: Loss: 0.2523, Accuracy: 0.9167\n",
      "Batch number: 113, Training: Loss: 0.3281, Accuracy: 0.9167\n",
      "Batch number: 114, Training: Loss: 0.0457, Accuracy: 1.0000\n",
      "Batch number: 115, Training: Loss: 0.0735, Accuracy: 1.0000\n",
      "Batch number: 116, Training: Loss: 0.1506, Accuracy: 0.9167\n",
      "Batch number: 117, Training: Loss: 0.5440, Accuracy: 0.8333\n",
      "Batch number: 118, Training: Loss: 0.0959, Accuracy: 0.9583\n",
      "Batch number: 119, Training: Loss: 0.1270, Accuracy: 0.9167\n",
      "Batch number: 120, Training: Loss: 0.4337, Accuracy: 0.8750\n",
      "Batch number: 121, Training: Loss: 0.1098, Accuracy: 0.9167\n",
      "Batch number: 122, Training: Loss: 0.1013, Accuracy: 0.9583\n",
      "Batch number: 123, Training: Loss: 0.0514, Accuracy: 1.0000\n",
      "Batch number: 124, Training: Loss: 0.1148, Accuracy: 0.9167\n",
      "Batch number: 125, Training: Loss: 0.1528, Accuracy: 0.9167\n",
      "Batch number: 126, Training: Loss: 0.1818, Accuracy: 0.9583\n",
      "Batch number: 127, Training: Loss: 0.3223, Accuracy: 0.8750\n",
      "Batch number: 128, Training: Loss: 0.0469, Accuracy: 1.0000\n",
      "Batch number: 129, Training: Loss: 0.0735, Accuracy: 0.9583\n",
      "Batch number: 130, Training: Loss: 0.0311, Accuracy: 1.0000\n",
      "Batch number: 131, Training: Loss: 0.3023, Accuracy: 0.8750\n",
      "Batch number: 132, Training: Loss: 0.0659, Accuracy: 1.0000\n",
      "Batch number: 133, Training: Loss: 0.1244, Accuracy: 0.9583\n",
      "Batch number: 134, Training: Loss: 0.0756, Accuracy: 1.0000\n",
      "Batch number: 135, Training: Loss: 0.2072, Accuracy: 0.9167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number: 136, Training: Loss: 0.1324, Accuracy: 0.9583\n",
      "Batch number: 137, Training: Loss: 0.0590, Accuracy: 1.0000\n",
      "Batch number: 138, Training: Loss: 0.1230, Accuracy: 0.9167\n",
      "Batch number: 139, Training: Loss: 0.2312, Accuracy: 0.9167\n",
      "Batch number: 140, Training: Loss: 0.0945, Accuracy: 0.9583\n",
      "Batch number: 141, Training: Loss: 0.1519, Accuracy: 0.9167\n",
      "Batch number: 142, Training: Loss: 0.0935, Accuracy: 1.0000\n",
      "Batch number: 143, Training: Loss: 0.0680, Accuracy: 1.0000\n",
      "Batch number: 144, Training: Loss: 0.1636, Accuracy: 0.8333\n",
      "Batch number: 145, Training: Loss: 0.0929, Accuracy: 0.9583\n",
      "Batch number: 146, Training: Loss: 0.2123, Accuracy: 0.8750\n",
      "Batch number: 147, Training: Loss: 0.1279, Accuracy: 0.9167\n",
      "Batch number: 148, Training: Loss: 0.1828, Accuracy: 0.8750\n",
      "Batch number: 149, Training: Loss: 0.0933, Accuracy: 0.9583\n",
      "Batch number: 150, Training: Loss: 0.1127, Accuracy: 0.9583\n",
      "Batch number: 151, Training: Loss: 0.1275, Accuracy: 0.9583\n",
      "Batch number: 152, Training: Loss: 0.2402, Accuracy: 0.8750\n",
      "Batch number: 153, Training: Loss: 0.0722, Accuracy: 1.0000\n",
      "Batch number: 154, Training: Loss: 0.0709, Accuracy: 0.9583\n",
      "Batch number: 155, Training: Loss: 0.0790, Accuracy: 1.0000\n",
      "Batch number: 156, Training: Loss: 0.3637, Accuracy: 0.8750\n",
      "Batch number: 157, Training: Loss: 0.2882, Accuracy: 0.9583\n",
      "Batch number: 158, Training: Loss: 0.2214, Accuracy: 0.9167\n",
      "Batch number: 159, Training: Loss: 0.0701, Accuracy: 1.0000\n",
      "Batch number: 160, Training: Loss: 0.3546, Accuracy: 0.8750\n",
      "Batch number: 161, Training: Loss: 0.0884, Accuracy: 0.9583\n",
      "Batch number: 162, Training: Loss: 0.1459, Accuracy: 0.9583\n",
      "Batch number: 163, Training: Loss: 0.1199, Accuracy: 0.9583\n",
      "Batch number: 164, Training: Loss: 0.3206, Accuracy: 0.8750\n",
      "Batch number: 165, Training: Loss: 0.1959, Accuracy: 0.9167\n",
      "Batch number: 166, Training: Loss: 0.0244, Accuracy: 1.0000\n",
      "Batch number: 167, Training: Loss: 0.0576, Accuracy: 1.0000\n",
      "Batch number: 168, Training: Loss: 0.1850, Accuracy: 0.8333\n",
      "Batch number: 169, Training: Loss: 0.0452, Accuracy: 1.0000\n",
      "Batch number: 170, Training: Loss: 0.0433, Accuracy: 1.0000\n",
      "Batch number: 171, Training: Loss: 0.1862, Accuracy: 0.9583\n",
      "Batch number: 172, Training: Loss: 0.2868, Accuracy: 0.8750\n",
      "Batch number: 173, Training: Loss: 0.1888, Accuracy: 0.9167\n",
      "Batch number: 174, Training: Loss: 0.1947, Accuracy: 0.8750\n",
      "Batch number: 175, Training: Loss: 0.0700, Accuracy: 0.9583\n",
      "Batch number: 176, Training: Loss: 0.2084, Accuracy: 0.9167\n",
      "Batch number: 177, Training: Loss: 0.0558, Accuracy: 1.0000\n",
      "Batch number: 178, Training: Loss: 0.2218, Accuracy: 0.9583\n",
      "Batch number: 179, Training: Loss: 0.1259, Accuracy: 0.9167\n",
      "Batch number: 180, Training: Loss: 0.1057, Accuracy: 0.9583\n",
      "Batch number: 181, Training: Loss: 0.0414, Accuracy: 1.0000\n",
      "Batch number: 182, Training: Loss: 0.1829, Accuracy: 0.8750\n",
      "Batch number: 183, Training: Loss: 0.0543, Accuracy: 1.0000\n",
      "Batch number: 184, Training: Loss: 0.1367, Accuracy: 0.8750\n",
      "Batch number: 185, Training: Loss: 0.1086, Accuracy: 0.9583\n",
      "Batch number: 186, Training: Loss: 0.4315, Accuracy: 0.8333\n",
      "Batch number: 187, Training: Loss: 0.0797, Accuracy: 0.9583\n",
      "Batch number: 188, Training: Loss: 0.0729, Accuracy: 0.9583\n",
      "Batch number: 189, Training: Loss: 0.0290, Accuracy: 1.0000\n",
      "Batch number: 190, Training: Loss: 0.5136, Accuracy: 0.9167\n",
      "Batch number: 191, Training: Loss: 0.7494, Accuracy: 0.7895\n",
      "Epoch: 7/150\n",
      "Batch number: 000, Training: Loss: 0.6170, Accuracy: 0.8333\n",
      "Batch number: 001, Training: Loss: 0.5290, Accuracy: 0.8750\n",
      "Batch number: 002, Training: Loss: 0.2311, Accuracy: 0.8750\n",
      "Batch number: 003, Training: Loss: 0.2436, Accuracy: 0.8750\n",
      "Batch number: 004, Training: Loss: 0.1151, Accuracy: 0.9167\n",
      "Batch number: 005, Training: Loss: 0.1234, Accuracy: 0.9583\n",
      "Batch number: 006, Training: Loss: 0.0790, Accuracy: 0.9583\n",
      "Batch number: 007, Training: Loss: 0.1003, Accuracy: 1.0000\n",
      "Batch number: 008, Training: Loss: 0.1239, Accuracy: 0.9583\n",
      "Batch number: 009, Training: Loss: 0.1491, Accuracy: 0.9167\n",
      "Batch number: 010, Training: Loss: 0.1275, Accuracy: 0.9167\n",
      "Batch number: 011, Training: Loss: 0.1670, Accuracy: 0.9167\n",
      "Batch number: 012, Training: Loss: 0.1479, Accuracy: 0.9167\n",
      "Batch number: 013, Training: Loss: 0.0847, Accuracy: 0.9583\n",
      "Batch number: 014, Training: Loss: 0.1137, Accuracy: 0.9583\n",
      "Batch number: 015, Training: Loss: 0.1429, Accuracy: 0.9167\n",
      "Batch number: 016, Training: Loss: 0.3020, Accuracy: 0.8750\n",
      "Batch number: 017, Training: Loss: 0.1856, Accuracy: 0.9583\n",
      "Batch number: 018, Training: Loss: 0.1862, Accuracy: 0.8333\n",
      "Batch number: 019, Training: Loss: 0.4395, Accuracy: 0.7917\n",
      "Batch number: 020, Training: Loss: 0.1734, Accuracy: 0.9167\n",
      "Batch number: 021, Training: Loss: 0.1141, Accuracy: 0.9167\n",
      "Batch number: 022, Training: Loss: 0.1119, Accuracy: 0.8750\n",
      "Batch number: 023, Training: Loss: 0.2729, Accuracy: 0.9167\n",
      "Batch number: 024, Training: Loss: 0.1445, Accuracy: 0.9167\n",
      "Batch number: 025, Training: Loss: 0.1225, Accuracy: 0.9583\n",
      "Batch number: 026, Training: Loss: 0.2011, Accuracy: 0.8750\n",
      "Batch number: 027, Training: Loss: 0.1863, Accuracy: 0.9167\n",
      "Batch number: 028, Training: Loss: 0.0875, Accuracy: 0.9583\n",
      "Batch number: 029, Training: Loss: 0.1299, Accuracy: 0.9167\n",
      "Batch number: 030, Training: Loss: 0.1237, Accuracy: 0.9167\n",
      "Batch number: 031, Training: Loss: 0.0334, Accuracy: 1.0000\n",
      "Batch number: 032, Training: Loss: 0.0386, Accuracy: 1.0000\n",
      "Batch number: 033, Training: Loss: 0.0216, Accuracy: 1.0000\n",
      "Batch number: 034, Training: Loss: 0.0815, Accuracy: 1.0000\n",
      "Batch number: 035, Training: Loss: 0.0325, Accuracy: 1.0000\n",
      "Batch number: 036, Training: Loss: 0.3229, Accuracy: 0.9167\n",
      "Batch number: 037, Training: Loss: 0.1006, Accuracy: 0.9167\n",
      "Batch number: 038, Training: Loss: 0.0976, Accuracy: 1.0000\n",
      "Batch number: 039, Training: Loss: 0.1526, Accuracy: 0.9167\n",
      "Batch number: 040, Training: Loss: 0.1776, Accuracy: 0.9583\n",
      "Batch number: 041, Training: Loss: 0.3564, Accuracy: 0.9583\n",
      "Batch number: 042, Training: Loss: 0.0448, Accuracy: 1.0000\n",
      "Batch number: 043, Training: Loss: 0.1353, Accuracy: 0.9167\n",
      "Batch number: 044, Training: Loss: 0.0838, Accuracy: 0.9583\n",
      "Batch number: 045, Training: Loss: 0.1577, Accuracy: 0.9583\n",
      "Batch number: 046, Training: Loss: 0.0371, Accuracy: 1.0000\n",
      "Batch number: 047, Training: Loss: 0.0492, Accuracy: 1.0000\n",
      "Batch number: 048, Training: Loss: 0.1908, Accuracy: 0.9583\n",
      "Batch number: 049, Training: Loss: 0.3454, Accuracy: 0.7917\n",
      "Batch number: 050, Training: Loss: 0.2591, Accuracy: 0.8333\n",
      "Batch number: 051, Training: Loss: 0.2162, Accuracy: 0.8750\n",
      "Batch number: 052, Training: Loss: 0.0413, Accuracy: 1.0000\n",
      "Batch number: 053, Training: Loss: 0.1356, Accuracy: 0.9167\n",
      "Batch number: 054, Training: Loss: 0.1376, Accuracy: 0.8750\n",
      "Batch number: 055, Training: Loss: 0.1329, Accuracy: 0.9583\n",
      "Batch number: 056, Training: Loss: 0.2227, Accuracy: 0.8750\n",
      "Batch number: 057, Training: Loss: 0.1136, Accuracy: 0.9583\n",
      "Batch number: 058, Training: Loss: 0.0513, Accuracy: 1.0000\n",
      "Batch number: 059, Training: Loss: 0.1087, Accuracy: 0.9167\n",
      "Batch number: 060, Training: Loss: 0.0698, Accuracy: 1.0000\n",
      "Batch number: 061, Training: Loss: 0.0658, Accuracy: 1.0000\n",
      "Batch number: 062, Training: Loss: 0.1094, Accuracy: 0.9583\n",
      "Batch number: 063, Training: Loss: 0.0650, Accuracy: 0.9583\n",
      "Batch number: 064, Training: Loss: 0.0839, Accuracy: 0.9583\n",
      "Batch number: 065, Training: Loss: 0.3471, Accuracy: 0.8750\n",
      "Batch number: 066, Training: Loss: 0.1418, Accuracy: 1.0000\n",
      "Batch number: 067, Training: Loss: 0.4531, Accuracy: 0.9167\n",
      "Batch number: 068, Training: Loss: 0.0859, Accuracy: 0.9583\n",
      "Batch number: 069, Training: Loss: 0.1461, Accuracy: 0.9583\n",
      "Batch number: 070, Training: Loss: 0.2198, Accuracy: 0.9167\n",
      "Batch number: 071, Training: Loss: 0.1800, Accuracy: 0.8750\n",
      "Batch number: 072, Training: Loss: 0.0642, Accuracy: 1.0000\n",
      "Batch number: 073, Training: Loss: 0.1700, Accuracy: 0.9167\n",
      "Batch number: 074, Training: Loss: 0.0618, Accuracy: 1.0000\n",
      "Batch number: 075, Training: Loss: 0.2996, Accuracy: 0.8333\n",
      "Batch number: 076, Training: Loss: 0.0812, Accuracy: 0.9167\n",
      "Batch number: 077, Training: Loss: 0.1209, Accuracy: 0.9583\n",
      "Batch number: 078, Training: Loss: 0.2019, Accuracy: 0.9167\n",
      "Batch number: 079, Training: Loss: 0.0812, Accuracy: 0.9583\n",
      "Batch number: 080, Training: Loss: 0.1388, Accuracy: 0.9583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number: 081, Training: Loss: 0.0419, Accuracy: 1.0000\n",
      "Batch number: 082, Training: Loss: 0.0860, Accuracy: 1.0000\n",
      "Batch number: 083, Training: Loss: 0.0454, Accuracy: 1.0000\n",
      "Batch number: 084, Training: Loss: 0.1210, Accuracy: 0.9167\n",
      "Batch number: 085, Training: Loss: 0.3431, Accuracy: 0.8750\n",
      "Batch number: 086, Training: Loss: 0.0414, Accuracy: 1.0000\n",
      "Batch number: 087, Training: Loss: 0.0501, Accuracy: 1.0000\n",
      "Batch number: 088, Training: Loss: 0.0739, Accuracy: 1.0000\n",
      "Batch number: 089, Training: Loss: 0.3377, Accuracy: 0.8750\n",
      "Batch number: 090, Training: Loss: 0.1341, Accuracy: 0.9583\n",
      "Batch number: 091, Training: Loss: 0.0531, Accuracy: 1.0000\n",
      "Batch number: 092, Training: Loss: 0.4216, Accuracy: 0.9167\n",
      "Batch number: 093, Training: Loss: 0.0970, Accuracy: 0.9167\n",
      "Batch number: 094, Training: Loss: 0.1655, Accuracy: 0.9167\n",
      "Batch number: 095, Training: Loss: 0.1053, Accuracy: 0.9583\n",
      "Batch number: 096, Training: Loss: 0.1415, Accuracy: 0.9583\n",
      "Batch number: 097, Training: Loss: 0.1476, Accuracy: 0.9583\n",
      "Batch number: 098, Training: Loss: 0.0780, Accuracy: 0.9583\n",
      "Batch number: 099, Training: Loss: 0.3047, Accuracy: 0.9583\n",
      "Batch number: 100, Training: Loss: 0.0695, Accuracy: 0.9583\n",
      "Batch number: 101, Training: Loss: 0.0653, Accuracy: 0.9583\n",
      "Batch number: 102, Training: Loss: 0.2244, Accuracy: 0.9583\n",
      "Batch number: 103, Training: Loss: 0.0965, Accuracy: 0.9167\n",
      "Batch number: 104, Training: Loss: 0.2287, Accuracy: 0.9167\n",
      "Batch number: 105, Training: Loss: 0.0300, Accuracy: 1.0000\n",
      "Batch number: 106, Training: Loss: 0.0663, Accuracy: 1.0000\n",
      "Batch number: 107, Training: Loss: 0.0588, Accuracy: 1.0000\n",
      "Batch number: 108, Training: Loss: 0.0348, Accuracy: 1.0000\n",
      "Batch number: 109, Training: Loss: 0.3959, Accuracy: 0.9583\n",
      "Batch number: 110, Training: Loss: 0.0384, Accuracy: 1.0000\n",
      "Batch number: 111, Training: Loss: 0.0820, Accuracy: 0.9583\n",
      "Batch number: 112, Training: Loss: 0.1658, Accuracy: 0.9583\n",
      "Batch number: 113, Training: Loss: 0.2041, Accuracy: 0.9167\n",
      "Batch number: 114, Training: Loss: 0.0498, Accuracy: 1.0000\n",
      "Batch number: 115, Training: Loss: 0.0675, Accuracy: 0.9583\n",
      "Batch number: 116, Training: Loss: 0.1124, Accuracy: 1.0000\n",
      "Batch number: 117, Training: Loss: 0.0778, Accuracy: 0.9583\n",
      "Batch number: 118, Training: Loss: 0.2962, Accuracy: 0.9167\n",
      "Batch number: 119, Training: Loss: 0.1501, Accuracy: 0.9583\n",
      "Batch number: 120, Training: Loss: 0.1406, Accuracy: 0.9583\n",
      "Batch number: 121, Training: Loss: 0.1831, Accuracy: 0.8750\n",
      "Batch number: 122, Training: Loss: 0.1418, Accuracy: 0.9167\n",
      "Batch number: 123, Training: Loss: 0.0838, Accuracy: 0.9167\n",
      "Batch number: 124, Training: Loss: 0.0420, Accuracy: 1.0000\n",
      "Batch number: 125, Training: Loss: 0.1383, Accuracy: 0.9167\n",
      "Batch number: 126, Training: Loss: 0.1308, Accuracy: 0.9167\n",
      "Batch number: 127, Training: Loss: 0.1161, Accuracy: 0.9167\n",
      "Batch number: 128, Training: Loss: 0.1694, Accuracy: 0.8750\n",
      "Batch number: 129, Training: Loss: 0.0771, Accuracy: 1.0000\n",
      "Batch number: 130, Training: Loss: 0.0607, Accuracy: 1.0000\n",
      "Batch number: 131, Training: Loss: 0.1386, Accuracy: 0.9583\n",
      "Batch number: 132, Training: Loss: 0.2871, Accuracy: 0.9167\n",
      "Batch number: 133, Training: Loss: 0.0494, Accuracy: 1.0000\n",
      "Batch number: 134, Training: Loss: 0.1944, Accuracy: 0.9167\n",
      "Batch number: 135, Training: Loss: 0.1137, Accuracy: 0.9167\n",
      "Batch number: 136, Training: Loss: 0.0275, Accuracy: 1.0000\n",
      "Batch number: 137, Training: Loss: 0.1355, Accuracy: 0.9583\n",
      "Batch number: 138, Training: Loss: 0.3610, Accuracy: 0.9167\n",
      "Batch number: 139, Training: Loss: 0.1201, Accuracy: 0.9167\n",
      "Batch number: 140, Training: Loss: 0.0578, Accuracy: 0.9583\n",
      "Batch number: 141, Training: Loss: 0.0378, Accuracy: 1.0000\n",
      "Batch number: 142, Training: Loss: 0.0859, Accuracy: 0.9583\n",
      "Batch number: 143, Training: Loss: 0.2449, Accuracy: 0.9167\n",
      "Batch number: 144, Training: Loss: 0.0315, Accuracy: 1.0000\n",
      "Batch number: 145, Training: Loss: 0.0200, Accuracy: 1.0000\n",
      "Batch number: 146, Training: Loss: 0.1410, Accuracy: 0.9167\n",
      "Batch number: 147, Training: Loss: 0.6158, Accuracy: 0.7500\n",
      "Batch number: 148, Training: Loss: 0.0827, Accuracy: 0.9583\n",
      "Batch number: 149, Training: Loss: 0.0641, Accuracy: 0.9583\n",
      "Batch number: 150, Training: Loss: 0.1640, Accuracy: 0.9167\n",
      "Batch number: 151, Training: Loss: 0.0649, Accuracy: 1.0000\n",
      "Batch number: 152, Training: Loss: 0.1228, Accuracy: 0.9583\n",
      "Batch number: 153, Training: Loss: 0.0432, Accuracy: 1.0000\n",
      "Batch number: 154, Training: Loss: 0.4305, Accuracy: 0.7500\n",
      "Batch number: 155, Training: Loss: 0.3952, Accuracy: 0.9583\n",
      "Batch number: 156, Training: Loss: 0.0255, Accuracy: 1.0000\n",
      "Batch number: 157, Training: Loss: 0.2401, Accuracy: 0.9167\n",
      "Batch number: 158, Training: Loss: 0.1164, Accuracy: 1.0000\n",
      "Batch number: 159, Training: Loss: 0.1005, Accuracy: 0.9583\n",
      "Batch number: 160, Training: Loss: 0.0584, Accuracy: 0.9167\n",
      "Batch number: 161, Training: Loss: 0.0695, Accuracy: 0.9167\n",
      "Batch number: 162, Training: Loss: 0.0627, Accuracy: 1.0000\n",
      "Batch number: 163, Training: Loss: 0.1685, Accuracy: 0.9583\n",
      "Batch number: 164, Training: Loss: 0.1907, Accuracy: 0.8750\n",
      "Batch number: 165, Training: Loss: 0.1016, Accuracy: 0.9583\n",
      "Batch number: 166, Training: Loss: 0.1276, Accuracy: 0.9583\n",
      "Batch number: 167, Training: Loss: 0.2432, Accuracy: 0.8333\n",
      "Batch number: 168, Training: Loss: 0.2999, Accuracy: 0.8750\n",
      "Batch number: 169, Training: Loss: 0.1351, Accuracy: 0.9167\n",
      "Batch number: 170, Training: Loss: 0.0979, Accuracy: 0.9583\n",
      "Batch number: 171, Training: Loss: 0.0380, Accuracy: 1.0000\n",
      "Batch number: 172, Training: Loss: 0.0185, Accuracy: 1.0000\n",
      "Batch number: 173, Training: Loss: 0.2914, Accuracy: 0.9583\n",
      "Batch number: 174, Training: Loss: 0.0765, Accuracy: 1.0000\n",
      "Batch number: 175, Training: Loss: 0.2544, Accuracy: 0.9167\n",
      "Batch number: 176, Training: Loss: 0.1914, Accuracy: 0.8750\n",
      "Batch number: 177, Training: Loss: 0.1906, Accuracy: 0.9583\n",
      "Batch number: 178, Training: Loss: 0.0409, Accuracy: 0.9583\n",
      "Batch number: 179, Training: Loss: 0.1798, Accuracy: 0.9167\n",
      "Batch number: 180, Training: Loss: 0.0568, Accuracy: 1.0000\n",
      "Batch number: 181, Training: Loss: 0.3774, Accuracy: 0.7917\n",
      "Batch number: 182, Training: Loss: 0.1421, Accuracy: 0.9583\n",
      "Batch number: 183, Training: Loss: 0.0810, Accuracy: 0.9583\n",
      "Batch number: 184, Training: Loss: 0.1294, Accuracy: 0.9583\n",
      "Batch number: 185, Training: Loss: 0.2892, Accuracy: 0.7917\n",
      "Batch number: 186, Training: Loss: 0.2176, Accuracy: 0.8333\n",
      "Batch number: 187, Training: Loss: 0.1368, Accuracy: 0.9167\n",
      "Batch number: 188, Training: Loss: 0.1363, Accuracy: 0.9583\n",
      "Batch number: 189, Training: Loss: 0.0657, Accuracy: 1.0000\n",
      "Batch number: 190, Training: Loss: 0.2285, Accuracy: 0.9167\n",
      "Batch number: 191, Training: Loss: 0.0643, Accuracy: 1.0000\n",
      "Epoch: 8/150\n",
      "Batch number: 000, Training: Loss: 0.0860, Accuracy: 1.0000\n",
      "Batch number: 001, Training: Loss: 0.1342, Accuracy: 0.9583\n",
      "Batch number: 002, Training: Loss: 0.2399, Accuracy: 0.9167\n",
      "Batch number: 003, Training: Loss: 0.1186, Accuracy: 0.9167\n",
      "Batch number: 004, Training: Loss: 0.0747, Accuracy: 1.0000\n",
      "Batch number: 005, Training: Loss: 0.2999, Accuracy: 0.9167\n",
      "Batch number: 006, Training: Loss: 0.1065, Accuracy: 1.0000\n",
      "Batch number: 007, Training: Loss: 0.1054, Accuracy: 0.9583\n",
      "Batch number: 008, Training: Loss: 0.2089, Accuracy: 0.9583\n",
      "Batch number: 009, Training: Loss: 0.1894, Accuracy: 0.9583\n",
      "Batch number: 010, Training: Loss: 0.1813, Accuracy: 0.9167\n",
      "Batch number: 011, Training: Loss: 0.0927, Accuracy: 1.0000\n",
      "Batch number: 012, Training: Loss: 0.1975, Accuracy: 0.8750\n",
      "Batch number: 013, Training: Loss: 0.0515, Accuracy: 1.0000\n",
      "Batch number: 014, Training: Loss: 0.0456, Accuracy: 1.0000\n",
      "Batch number: 015, Training: Loss: 0.0606, Accuracy: 1.0000\n",
      "Batch number: 016, Training: Loss: 0.0440, Accuracy: 1.0000\n",
      "Batch number: 017, Training: Loss: 0.1336, Accuracy: 0.9167\n",
      "Batch number: 018, Training: Loss: 0.1881, Accuracy: 0.9167\n",
      "Batch number: 019, Training: Loss: 0.0596, Accuracy: 1.0000\n",
      "Batch number: 020, Training: Loss: 0.1593, Accuracy: 0.8750\n",
      "Batch number: 021, Training: Loss: 0.1051, Accuracy: 0.9167\n",
      "Batch number: 022, Training: Loss: 0.0618, Accuracy: 0.9583\n",
      "Batch number: 023, Training: Loss: 0.1068, Accuracy: 0.9583\n",
      "Batch number: 024, Training: Loss: 0.1093, Accuracy: 0.9167\n",
      "Batch number: 025, Training: Loss: 0.1457, Accuracy: 0.9167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number: 026, Training: Loss: 0.2023, Accuracy: 0.9167\n",
      "Batch number: 027, Training: Loss: 0.1645, Accuracy: 0.9583\n",
      "Batch number: 028, Training: Loss: 0.0854, Accuracy: 0.9583\n",
      "Batch number: 029, Training: Loss: 0.1742, Accuracy: 0.9167\n",
      "Batch number: 030, Training: Loss: 0.0403, Accuracy: 1.0000\n",
      "Batch number: 031, Training: Loss: 0.1327, Accuracy: 0.9167\n",
      "Batch number: 032, Training: Loss: 0.2146, Accuracy: 0.9167\n",
      "Batch number: 033, Training: Loss: 0.1296, Accuracy: 0.9583\n",
      "Batch number: 034, Training: Loss: 0.1403, Accuracy: 0.9583\n",
      "Batch number: 035, Training: Loss: 0.3049, Accuracy: 0.9167\n",
      "Batch number: 036, Training: Loss: 0.2975, Accuracy: 0.8750\n",
      "Batch number: 037, Training: Loss: 0.0145, Accuracy: 1.0000\n",
      "Batch number: 038, Training: Loss: 0.1880, Accuracy: 0.9167\n",
      "Batch number: 039, Training: Loss: 0.1045, Accuracy: 1.0000\n",
      "Batch number: 040, Training: Loss: 0.0330, Accuracy: 1.0000\n",
      "Batch number: 041, Training: Loss: 0.2339, Accuracy: 0.9167\n",
      "Batch number: 042, Training: Loss: 0.1424, Accuracy: 0.8750\n",
      "Batch number: 043, Training: Loss: 0.0404, Accuracy: 1.0000\n",
      "Batch number: 044, Training: Loss: 0.5899, Accuracy: 0.8333\n",
      "Batch number: 045, Training: Loss: 0.0690, Accuracy: 1.0000\n",
      "Batch number: 046, Training: Loss: 0.1275, Accuracy: 0.9583\n",
      "Batch number: 047, Training: Loss: 0.0803, Accuracy: 0.9583\n",
      "Batch number: 048, Training: Loss: 0.1920, Accuracy: 0.9583\n",
      "Batch number: 049, Training: Loss: 0.3604, Accuracy: 0.7917\n",
      "Batch number: 050, Training: Loss: 0.3757, Accuracy: 0.8750\n",
      "Batch number: 051, Training: Loss: 0.0963, Accuracy: 0.9583\n",
      "Batch number: 052, Training: Loss: 0.1170, Accuracy: 0.9583\n",
      "Batch number: 053, Training: Loss: 0.1011, Accuracy: 0.9167\n",
      "Batch number: 054, Training: Loss: 0.2904, Accuracy: 0.8333\n",
      "Batch number: 055, Training: Loss: 0.5241, Accuracy: 0.8333\n",
      "Batch number: 056, Training: Loss: 0.0476, Accuracy: 1.0000\n",
      "Batch number: 057, Training: Loss: 0.1084, Accuracy: 0.9583\n",
      "Batch number: 058, Training: Loss: 0.0363, Accuracy: 1.0000\n",
      "Batch number: 059, Training: Loss: 0.0474, Accuracy: 1.0000\n",
      "Batch number: 060, Training: Loss: 0.2876, Accuracy: 0.8333\n",
      "Batch number: 061, Training: Loss: 0.0804, Accuracy: 1.0000\n",
      "Batch number: 062, Training: Loss: 0.1049, Accuracy: 0.9167\n",
      "Batch number: 063, Training: Loss: 0.0545, Accuracy: 1.0000\n",
      "Batch number: 064, Training: Loss: 0.4353, Accuracy: 0.8750\n",
      "Batch number: 065, Training: Loss: 0.3986, Accuracy: 0.8750\n",
      "Batch number: 066, Training: Loss: 0.1516, Accuracy: 0.9583\n",
      "Batch number: 067, Training: Loss: 0.1712, Accuracy: 0.9167\n",
      "Batch number: 068, Training: Loss: 0.1217, Accuracy: 0.9167\n",
      "Batch number: 069, Training: Loss: 0.0916, Accuracy: 0.9167\n",
      "Batch number: 070, Training: Loss: 0.1187, Accuracy: 0.9583\n",
      "Batch number: 071, Training: Loss: 0.1227, Accuracy: 0.9167\n",
      "Batch number: 072, Training: Loss: 0.0965, Accuracy: 0.9583\n",
      "Batch number: 073, Training: Loss: 0.1422, Accuracy: 0.9167\n",
      "Batch number: 074, Training: Loss: 0.1010, Accuracy: 0.9583\n",
      "Batch number: 075, Training: Loss: 0.2779, Accuracy: 0.8750\n",
      "Batch number: 076, Training: Loss: 0.1098, Accuracy: 0.9583\n",
      "Batch number: 077, Training: Loss: 0.1753, Accuracy: 0.9583\n",
      "Batch number: 078, Training: Loss: 0.1605, Accuracy: 0.8750\n",
      "Batch number: 079, Training: Loss: 0.1186, Accuracy: 0.9583\n",
      "Batch number: 080, Training: Loss: 0.0937, Accuracy: 1.0000\n",
      "Batch number: 081, Training: Loss: 0.0749, Accuracy: 1.0000\n",
      "Batch number: 082, Training: Loss: 0.6756, Accuracy: 0.7917\n",
      "Batch number: 083, Training: Loss: 0.0947, Accuracy: 0.9583\n",
      "Batch number: 084, Training: Loss: 0.1132, Accuracy: 0.9583\n",
      "Batch number: 085, Training: Loss: 0.1308, Accuracy: 0.9167\n",
      "Batch number: 086, Training: Loss: 0.1623, Accuracy: 0.9167\n",
      "Batch number: 087, Training: Loss: 0.3374, Accuracy: 0.9583\n",
      "Batch number: 088, Training: Loss: 0.1116, Accuracy: 1.0000\n",
      "Batch number: 089, Training: Loss: 0.1545, Accuracy: 0.9583\n",
      "Batch number: 090, Training: Loss: 0.2058, Accuracy: 0.9583\n",
      "Batch number: 091, Training: Loss: 0.0427, Accuracy: 0.9583\n",
      "Batch number: 092, Training: Loss: 0.1158, Accuracy: 0.9583\n",
      "Batch number: 093, Training: Loss: 0.0637, Accuracy: 0.9583\n",
      "Batch number: 094, Training: Loss: 0.1225, Accuracy: 0.9167\n",
      "Batch number: 095, Training: Loss: 0.1676, Accuracy: 0.9167\n",
      "Batch number: 096, Training: Loss: 0.0930, Accuracy: 0.9583\n",
      "Batch number: 097, Training: Loss: 0.1707, Accuracy: 0.9167\n",
      "Batch number: 098, Training: Loss: 0.1651, Accuracy: 0.9167\n",
      "Batch number: 099, Training: Loss: 0.1517, Accuracy: 1.0000\n",
      "Batch number: 100, Training: Loss: 0.1926, Accuracy: 0.9167\n",
      "Batch number: 101, Training: Loss: 0.0552, Accuracy: 1.0000\n",
      "Batch number: 102, Training: Loss: 0.5502, Accuracy: 0.7917\n",
      "Batch number: 103, Training: Loss: 0.1323, Accuracy: 0.9583\n",
      "Batch number: 104, Training: Loss: 0.1079, Accuracy: 0.9583\n",
      "Batch number: 105, Training: Loss: 0.0658, Accuracy: 1.0000\n",
      "Batch number: 106, Training: Loss: 0.0460, Accuracy: 1.0000\n",
      "Batch number: 107, Training: Loss: 0.0510, Accuracy: 0.9583\n",
      "Batch number: 108, Training: Loss: 0.2720, Accuracy: 0.8750\n",
      "Batch number: 109, Training: Loss: 0.1530, Accuracy: 0.8333\n",
      "Batch number: 110, Training: Loss: 0.1068, Accuracy: 0.9583\n",
      "Batch number: 111, Training: Loss: 0.3513, Accuracy: 0.8333\n",
      "Batch number: 112, Training: Loss: 0.1768, Accuracy: 0.8750\n",
      "Batch number: 113, Training: Loss: 0.1925, Accuracy: 0.9167\n",
      "Batch number: 114, Training: Loss: 0.2303, Accuracy: 0.9167\n",
      "Batch number: 115, Training: Loss: 0.0812, Accuracy: 0.9583\n",
      "Batch number: 116, Training: Loss: 0.1030, Accuracy: 0.9583\n",
      "Batch number: 117, Training: Loss: 0.0591, Accuracy: 1.0000\n",
      "Batch number: 118, Training: Loss: 0.1297, Accuracy: 0.9583\n",
      "Batch number: 119, Training: Loss: 0.1508, Accuracy: 0.9167\n",
      "Batch number: 120, Training: Loss: 0.5279, Accuracy: 0.7500\n",
      "Batch number: 121, Training: Loss: 0.0591, Accuracy: 1.0000\n",
      "Batch number: 122, Training: Loss: 0.0607, Accuracy: 0.9583\n",
      "Batch number: 123, Training: Loss: 0.1864, Accuracy: 0.8750\n",
      "Batch number: 124, Training: Loss: 0.1398, Accuracy: 0.9583\n",
      "Batch number: 125, Training: Loss: 0.0672, Accuracy: 1.0000\n",
      "Batch number: 126, Training: Loss: 0.0824, Accuracy: 0.9583\n",
      "Batch number: 127, Training: Loss: 0.3369, Accuracy: 0.9167\n",
      "Batch number: 128, Training: Loss: 0.2886, Accuracy: 0.7500\n",
      "Batch number: 129, Training: Loss: 0.0920, Accuracy: 1.0000\n",
      "Batch number: 130, Training: Loss: 0.0855, Accuracy: 1.0000\n",
      "Batch number: 131, Training: Loss: 0.1445, Accuracy: 0.9167\n",
      "Batch number: 132, Training: Loss: 0.2648, Accuracy: 0.8750\n",
      "Batch number: 133, Training: Loss: 0.0528, Accuracy: 0.9583\n",
      "Batch number: 134, Training: Loss: 0.0685, Accuracy: 0.9583\n",
      "Batch number: 135, Training: Loss: 0.1921, Accuracy: 0.9167\n",
      "Batch number: 136, Training: Loss: 0.2497, Accuracy: 0.9167\n",
      "Batch number: 137, Training: Loss: 0.0814, Accuracy: 0.9583\n",
      "Batch number: 138, Training: Loss: 0.2122, Accuracy: 0.8750\n",
      "Batch number: 139, Training: Loss: 0.3182, Accuracy: 0.8333\n",
      "Batch number: 140, Training: Loss: 0.0410, Accuracy: 1.0000\n",
      "Batch number: 141, Training: Loss: 0.1996, Accuracy: 0.9167\n",
      "Batch number: 142, Training: Loss: 0.1167, Accuracy: 0.9167\n",
      "Batch number: 143, Training: Loss: 0.0713, Accuracy: 1.0000\n",
      "Batch number: 144, Training: Loss: 0.3728, Accuracy: 0.8750\n",
      "Batch number: 145, Training: Loss: 0.1355, Accuracy: 0.9583\n",
      "Batch number: 146, Training: Loss: 0.1687, Accuracy: 0.9583\n",
      "Batch number: 147, Training: Loss: 0.0919, Accuracy: 0.9167\n",
      "Batch number: 148, Training: Loss: 0.1095, Accuracy: 1.0000\n",
      "Batch number: 149, Training: Loss: 0.2325, Accuracy: 0.9167\n",
      "Batch number: 150, Training: Loss: 0.1526, Accuracy: 0.8750\n",
      "Batch number: 151, Training: Loss: 0.2774, Accuracy: 0.9583\n",
      "Batch number: 152, Training: Loss: 0.2046, Accuracy: 0.8750\n",
      "Batch number: 153, Training: Loss: 0.1410, Accuracy: 0.9167\n",
      "Batch number: 154, Training: Loss: 0.1647, Accuracy: 0.9167\n",
      "Batch number: 155, Training: Loss: 0.0904, Accuracy: 1.0000\n",
      "Batch number: 156, Training: Loss: 0.2333, Accuracy: 0.9583\n",
      "Batch number: 157, Training: Loss: 0.0754, Accuracy: 0.9583\n",
      "Batch number: 158, Training: Loss: 0.1379, Accuracy: 1.0000\n",
      "Batch number: 159, Training: Loss: 0.0978, Accuracy: 1.0000\n",
      "Batch number: 160, Training: Loss: 0.1820, Accuracy: 0.8750\n",
      "Batch number: 161, Training: Loss: 0.0417, Accuracy: 1.0000\n",
      "Batch number: 162, Training: Loss: 0.0351, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number: 163, Training: Loss: 0.2246, Accuracy: 0.9167\n",
      "Batch number: 164, Training: Loss: 0.0527, Accuracy: 1.0000\n",
      "Batch number: 165, Training: Loss: 0.0713, Accuracy: 1.0000\n",
      "Batch number: 166, Training: Loss: 0.1658, Accuracy: 0.9167\n",
      "Batch number: 167, Training: Loss: 0.3522, Accuracy: 0.7500\n",
      "Batch number: 168, Training: Loss: 0.0500, Accuracy: 1.0000\n",
      "Batch number: 169, Training: Loss: 0.0524, Accuracy: 1.0000\n",
      "Batch number: 170, Training: Loss: 0.2016, Accuracy: 0.8750\n",
      "Batch number: 171, Training: Loss: 0.0829, Accuracy: 1.0000\n",
      "Batch number: 172, Training: Loss: 0.0686, Accuracy: 1.0000\n",
      "Batch number: 173, Training: Loss: 0.2045, Accuracy: 0.9167\n",
      "Batch number: 174, Training: Loss: 0.0282, Accuracy: 1.0000\n",
      "Batch number: 175, Training: Loss: 0.1006, Accuracy: 0.9583\n",
      "Batch number: 176, Training: Loss: 0.4831, Accuracy: 0.8750\n",
      "Batch number: 177, Training: Loss: 0.1763, Accuracy: 0.9167\n",
      "Batch number: 178, Training: Loss: 0.1650, Accuracy: 0.9583\n",
      "Batch number: 179, Training: Loss: 0.1340, Accuracy: 1.0000\n",
      "Batch number: 180, Training: Loss: 0.0486, Accuracy: 1.0000\n",
      "Batch number: 181, Training: Loss: 0.2388, Accuracy: 0.8750\n",
      "Batch number: 182, Training: Loss: 0.1208, Accuracy: 0.9167\n",
      "Batch number: 183, Training: Loss: 0.1356, Accuracy: 0.8750\n",
      "Batch number: 184, Training: Loss: 0.1067, Accuracy: 0.9167\n",
      "Batch number: 185, Training: Loss: 0.1251, Accuracy: 0.8750\n",
      "Batch number: 186, Training: Loss: 0.1826, Accuracy: 0.8333\n",
      "Batch number: 187, Training: Loss: 0.1831, Accuracy: 0.8750\n",
      "Batch number: 188, Training: Loss: 0.1081, Accuracy: 0.8750\n",
      "Batch number: 189, Training: Loss: 0.0612, Accuracy: 1.0000\n",
      "Batch number: 190, Training: Loss: 0.0649, Accuracy: 0.9583\n",
      "Batch number: 191, Training: Loss: 0.0940, Accuracy: 0.9474\n",
      "Epoch: 9/150\n",
      "Batch number: 000, Training: Loss: 0.0244, Accuracy: 1.0000\n",
      "Batch number: 001, Training: Loss: 0.0433, Accuracy: 1.0000\n",
      "Batch number: 002, Training: Loss: 0.0622, Accuracy: 1.0000\n",
      "Batch number: 003, Training: Loss: 0.0311, Accuracy: 1.0000\n",
      "Batch number: 004, Training: Loss: 0.0949, Accuracy: 0.9583\n",
      "Batch number: 005, Training: Loss: 0.1391, Accuracy: 0.9583\n",
      "Batch number: 006, Training: Loss: 0.0555, Accuracy: 1.0000\n",
      "Batch number: 007, Training: Loss: 0.7059, Accuracy: 0.9167\n",
      "Batch number: 008, Training: Loss: 0.4856, Accuracy: 0.9167\n",
      "Batch number: 009, Training: Loss: 0.3230, Accuracy: 0.8750\n",
      "Batch number: 010, Training: Loss: 0.0573, Accuracy: 0.9583\n",
      "Batch number: 011, Training: Loss: 0.1170, Accuracy: 0.8750\n",
      "Batch number: 012, Training: Loss: 0.2398, Accuracy: 0.9167\n",
      "Batch number: 013, Training: Loss: 0.0473, Accuracy: 1.0000\n",
      "Batch number: 014, Training: Loss: 0.0730, Accuracy: 0.9583\n",
      "Batch number: 015, Training: Loss: 0.3127, Accuracy: 0.7917\n",
      "Batch number: 016, Training: Loss: 0.0997, Accuracy: 0.9583\n",
      "Batch number: 017, Training: Loss: 0.2219, Accuracy: 0.8750\n",
      "Batch number: 018, Training: Loss: 0.1512, Accuracy: 0.9167\n",
      "Batch number: 019, Training: Loss: 0.2619, Accuracy: 0.8750\n",
      "Batch number: 020, Training: Loss: 0.1001, Accuracy: 0.9583\n",
      "Batch number: 021, Training: Loss: 0.2817, Accuracy: 0.7917\n",
      "Batch number: 022, Training: Loss: 0.1295, Accuracy: 0.9583\n",
      "Batch number: 023, Training: Loss: 0.3039, Accuracy: 0.8333\n",
      "Batch number: 024, Training: Loss: 0.1485, Accuracy: 0.9583\n",
      "Batch number: 025, Training: Loss: 0.3175, Accuracy: 0.9167\n",
      "Batch number: 026, Training: Loss: 0.2758, Accuracy: 0.9167\n",
      "Batch number: 027, Training: Loss: 0.0944, Accuracy: 0.9583\n",
      "Batch number: 028, Training: Loss: 0.4082, Accuracy: 0.8333\n",
      "Batch number: 029, Training: Loss: 0.1621, Accuracy: 0.9583\n",
      "Batch number: 030, Training: Loss: 0.1081, Accuracy: 1.0000\n",
      "Batch number: 031, Training: Loss: 0.1450, Accuracy: 0.9167\n",
      "Batch number: 032, Training: Loss: 0.0843, Accuracy: 1.0000\n",
      "Batch number: 033, Training: Loss: 0.0716, Accuracy: 0.9167\n",
      "Batch number: 034, Training: Loss: 0.3233, Accuracy: 0.8333\n",
      "Batch number: 035, Training: Loss: 0.0633, Accuracy: 0.9583\n",
      "Batch number: 036, Training: Loss: 0.3930, Accuracy: 0.8333\n",
      "Batch number: 037, Training: Loss: 0.0590, Accuracy: 1.0000\n",
      "Batch number: 038, Training: Loss: 0.1542, Accuracy: 0.9167\n",
      "Batch number: 039, Training: Loss: 0.2781, Accuracy: 0.8750\n",
      "Batch number: 040, Training: Loss: 0.0929, Accuracy: 0.9167\n",
      "Batch number: 041, Training: Loss: 0.1264, Accuracy: 0.9167\n",
      "Batch number: 042, Training: Loss: 0.1876, Accuracy: 0.9167\n",
      "Batch number: 043, Training: Loss: 0.1386, Accuracy: 0.9583\n",
      "Batch number: 044, Training: Loss: 0.0656, Accuracy: 0.9583\n",
      "Batch number: 045, Training: Loss: 0.3400, Accuracy: 0.9583\n",
      "Batch number: 046, Training: Loss: 0.1392, Accuracy: 0.9583\n",
      "Batch number: 047, Training: Loss: 0.0966, Accuracy: 0.9583\n",
      "Batch number: 048, Training: Loss: 0.0145, Accuracy: 1.0000\n",
      "Batch number: 049, Training: Loss: 0.1374, Accuracy: 0.9167\n",
      "Batch number: 050, Training: Loss: 0.0837, Accuracy: 1.0000\n",
      "Batch number: 051, Training: Loss: 0.5455, Accuracy: 0.7500\n",
      "Batch number: 052, Training: Loss: 0.0717, Accuracy: 0.9583\n",
      "Batch number: 053, Training: Loss: 0.0784, Accuracy: 1.0000\n",
      "Batch number: 054, Training: Loss: 0.0521, Accuracy: 0.9583\n",
      "Batch number: 055, Training: Loss: 0.0316, Accuracy: 1.0000\n",
      "Batch number: 056, Training: Loss: 0.1053, Accuracy: 1.0000\n",
      "Batch number: 057, Training: Loss: 0.0103, Accuracy: 1.0000\n",
      "Batch number: 058, Training: Loss: 0.0565, Accuracy: 0.9583\n",
      "Batch number: 059, Training: Loss: 0.0191, Accuracy: 1.0000\n",
      "Batch number: 060, Training: Loss: 0.0283, Accuracy: 1.0000\n",
      "Batch number: 061, Training: Loss: 0.0781, Accuracy: 0.9583\n",
      "Batch number: 062, Training: Loss: 0.0668, Accuracy: 0.9583\n",
      "Batch number: 063, Training: Loss: 0.0844, Accuracy: 0.9583\n",
      "Batch number: 064, Training: Loss: 0.0805, Accuracy: 0.9583\n",
      "Batch number: 065, Training: Loss: 0.0125, Accuracy: 1.0000\n",
      "Batch number: 066, Training: Loss: 0.1410, Accuracy: 0.9583\n",
      "Batch number: 067, Training: Loss: 0.2508, Accuracy: 0.9167\n",
      "Batch number: 068, Training: Loss: 0.0305, Accuracy: 0.9583\n",
      "Batch number: 069, Training: Loss: 0.0729, Accuracy: 0.9167\n",
      "Batch number: 070, Training: Loss: 0.0865, Accuracy: 0.9583\n",
      "Batch number: 071, Training: Loss: 0.0268, Accuracy: 1.0000\n",
      "Batch number: 072, Training: Loss: 0.1415, Accuracy: 0.9167\n",
      "Batch number: 073, Training: Loss: 0.1046, Accuracy: 0.9583\n",
      "Batch number: 074, Training: Loss: 0.2014, Accuracy: 0.8750\n",
      "Batch number: 075, Training: Loss: 0.2714, Accuracy: 0.8750\n",
      "Batch number: 076, Training: Loss: 0.0297, Accuracy: 1.0000\n",
      "Batch number: 077, Training: Loss: 0.2705, Accuracy: 0.9167\n",
      "Batch number: 078, Training: Loss: 0.1296, Accuracy: 0.9167\n",
      "Batch number: 079, Training: Loss: 0.1894, Accuracy: 0.9167\n",
      "Batch number: 080, Training: Loss: 0.5229, Accuracy: 0.8750\n",
      "Batch number: 081, Training: Loss: 0.3121, Accuracy: 0.8750\n",
      "Batch number: 082, Training: Loss: 0.1338, Accuracy: 0.8750\n",
      "Batch number: 083, Training: Loss: 0.1353, Accuracy: 0.9167\n",
      "Batch number: 084, Training: Loss: 0.1792, Accuracy: 0.9167\n",
      "Batch number: 085, Training: Loss: 0.0782, Accuracy: 0.9583\n",
      "Batch number: 086, Training: Loss: 0.1969, Accuracy: 0.8750\n",
      "Batch number: 087, Training: Loss: 0.3304, Accuracy: 0.7917\n",
      "Batch number: 088, Training: Loss: 0.4448, Accuracy: 0.7917\n",
      "Batch number: 089, Training: Loss: 0.0523, Accuracy: 1.0000\n",
      "Batch number: 090, Training: Loss: 0.3220, Accuracy: 0.8750\n",
      "Batch number: 091, Training: Loss: 0.1397, Accuracy: 0.9167\n",
      "Batch number: 092, Training: Loss: 0.1579, Accuracy: 0.8750\n",
      "Batch number: 093, Training: Loss: 0.0538, Accuracy: 1.0000\n",
      "Batch number: 094, Training: Loss: 0.0326, Accuracy: 1.0000\n",
      "Batch number: 095, Training: Loss: 0.1972, Accuracy: 0.9167\n",
      "Batch number: 096, Training: Loss: 0.0856, Accuracy: 1.0000\n",
      "Batch number: 097, Training: Loss: 0.2230, Accuracy: 0.9167\n",
      "Batch number: 098, Training: Loss: 0.0825, Accuracy: 1.0000\n",
      "Batch number: 099, Training: Loss: 0.2441, Accuracy: 0.8750\n",
      "Batch number: 100, Training: Loss: 0.1063, Accuracy: 0.9167\n",
      "Batch number: 101, Training: Loss: 0.1037, Accuracy: 1.0000\n",
      "Batch number: 102, Training: Loss: 0.1309, Accuracy: 1.0000\n",
      "Batch number: 103, Training: Loss: 0.1079, Accuracy: 1.0000\n",
      "Batch number: 104, Training: Loss: 0.1666, Accuracy: 0.9167\n",
      "Batch number: 105, Training: Loss: 0.0753, Accuracy: 1.0000\n",
      "Batch number: 106, Training: Loss: 0.0787, Accuracy: 1.0000\n",
      "Batch number: 107, Training: Loss: 0.2303, Accuracy: 0.8750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number: 108, Training: Loss: 0.1450, Accuracy: 0.9583\n",
      "Batch number: 109, Training: Loss: 0.1380, Accuracy: 0.9583\n",
      "Batch number: 110, Training: Loss: 0.6616, Accuracy: 0.8333\n",
      "Batch number: 111, Training: Loss: 0.2396, Accuracy: 0.9167\n",
      "Batch number: 112, Training: Loss: 0.0629, Accuracy: 1.0000\n",
      "Batch number: 113, Training: Loss: 0.2739, Accuracy: 0.8333\n",
      "Batch number: 114, Training: Loss: 0.1601, Accuracy: 0.8750\n",
      "Batch number: 115, Training: Loss: 0.1109, Accuracy: 1.0000\n",
      "Batch number: 116, Training: Loss: 0.0850, Accuracy: 0.9583\n",
      "Batch number: 117, Training: Loss: 0.2938, Accuracy: 0.7917\n",
      "Batch number: 118, Training: Loss: 0.2331, Accuracy: 0.8750\n",
      "Batch number: 119, Training: Loss: 0.1208, Accuracy: 0.8750\n",
      "Batch number: 120, Training: Loss: 0.1393, Accuracy: 0.9583\n",
      "Batch number: 121, Training: Loss: 0.1029, Accuracy: 0.9583\n",
      "Batch number: 122, Training: Loss: 0.2624, Accuracy: 0.8333\n",
      "Batch number: 123, Training: Loss: 0.0875, Accuracy: 1.0000\n",
      "Batch number: 124, Training: Loss: 0.1324, Accuracy: 0.9167\n",
      "Batch number: 125, Training: Loss: 0.1279, Accuracy: 0.9583\n",
      "Batch number: 126, Training: Loss: 0.1839, Accuracy: 0.9583\n",
      "Batch number: 127, Training: Loss: 0.1709, Accuracy: 0.9167\n",
      "Batch number: 128, Training: Loss: 0.1099, Accuracy: 1.0000\n",
      "Batch number: 129, Training: Loss: 0.0887, Accuracy: 0.9583\n",
      "Batch number: 130, Training: Loss: 0.2627, Accuracy: 0.8750\n",
      "Batch number: 131, Training: Loss: 0.1072, Accuracy: 1.0000\n",
      "Batch number: 132, Training: Loss: 0.4593, Accuracy: 0.7500\n",
      "Batch number: 133, Training: Loss: 0.1849, Accuracy: 0.8750\n",
      "Batch number: 134, Training: Loss: 0.0422, Accuracy: 1.0000\n",
      "Batch number: 135, Training: Loss: 0.3245, Accuracy: 0.9167\n",
      "Batch number: 136, Training: Loss: 0.0415, Accuracy: 1.0000\n",
      "Batch number: 137, Training: Loss: 0.0839, Accuracy: 0.9583\n",
      "Batch number: 138, Training: Loss: 0.2473, Accuracy: 0.8750\n",
      "Batch number: 139, Training: Loss: 0.2639, Accuracy: 0.9167\n",
      "Batch number: 140, Training: Loss: 0.0447, Accuracy: 1.0000\n",
      "Batch number: 141, Training: Loss: 0.1264, Accuracy: 0.9583\n",
      "Batch number: 142, Training: Loss: 0.0713, Accuracy: 1.0000\n",
      "Batch number: 143, Training: Loss: 0.0542, Accuracy: 1.0000\n",
      "Batch number: 144, Training: Loss: 0.2200, Accuracy: 0.8750\n",
      "Batch number: 145, Training: Loss: 0.1785, Accuracy: 0.9167\n",
      "Batch number: 146, Training: Loss: 0.0993, Accuracy: 0.9583\n",
      "Batch number: 147, Training: Loss: 0.0818, Accuracy: 1.0000\n",
      "Batch number: 148, Training: Loss: 0.0339, Accuracy: 1.0000\n",
      "Batch number: 149, Training: Loss: 0.3723, Accuracy: 0.9167\n",
      "Batch number: 150, Training: Loss: 0.0417, Accuracy: 1.0000\n",
      "Batch number: 151, Training: Loss: 0.0413, Accuracy: 1.0000\n",
      "Batch number: 152, Training: Loss: 0.2041, Accuracy: 0.9583\n",
      "Batch number: 153, Training: Loss: 0.1653, Accuracy: 0.9167\n",
      "Batch number: 154, Training: Loss: 0.0962, Accuracy: 0.9583\n",
      "Batch number: 155, Training: Loss: 0.0687, Accuracy: 0.9583\n",
      "Batch number: 156, Training: Loss: 0.1817, Accuracy: 0.8750\n",
      "Batch number: 157, Training: Loss: 0.5170, Accuracy: 0.7917\n",
      "Batch number: 158, Training: Loss: 0.1783, Accuracy: 0.9167\n",
      "Batch number: 159, Training: Loss: 0.0696, Accuracy: 0.9583\n",
      "Batch number: 160, Training: Loss: 0.1833, Accuracy: 0.9167\n",
      "Batch number: 161, Training: Loss: 0.1506, Accuracy: 0.8750\n",
      "Batch number: 162, Training: Loss: 0.2551, Accuracy: 0.8750\n",
      "Batch number: 163, Training: Loss: 0.0353, Accuracy: 1.0000\n",
      "Batch number: 164, Training: Loss: 0.0655, Accuracy: 1.0000\n",
      "Batch number: 165, Training: Loss: 0.0972, Accuracy: 0.9583\n",
      "Batch number: 166, Training: Loss: 0.0766, Accuracy: 0.9583\n",
      "Batch number: 167, Training: Loss: 0.2886, Accuracy: 0.8750\n",
      "Batch number: 168, Training: Loss: 0.0707, Accuracy: 1.0000\n",
      "Batch number: 169, Training: Loss: 0.0714, Accuracy: 0.9583\n",
      "Batch number: 170, Training: Loss: 0.0773, Accuracy: 1.0000\n",
      "Batch number: 171, Training: Loss: 0.2061, Accuracy: 0.9167\n",
      "Batch number: 172, Training: Loss: 0.0224, Accuracy: 1.0000\n",
      "Batch number: 173, Training: Loss: 0.5453, Accuracy: 0.9167\n",
      "Batch number: 174, Training: Loss: 0.2120, Accuracy: 0.9167\n",
      "Batch number: 175, Training: Loss: 0.0307, Accuracy: 1.0000\n",
      "Batch number: 176, Training: Loss: 0.0995, Accuracy: 0.9583\n",
      "Batch number: 177, Training: Loss: 0.1614, Accuracy: 0.8750\n",
      "Batch number: 178, Training: Loss: 0.0881, Accuracy: 0.9167\n",
      "Batch number: 179, Training: Loss: 0.2955, Accuracy: 0.9167\n",
      "Batch number: 180, Training: Loss: 0.0923, Accuracy: 0.9583\n",
      "Batch number: 181, Training: Loss: 0.1805, Accuracy: 0.9167\n",
      "Batch number: 182, Training: Loss: 0.0687, Accuracy: 1.0000\n",
      "Batch number: 183, Training: Loss: 0.0593, Accuracy: 1.0000\n",
      "Batch number: 184, Training: Loss: 0.0508, Accuracy: 1.0000\n",
      "Batch number: 185, Training: Loss: 0.1557, Accuracy: 0.8750\n",
      "Batch number: 186, Training: Loss: 0.5004, Accuracy: 0.9167\n",
      "Batch number: 187, Training: Loss: 0.1702, Accuracy: 0.9167\n",
      "Batch number: 188, Training: Loss: 0.1381, Accuracy: 1.0000\n",
      "Batch number: 189, Training: Loss: 0.0215, Accuracy: 1.0000\n",
      "Batch number: 190, Training: Loss: 0.1168, Accuracy: 0.9583\n",
      "Batch number: 191, Training: Loss: 0.0530, Accuracy: 1.0000\n",
      "Epoch: 10/150\n",
      "Batch number: 000, Training: Loss: 0.2426, Accuracy: 0.9167\n",
      "Batch number: 001, Training: Loss: 0.1929, Accuracy: 0.9167\n",
      "Batch number: 002, Training: Loss: 0.0356, Accuracy: 1.0000\n",
      "Batch number: 003, Training: Loss: 0.2449, Accuracy: 0.9167\n",
      "Batch number: 004, Training: Loss: 0.1760, Accuracy: 0.8750\n",
      "Batch number: 005, Training: Loss: 0.4311, Accuracy: 0.9167\n",
      "Batch number: 006, Training: Loss: 0.0356, Accuracy: 1.0000\n",
      "Batch number: 007, Training: Loss: 0.0498, Accuracy: 1.0000\n",
      "Batch number: 008, Training: Loss: 0.0475, Accuracy: 1.0000\n",
      "Batch number: 009, Training: Loss: 0.2902, Accuracy: 0.9167\n",
      "Batch number: 010, Training: Loss: 0.0474, Accuracy: 1.0000\n",
      "Batch number: 011, Training: Loss: 0.3082, Accuracy: 0.8750\n",
      "Batch number: 012, Training: Loss: 0.1718, Accuracy: 0.9583\n",
      "Batch number: 013, Training: Loss: 0.1317, Accuracy: 0.9583\n",
      "Batch number: 014, Training: Loss: 0.0826, Accuracy: 1.0000\n",
      "Batch number: 015, Training: Loss: 0.1617, Accuracy: 0.9583\n",
      "Batch number: 016, Training: Loss: 0.0541, Accuracy: 0.9583\n",
      "Batch number: 017, Training: Loss: 0.0923, Accuracy: 0.9583\n",
      "Batch number: 018, Training: Loss: 0.2223, Accuracy: 0.8333\n",
      "Batch number: 019, Training: Loss: 0.1128, Accuracy: 0.9167\n",
      "Batch number: 020, Training: Loss: 0.0307, Accuracy: 1.0000\n",
      "Batch number: 021, Training: Loss: 0.1029, Accuracy: 0.9583\n",
      "Batch number: 022, Training: Loss: 0.1550, Accuracy: 0.9167\n",
      "Batch number: 023, Training: Loss: 0.2331, Accuracy: 0.9167\n",
      "Batch number: 024, Training: Loss: 0.0906, Accuracy: 1.0000\n",
      "Batch number: 025, Training: Loss: 0.1674, Accuracy: 0.9167\n",
      "Batch number: 026, Training: Loss: 0.0974, Accuracy: 0.9583\n",
      "Batch number: 027, Training: Loss: 0.2854, Accuracy: 0.8750\n",
      "Batch number: 028, Training: Loss: 0.1577, Accuracy: 0.9167\n",
      "Batch number: 029, Training: Loss: 0.2179, Accuracy: 0.8750\n",
      "Batch number: 030, Training: Loss: 0.5290, Accuracy: 0.7917\n",
      "Batch number: 031, Training: Loss: 0.0708, Accuracy: 0.9583\n",
      "Batch number: 032, Training: Loss: 0.1126, Accuracy: 0.9583\n",
      "Batch number: 033, Training: Loss: 0.0657, Accuracy: 1.0000\n",
      "Batch number: 034, Training: Loss: 0.1122, Accuracy: 0.9583\n",
      "Batch number: 035, Training: Loss: 0.0883, Accuracy: 0.9583\n",
      "Batch number: 036, Training: Loss: 0.2533, Accuracy: 0.9167\n",
      "Batch number: 037, Training: Loss: 0.0572, Accuracy: 1.0000\n",
      "Batch number: 038, Training: Loss: 0.0199, Accuracy: 1.0000\n",
      "Batch number: 039, Training: Loss: 0.0699, Accuracy: 1.0000\n",
      "Batch number: 040, Training: Loss: 0.5278, Accuracy: 0.8750\n",
      "Batch number: 041, Training: Loss: 0.2177, Accuracy: 0.9167\n",
      "Batch number: 042, Training: Loss: 0.0645, Accuracy: 0.9583\n",
      "Batch number: 043, Training: Loss: 0.1293, Accuracy: 0.9583\n",
      "Batch number: 044, Training: Loss: 0.1202, Accuracy: 0.9167\n",
      "Batch number: 045, Training: Loss: 0.1454, Accuracy: 0.9583\n",
      "Batch number: 046, Training: Loss: 0.0748, Accuracy: 1.0000\n",
      "Batch number: 047, Training: Loss: 0.0657, Accuracy: 1.0000\n",
      "Batch number: 048, Training: Loss: 0.2204, Accuracy: 0.9167\n",
      "Batch number: 049, Training: Loss: 0.0813, Accuracy: 0.9583\n",
      "Batch number: 050, Training: Loss: 0.1131, Accuracy: 0.9583\n",
      "Batch number: 051, Training: Loss: 0.1539, Accuracy: 0.9167\n",
      "Batch number: 052, Training: Loss: 0.2845, Accuracy: 0.8750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number: 053, Training: Loss: 0.0668, Accuracy: 0.9583\n",
      "Batch number: 054, Training: Loss: 0.0662, Accuracy: 1.0000\n",
      "Batch number: 055, Training: Loss: 0.1367, Accuracy: 0.9583\n",
      "Batch number: 056, Training: Loss: 0.1594, Accuracy: 0.8750\n",
      "Batch number: 057, Training: Loss: 0.2053, Accuracy: 0.8750\n",
      "Batch number: 058, Training: Loss: 0.2405, Accuracy: 0.8750\n",
      "Batch number: 059, Training: Loss: 0.1906, Accuracy: 0.9583\n",
      "Batch number: 060, Training: Loss: 0.1136, Accuracy: 0.9583\n",
      "Batch number: 061, Training: Loss: 0.1049, Accuracy: 0.9583\n",
      "Batch number: 062, Training: Loss: 0.0407, Accuracy: 1.0000\n",
      "Batch number: 063, Training: Loss: 0.0352, Accuracy: 1.0000\n",
      "Batch number: 064, Training: Loss: 0.0411, Accuracy: 1.0000\n",
      "Batch number: 065, Training: Loss: 0.1821, Accuracy: 0.9583\n",
      "Batch number: 066, Training: Loss: 0.3785, Accuracy: 0.8333\n",
      "Batch number: 067, Training: Loss: 0.0549, Accuracy: 1.0000\n",
      "Batch number: 068, Training: Loss: 0.0684, Accuracy: 1.0000\n",
      "Batch number: 069, Training: Loss: 0.1642, Accuracy: 0.9167\n",
      "Batch number: 070, Training: Loss: 0.0561, Accuracy: 0.9583\n",
      "Batch number: 071, Training: Loss: 0.2047, Accuracy: 0.8750\n",
      "Batch number: 072, Training: Loss: 0.5397, Accuracy: 0.8750\n",
      "Batch number: 073, Training: Loss: 0.2051, Accuracy: 0.8750\n",
      "Batch number: 074, Training: Loss: 0.1745, Accuracy: 0.8750\n",
      "Batch number: 075, Training: Loss: 0.0445, Accuracy: 1.0000\n",
      "Batch number: 076, Training: Loss: 0.0235, Accuracy: 1.0000\n",
      "Batch number: 077, Training: Loss: 0.1096, Accuracy: 0.9167\n",
      "Batch number: 078, Training: Loss: 0.2139, Accuracy: 0.8750\n",
      "Batch number: 079, Training: Loss: 0.4847, Accuracy: 0.7917\n",
      "Batch number: 080, Training: Loss: 0.1200, Accuracy: 0.9167\n",
      "Batch number: 081, Training: Loss: 0.3072, Accuracy: 0.8333\n",
      "Batch number: 082, Training: Loss: 0.1154, Accuracy: 0.9583\n",
      "Batch number: 083, Training: Loss: 0.0674, Accuracy: 1.0000\n",
      "Batch number: 084, Training: Loss: 0.2567, Accuracy: 0.8333\n",
      "Batch number: 085, Training: Loss: 0.1164, Accuracy: 1.0000\n",
      "Batch number: 086, Training: Loss: 0.1620, Accuracy: 0.9583\n",
      "Batch number: 087, Training: Loss: 0.2191, Accuracy: 0.9167\n",
      "Batch number: 088, Training: Loss: 0.2718, Accuracy: 0.8750\n",
      "Batch number: 089, Training: Loss: 0.2790, Accuracy: 0.8750\n",
      "Batch number: 090, Training: Loss: 0.1241, Accuracy: 0.9583\n",
      "Batch number: 091, Training: Loss: 0.0784, Accuracy: 0.9583\n",
      "Batch number: 092, Training: Loss: 0.1274, Accuracy: 0.9583\n",
      "Batch number: 093, Training: Loss: 0.3488, Accuracy: 0.9167\n",
      "Batch number: 094, Training: Loss: 0.1309, Accuracy: 0.9583\n",
      "Batch number: 095, Training: Loss: 0.0694, Accuracy: 1.0000\n",
      "Batch number: 096, Training: Loss: 0.1128, Accuracy: 1.0000\n",
      "Batch number: 097, Training: Loss: 0.0856, Accuracy: 1.0000\n",
      "Batch number: 098, Training: Loss: 0.1682, Accuracy: 0.9167\n",
      "Batch number: 099, Training: Loss: 0.0652, Accuracy: 1.0000\n",
      "Batch number: 100, Training: Loss: 0.0896, Accuracy: 1.0000\n",
      "Batch number: 101, Training: Loss: 0.1509, Accuracy: 0.9167\n",
      "Batch number: 102, Training: Loss: 0.0925, Accuracy: 0.9583\n",
      "Batch number: 103, Training: Loss: 0.0644, Accuracy: 0.9583\n",
      "Batch number: 104, Training: Loss: 0.1003, Accuracy: 1.0000\n",
      "Batch number: 105, Training: Loss: 0.1118, Accuracy: 0.8750\n",
      "Batch number: 106, Training: Loss: 0.0662, Accuracy: 0.9583\n",
      "Batch number: 107, Training: Loss: 0.3175, Accuracy: 0.9167\n",
      "Batch number: 108, Training: Loss: 0.0314, Accuracy: 1.0000\n",
      "Batch number: 109, Training: Loss: 0.0560, Accuracy: 1.0000\n",
      "Batch number: 110, Training: Loss: 0.0986, Accuracy: 0.9583\n",
      "Batch number: 111, Training: Loss: 0.2099, Accuracy: 0.9167\n",
      "Batch number: 112, Training: Loss: 0.0945, Accuracy: 0.9167\n",
      "Batch number: 113, Training: Loss: 0.3787, Accuracy: 0.9167\n",
      "Batch number: 114, Training: Loss: 0.1427, Accuracy: 0.9583\n",
      "Batch number: 115, Training: Loss: 0.2113, Accuracy: 0.9583\n",
      "Batch number: 116, Training: Loss: 0.0367, Accuracy: 1.0000\n",
      "Batch number: 117, Training: Loss: 0.1750, Accuracy: 0.8750\n",
      "Batch number: 118, Training: Loss: 0.1789, Accuracy: 0.8750\n",
      "Batch number: 119, Training: Loss: 0.2015, Accuracy: 0.8750\n",
      "Batch number: 120, Training: Loss: 0.0837, Accuracy: 0.9583\n",
      "Batch number: 121, Training: Loss: 0.1597, Accuracy: 1.0000\n",
      "Batch number: 122, Training: Loss: 0.0696, Accuracy: 0.9167\n",
      "Batch number: 123, Training: Loss: 0.2399, Accuracy: 0.8750\n",
      "Batch number: 124, Training: Loss: 0.1268, Accuracy: 0.9583\n",
      "Batch number: 125, Training: Loss: 0.1525, Accuracy: 0.9167\n",
      "Batch number: 126, Training: Loss: 0.0539, Accuracy: 0.9583\n",
      "Batch number: 127, Training: Loss: 0.2150, Accuracy: 0.8750\n",
      "Batch number: 128, Training: Loss: 0.1624, Accuracy: 0.9167\n",
      "Batch number: 129, Training: Loss: 0.0483, Accuracy: 0.9583\n",
      "Batch number: 130, Training: Loss: 0.0654, Accuracy: 0.9583\n",
      "Batch number: 131, Training: Loss: 0.0662, Accuracy: 0.9583\n",
      "Batch number: 132, Training: Loss: 0.1530, Accuracy: 0.9167\n",
      "Batch number: 133, Training: Loss: 0.1435, Accuracy: 0.9583\n",
      "Batch number: 134, Training: Loss: 0.0904, Accuracy: 0.9583\n",
      "Batch number: 135, Training: Loss: 0.0607, Accuracy: 1.0000\n",
      "Batch number: 136, Training: Loss: 0.0742, Accuracy: 0.9583\n",
      "Batch number: 137, Training: Loss: 0.1516, Accuracy: 0.9583\n",
      "Batch number: 138, Training: Loss: 0.0275, Accuracy: 1.0000\n",
      "Batch number: 139, Training: Loss: 0.3998, Accuracy: 0.8750\n",
      "Batch number: 140, Training: Loss: 0.1504, Accuracy: 0.8750\n",
      "Batch number: 141, Training: Loss: 0.0645, Accuracy: 1.0000\n",
      "Batch number: 142, Training: Loss: 0.3893, Accuracy: 0.8750\n",
      "Batch number: 143, Training: Loss: 0.1908, Accuracy: 0.8750\n",
      "Batch number: 144, Training: Loss: 0.0416, Accuracy: 0.9583\n",
      "Batch number: 145, Training: Loss: 0.0537, Accuracy: 1.0000\n",
      "Batch number: 146, Training: Loss: 0.0215, Accuracy: 1.0000\n",
      "Batch number: 147, Training: Loss: 0.0473, Accuracy: 1.0000\n",
      "Batch number: 148, Training: Loss: 0.1232, Accuracy: 0.9583\n",
      "Batch number: 149, Training: Loss: 0.1485, Accuracy: 0.9583\n",
      "Batch number: 150, Training: Loss: 0.0381, Accuracy: 1.0000\n",
      "Batch number: 151, Training: Loss: 0.0779, Accuracy: 0.9583\n",
      "Batch number: 152, Training: Loss: 0.1719, Accuracy: 0.9167\n",
      "Batch number: 153, Training: Loss: 0.0780, Accuracy: 1.0000\n",
      "Batch number: 154, Training: Loss: 0.0404, Accuracy: 1.0000\n",
      "Batch number: 155, Training: Loss: 0.1016, Accuracy: 0.9583\n",
      "Batch number: 156, Training: Loss: 0.0402, Accuracy: 1.0000\n",
      "Batch number: 157, Training: Loss: 0.1298, Accuracy: 0.9583\n",
      "Batch number: 158, Training: Loss: 0.1780, Accuracy: 0.9167\n",
      "Batch number: 159, Training: Loss: 0.0709, Accuracy: 0.9583\n",
      "Batch number: 160, Training: Loss: 0.3970, Accuracy: 0.8750\n",
      "Batch number: 161, Training: Loss: 0.1347, Accuracy: 0.9583\n",
      "Batch number: 162, Training: Loss: 0.0345, Accuracy: 1.0000\n",
      "Batch number: 163, Training: Loss: 0.1841, Accuracy: 0.9583\n",
      "Batch number: 164, Training: Loss: 0.0529, Accuracy: 1.0000\n",
      "Batch number: 165, Training: Loss: 0.2042, Accuracy: 0.9167\n",
      "Batch number: 166, Training: Loss: 0.1477, Accuracy: 0.8750\n",
      "Batch number: 167, Training: Loss: 0.0437, Accuracy: 1.0000\n",
      "Batch number: 168, Training: Loss: 0.0439, Accuracy: 1.0000\n",
      "Batch number: 169, Training: Loss: 0.1440, Accuracy: 0.9583\n",
      "Batch number: 170, Training: Loss: 0.2592, Accuracy: 0.9167\n",
      "Batch number: 171, Training: Loss: 0.4162, Accuracy: 0.9583\n",
      "Batch number: 172, Training: Loss: 0.0722, Accuracy: 0.9583\n",
      "Batch number: 173, Training: Loss: 0.1789, Accuracy: 0.9583\n",
      "Batch number: 174, Training: Loss: 0.3105, Accuracy: 0.8750\n",
      "Batch number: 175, Training: Loss: 0.2801, Accuracy: 0.9167\n",
      "Batch number: 176, Training: Loss: 0.0637, Accuracy: 1.0000\n",
      "Batch number: 177, Training: Loss: 0.1536, Accuracy: 0.8750\n",
      "Batch number: 178, Training: Loss: 0.1531, Accuracy: 0.9583\n",
      "Batch number: 179, Training: Loss: 0.0715, Accuracy: 0.9583\n",
      "Batch number: 180, Training: Loss: 0.0700, Accuracy: 1.0000\n",
      "Batch number: 181, Training: Loss: 0.1238, Accuracy: 0.9583\n",
      "Batch number: 182, Training: Loss: 0.4261, Accuracy: 0.7917\n",
      "Batch number: 183, Training: Loss: 0.2644, Accuracy: 0.7917\n",
      "Batch number: 184, Training: Loss: 0.1750, Accuracy: 0.9583\n",
      "Batch number: 185, Training: Loss: 0.1982, Accuracy: 0.8750\n",
      "Batch number: 186, Training: Loss: 0.1480, Accuracy: 0.9167\n",
      "Batch number: 187, Training: Loss: 0.2358, Accuracy: 0.9167\n",
      "Batch number: 188, Training: Loss: 0.0574, Accuracy: 1.0000\n",
      "Batch number: 189, Training: Loss: 0.2752, Accuracy: 0.9167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number: 190, Training: Loss: 0.1336, Accuracy: 0.9167\n",
      "Batch number: 191, Training: Loss: 0.2769, Accuracy: 0.9474\n",
      "Epoch: 11/150\n",
      "Batch number: 000, Training: Loss: 0.0895, Accuracy: 0.9583\n",
      "Batch number: 001, Training: Loss: 0.1244, Accuracy: 0.9583\n",
      "Batch number: 002, Training: Loss: 0.1199, Accuracy: 0.8750\n",
      "Batch number: 003, Training: Loss: 0.1038, Accuracy: 0.9167\n",
      "Batch number: 004, Training: Loss: 0.0576, Accuracy: 1.0000\n",
      "Batch number: 005, Training: Loss: 0.1376, Accuracy: 0.9583\n",
      "Batch number: 006, Training: Loss: 0.1335, Accuracy: 0.9167\n",
      "Batch number: 007, Training: Loss: 0.1864, Accuracy: 0.9167\n",
      "Batch number: 008, Training: Loss: 0.0681, Accuracy: 0.9167\n",
      "Batch number: 009, Training: Loss: 0.0892, Accuracy: 0.9583\n",
      "Batch number: 010, Training: Loss: 0.1667, Accuracy: 0.9167\n",
      "Batch number: 011, Training: Loss: 0.0243, Accuracy: 1.0000\n",
      "Batch number: 012, Training: Loss: 0.3204, Accuracy: 0.8333\n",
      "Batch number: 013, Training: Loss: 0.6643, Accuracy: 0.8333\n",
      "Batch number: 014, Training: Loss: 0.0638, Accuracy: 1.0000\n",
      "Batch number: 015, Training: Loss: 0.0757, Accuracy: 1.0000\n",
      "Batch number: 016, Training: Loss: 0.1371, Accuracy: 0.9583\n",
      "Batch number: 017, Training: Loss: 0.1152, Accuracy: 1.0000\n",
      "Batch number: 018, Training: Loss: 0.1788, Accuracy: 0.9167\n",
      "Batch number: 019, Training: Loss: 0.1330, Accuracy: 0.9167\n",
      "Batch number: 020, Training: Loss: 0.0923, Accuracy: 0.9583\n",
      "Batch number: 021, Training: Loss: 0.2703, Accuracy: 0.9167\n",
      "Batch number: 022, Training: Loss: 0.0925, Accuracy: 0.9167\n",
      "Batch number: 023, Training: Loss: 0.0210, Accuracy: 1.0000\n",
      "Batch number: 024, Training: Loss: 0.0870, Accuracy: 1.0000\n",
      "Batch number: 025, Training: Loss: 0.0877, Accuracy: 1.0000\n",
      "Batch number: 026, Training: Loss: 0.2300, Accuracy: 0.9167\n",
      "Batch number: 027, Training: Loss: 0.4086, Accuracy: 0.8333\n",
      "Batch number: 028, Training: Loss: 0.0610, Accuracy: 1.0000\n",
      "Batch number: 029, Training: Loss: 0.2343, Accuracy: 0.9583\n",
      "Batch number: 030, Training: Loss: 0.3058, Accuracy: 0.9583\n",
      "Batch number: 031, Training: Loss: 0.2252, Accuracy: 0.9583\n",
      "Batch number: 032, Training: Loss: 0.1561, Accuracy: 0.9167\n",
      "Batch number: 033, Training: Loss: 0.1094, Accuracy: 0.9583\n",
      "Batch number: 034, Training: Loss: 0.0526, Accuracy: 0.9583\n",
      "Batch number: 035, Training: Loss: 0.0325, Accuracy: 1.0000\n",
      "Batch number: 036, Training: Loss: 0.1208, Accuracy: 0.8750\n",
      "Batch number: 037, Training: Loss: 0.4254, Accuracy: 0.8333\n",
      "Batch number: 038, Training: Loss: 0.5075, Accuracy: 0.7917\n",
      "Batch number: 039, Training: Loss: 0.0253, Accuracy: 1.0000\n",
      "Batch number: 040, Training: Loss: 0.0273, Accuracy: 1.0000\n",
      "Batch number: 041, Training: Loss: 0.1478, Accuracy: 0.9167\n",
      "Batch number: 042, Training: Loss: 0.1148, Accuracy: 0.9583\n",
      "Batch number: 043, Training: Loss: 0.1264, Accuracy: 0.9583\n",
      "Batch number: 044, Training: Loss: 0.1193, Accuracy: 0.9583\n",
      "Batch number: 045, Training: Loss: 0.1054, Accuracy: 0.9583\n",
      "Batch number: 046, Training: Loss: 0.4742, Accuracy: 0.9167\n",
      "Batch number: 047, Training: Loss: 0.1353, Accuracy: 0.9583\n",
      "Batch number: 048, Training: Loss: 0.1896, Accuracy: 0.9167\n",
      "Batch number: 049, Training: Loss: 0.1294, Accuracy: 0.9583\n",
      "Batch number: 050, Training: Loss: 0.1460, Accuracy: 0.9583\n",
      "Batch number: 051, Training: Loss: 0.0685, Accuracy: 1.0000\n",
      "Batch number: 052, Training: Loss: 0.0349, Accuracy: 1.0000\n",
      "Batch number: 053, Training: Loss: 0.1300, Accuracy: 0.9167\n",
      "Batch number: 054, Training: Loss: 0.2455, Accuracy: 0.8750\n",
      "Batch number: 055, Training: Loss: 0.1884, Accuracy: 0.9167\n",
      "Batch number: 056, Training: Loss: 0.1063, Accuracy: 0.9167\n",
      "Batch number: 057, Training: Loss: 0.2046, Accuracy: 0.9583\n",
      "Batch number: 058, Training: Loss: 0.0464, Accuracy: 1.0000\n",
      "Batch number: 059, Training: Loss: 0.0904, Accuracy: 1.0000\n",
      "Batch number: 060, Training: Loss: 0.1514, Accuracy: 0.9167\n",
      "Batch number: 061, Training: Loss: 0.2309, Accuracy: 0.9167\n",
      "Batch number: 062, Training: Loss: 0.0562, Accuracy: 1.0000\n",
      "Batch number: 063, Training: Loss: 0.1426, Accuracy: 0.9167\n",
      "Batch number: 064, Training: Loss: 0.0412, Accuracy: 1.0000\n",
      "Batch number: 065, Training: Loss: 0.0956, Accuracy: 0.9583\n",
      "Batch number: 066, Training: Loss: 0.6780, Accuracy: 0.7083\n",
      "Batch number: 067, Training: Loss: 0.0706, Accuracy: 1.0000\n",
      "Batch number: 068, Training: Loss: 0.0445, Accuracy: 1.0000\n",
      "Batch number: 069, Training: Loss: 0.1119, Accuracy: 0.9167\n",
      "Batch number: 070, Training: Loss: 0.1010, Accuracy: 1.0000\n",
      "Batch number: 071, Training: Loss: 0.1226, Accuracy: 0.9583\n",
      "Batch number: 072, Training: Loss: 0.0562, Accuracy: 0.9583\n",
      "Batch number: 073, Training: Loss: 0.0542, Accuracy: 1.0000\n",
      "Batch number: 074, Training: Loss: 0.1412, Accuracy: 0.9583\n",
      "Batch number: 075, Training: Loss: 0.0533, Accuracy: 1.0000\n",
      "Batch number: 076, Training: Loss: 0.2260, Accuracy: 0.8333\n",
      "Batch number: 077, Training: Loss: 0.0825, Accuracy: 0.9583\n",
      "Batch number: 078, Training: Loss: 0.0552, Accuracy: 1.0000\n",
      "Batch number: 079, Training: Loss: 0.1322, Accuracy: 0.9583\n",
      "Batch number: 080, Training: Loss: 0.0810, Accuracy: 1.0000\n",
      "Batch number: 081, Training: Loss: 0.2329, Accuracy: 0.9167\n",
      "Batch number: 082, Training: Loss: 0.1315, Accuracy: 0.9167\n",
      "Batch number: 083, Training: Loss: 0.0765, Accuracy: 0.9583\n",
      "Batch number: 084, Training: Loss: 0.1092, Accuracy: 0.9583\n",
      "Batch number: 085, Training: Loss: 0.4708, Accuracy: 0.7083\n",
      "Batch number: 086, Training: Loss: 0.1300, Accuracy: 0.9583\n",
      "Batch number: 087, Training: Loss: 0.0980, Accuracy: 0.9583\n",
      "Batch number: 088, Training: Loss: 0.1626, Accuracy: 0.9167\n",
      "Batch number: 089, Training: Loss: 0.4589, Accuracy: 0.7917\n",
      "Batch number: 090, Training: Loss: 0.0472, Accuracy: 1.0000\n",
      "Batch number: 091, Training: Loss: 0.2552, Accuracy: 0.8750\n",
      "Batch number: 092, Training: Loss: 0.1807, Accuracy: 0.9167\n",
      "Batch number: 093, Training: Loss: 0.2821, Accuracy: 0.8750\n",
      "Batch number: 094, Training: Loss: 0.0298, Accuracy: 1.0000\n",
      "Batch number: 095, Training: Loss: 0.6521, Accuracy: 0.8333\n",
      "Batch number: 096, Training: Loss: 0.7976, Accuracy: 0.8750\n",
      "Batch number: 097, Training: Loss: 0.1203, Accuracy: 0.9583\n",
      "Batch number: 098, Training: Loss: 0.1319, Accuracy: 0.9583\n",
      "Batch number: 099, Training: Loss: 0.0994, Accuracy: 0.9583\n",
      "Batch number: 100, Training: Loss: 0.1234, Accuracy: 0.9167\n",
      "Batch number: 101, Training: Loss: 0.0900, Accuracy: 1.0000\n",
      "Batch number: 102, Training: Loss: 0.0988, Accuracy: 1.0000\n",
      "Batch number: 103, Training: Loss: 0.1672, Accuracy: 1.0000\n",
      "Batch number: 104, Training: Loss: 0.1703, Accuracy: 0.9583\n",
      "Batch number: 105, Training: Loss: 0.1399, Accuracy: 1.0000\n",
      "Batch number: 106, Training: Loss: 0.2524, Accuracy: 0.8750\n",
      "Batch number: 107, Training: Loss: 0.2055, Accuracy: 0.8750\n",
      "Batch number: 108, Training: Loss: 0.1512, Accuracy: 0.9167\n",
      "Batch number: 109, Training: Loss: 0.0996, Accuracy: 0.9583\n",
      "Batch number: 110, Training: Loss: 0.2423, Accuracy: 0.9167\n",
      "Batch number: 111, Training: Loss: 0.3215, Accuracy: 0.7917\n",
      "Batch number: 112, Training: Loss: 0.1589, Accuracy: 0.9583\n",
      "Batch number: 113, Training: Loss: 0.2128, Accuracy: 0.9167\n",
      "Batch number: 114, Training: Loss: 0.2883, Accuracy: 0.7917\n",
      "Batch number: 115, Training: Loss: 0.1164, Accuracy: 1.0000\n",
      "Batch number: 116, Training: Loss: 0.1528, Accuracy: 0.9167\n",
      "Batch number: 117, Training: Loss: 0.1460, Accuracy: 0.9583\n",
      "Batch number: 118, Training: Loss: 0.0803, Accuracy: 1.0000\n",
      "Batch number: 119, Training: Loss: 0.0767, Accuracy: 0.9583\n",
      "Batch number: 120, Training: Loss: 0.0897, Accuracy: 1.0000\n",
      "Batch number: 121, Training: Loss: 0.3792, Accuracy: 0.7917\n",
      "Batch number: 122, Training: Loss: 0.1264, Accuracy: 0.9167\n",
      "Batch number: 123, Training: Loss: 0.0913, Accuracy: 1.0000\n",
      "Batch number: 124, Training: Loss: 0.1457, Accuracy: 0.9583\n",
      "Batch number: 125, Training: Loss: 0.1083, Accuracy: 0.9167\n",
      "Batch number: 126, Training: Loss: 0.0736, Accuracy: 0.9583\n",
      "Batch number: 127, Training: Loss: 0.1606, Accuracy: 0.9167\n",
      "Batch number: 128, Training: Loss: 0.2708, Accuracy: 0.8333\n",
      "Batch number: 129, Training: Loss: 0.3859, Accuracy: 0.7917\n",
      "Batch number: 130, Training: Loss: 0.0586, Accuracy: 0.9167\n",
      "Batch number: 131, Training: Loss: 0.0464, Accuracy: 1.0000\n",
      "Batch number: 132, Training: Loss: 0.1206, Accuracy: 0.9583\n",
      "Batch number: 133, Training: Loss: 0.1573, Accuracy: 0.9167\n",
      "Batch number: 134, Training: Loss: 0.0896, Accuracy: 0.9583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number: 135, Training: Loss: 0.0295, Accuracy: 1.0000\n",
      "Batch number: 136, Training: Loss: 0.0612, Accuracy: 1.0000\n",
      "Batch number: 137, Training: Loss: 0.0875, Accuracy: 0.9583\n",
      "Batch number: 138, Training: Loss: 0.0852, Accuracy: 1.0000\n",
      "Batch number: 139, Training: Loss: 0.1261, Accuracy: 0.9167\n",
      "Batch number: 140, Training: Loss: 0.1160, Accuracy: 0.9583\n",
      "Batch number: 141, Training: Loss: 0.0636, Accuracy: 0.9583\n",
      "Batch number: 142, Training: Loss: 0.0352, Accuracy: 1.0000\n",
      "Batch number: 143, Training: Loss: 0.2967, Accuracy: 0.8750\n",
      "Batch number: 144, Training: Loss: 0.0579, Accuracy: 1.0000\n",
      "Batch number: 145, Training: Loss: 0.0785, Accuracy: 0.9583\n",
      "Batch number: 146, Training: Loss: 0.0715, Accuracy: 1.0000\n",
      "Batch number: 147, Training: Loss: 0.0227, Accuracy: 1.0000\n",
      "Batch number: 148, Training: Loss: 0.2048, Accuracy: 0.9167\n",
      "Batch number: 149, Training: Loss: 0.0821, Accuracy: 0.9583\n",
      "Batch number: 150, Training: Loss: 0.0531, Accuracy: 0.9583\n",
      "Batch number: 151, Training: Loss: 0.0407, Accuracy: 1.0000\n",
      "Batch number: 152, Training: Loss: 0.0052, Accuracy: 1.0000\n",
      "Batch number: 153, Training: Loss: 0.0333, Accuracy: 1.0000\n",
      "Batch number: 154, Training: Loss: 0.0968, Accuracy: 1.0000\n",
      "Batch number: 155, Training: Loss: 0.0299, Accuracy: 1.0000\n",
      "Batch number: 156, Training: Loss: 0.1936, Accuracy: 0.9167\n",
      "Batch number: 157, Training: Loss: 0.4989, Accuracy: 0.8750\n",
      "Batch number: 158, Training: Loss: 0.2440, Accuracy: 0.9167\n",
      "Batch number: 159, Training: Loss: 0.0172, Accuracy: 1.0000\n",
      "Batch number: 160, Training: Loss: 0.0588, Accuracy: 0.9583\n",
      "Batch number: 161, Training: Loss: 0.5001, Accuracy: 0.7917\n",
      "Batch number: 162, Training: Loss: 0.0778, Accuracy: 0.9167\n",
      "Batch number: 163, Training: Loss: 0.0606, Accuracy: 0.9583\n",
      "Batch number: 164, Training: Loss: 0.5342, Accuracy: 0.7917\n",
      "Batch number: 165, Training: Loss: 0.2026, Accuracy: 0.8333\n",
      "Batch number: 166, Training: Loss: 0.0968, Accuracy: 0.9583\n",
      "Batch number: 167, Training: Loss: 0.0628, Accuracy: 1.0000\n",
      "Batch number: 168, Training: Loss: 0.0422, Accuracy: 1.0000\n",
      "Batch number: 169, Training: Loss: 0.1098, Accuracy: 0.9583\n",
      "Batch number: 170, Training: Loss: 0.0421, Accuracy: 1.0000\n",
      "Batch number: 171, Training: Loss: 0.0697, Accuracy: 0.9583\n",
      "Batch number: 172, Training: Loss: 0.0171, Accuracy: 1.0000\n",
      "Batch number: 173, Training: Loss: 0.1347, Accuracy: 1.0000\n",
      "Batch number: 174, Training: Loss: 0.0807, Accuracy: 0.9583\n",
      "Batch number: 175, Training: Loss: 0.0545, Accuracy: 1.0000\n",
      "Batch number: 176, Training: Loss: 0.1513, Accuracy: 0.9167\n",
      "Batch number: 177, Training: Loss: 0.1615, Accuracy: 0.9583\n",
      "Batch number: 178, Training: Loss: 0.1439, Accuracy: 0.8750\n",
      "Batch number: 179, Training: Loss: 0.1477, Accuracy: 0.9583\n",
      "Batch number: 180, Training: Loss: 0.0492, Accuracy: 1.0000\n",
      "Batch number: 181, Training: Loss: 0.1084, Accuracy: 0.9167\n",
      "Batch number: 182, Training: Loss: 0.0660, Accuracy: 0.9583\n",
      "Batch number: 183, Training: Loss: 0.1253, Accuracy: 0.9167\n",
      "Batch number: 184, Training: Loss: 0.0194, Accuracy: 1.0000\n",
      "Batch number: 185, Training: Loss: 0.0810, Accuracy: 0.9167\n",
      "Batch number: 186, Training: Loss: 0.4896, Accuracy: 0.9167\n",
      "Batch number: 187, Training: Loss: 0.0624, Accuracy: 1.0000\n",
      "Batch number: 188, Training: Loss: 0.1630, Accuracy: 0.9167\n",
      "Batch number: 189, Training: Loss: 0.1092, Accuracy: 0.9583\n",
      "Batch number: 190, Training: Loss: 0.4445, Accuracy: 0.9167\n",
      "Batch number: 191, Training: Loss: 0.1475, Accuracy: 0.9474\n",
      "Epoch: 12/150\n",
      "Batch number: 000, Training: Loss: 0.3660, Accuracy: 0.9583\n",
      "Batch number: 001, Training: Loss: 0.0262, Accuracy: 1.0000\n",
      "Batch number: 002, Training: Loss: 0.1019, Accuracy: 1.0000\n",
      "Batch number: 003, Training: Loss: 0.2102, Accuracy: 0.9583\n",
      "Batch number: 004, Training: Loss: 0.1156, Accuracy: 0.9167\n",
      "Batch number: 005, Training: Loss: 0.1352, Accuracy: 0.9167\n",
      "Batch number: 006, Training: Loss: 0.1123, Accuracy: 0.9583\n",
      "Batch number: 007, Training: Loss: 0.1367, Accuracy: 0.9167\n",
      "Batch number: 008, Training: Loss: 0.2149, Accuracy: 0.8333\n",
      "Batch number: 009, Training: Loss: 0.0596, Accuracy: 0.9583\n",
      "Batch number: 010, Training: Loss: 0.0832, Accuracy: 1.0000\n",
      "Batch number: 011, Training: Loss: 0.0989, Accuracy: 0.9583\n",
      "Batch number: 012, Training: Loss: 0.0382, Accuracy: 1.0000\n",
      "Batch number: 013, Training: Loss: 0.0690, Accuracy: 0.9583\n",
      "Batch number: 014, Training: Loss: 0.0450, Accuracy: 1.0000\n",
      "Batch number: 015, Training: Loss: 0.1555, Accuracy: 0.9167\n",
      "Batch number: 016, Training: Loss: 0.0597, Accuracy: 0.9583\n",
      "Batch number: 017, Training: Loss: 0.0411, Accuracy: 1.0000\n",
      "Batch number: 018, Training: Loss: 0.0146, Accuracy: 1.0000\n",
      "Batch number: 019, Training: Loss: 0.1948, Accuracy: 0.8750\n",
      "Batch number: 020, Training: Loss: 0.2622, Accuracy: 0.8333\n",
      "Batch number: 021, Training: Loss: 0.0071, Accuracy: 1.0000\n",
      "Batch number: 022, Training: Loss: 0.0135, Accuracy: 1.0000\n",
      "Batch number: 023, Training: Loss: 0.1265, Accuracy: 0.9583\n",
      "Batch number: 024, Training: Loss: 0.2859, Accuracy: 0.8750\n",
      "Batch number: 025, Training: Loss: 0.3963, Accuracy: 0.9167\n",
      "Batch number: 026, Training: Loss: 0.1652, Accuracy: 0.9167\n",
      "Batch number: 027, Training: Loss: 0.0740, Accuracy: 0.9583\n",
      "Batch number: 028, Training: Loss: 0.1603, Accuracy: 0.8750\n",
      "Batch number: 029, Training: Loss: 0.4367, Accuracy: 0.8333\n",
      "Batch number: 030, Training: Loss: 0.0532, Accuracy: 1.0000\n",
      "Batch number: 031, Training: Loss: 0.0255, Accuracy: 1.0000\n",
      "Batch number: 032, Training: Loss: 0.1462, Accuracy: 0.9583\n",
      "Batch number: 033, Training: Loss: 0.1308, Accuracy: 0.9167\n",
      "Batch number: 034, Training: Loss: 0.1684, Accuracy: 0.9583\n",
      "Batch number: 035, Training: Loss: 0.2351, Accuracy: 0.9167\n",
      "Batch number: 036, Training: Loss: 0.1774, Accuracy: 0.8750\n",
      "Batch number: 037, Training: Loss: 0.0577, Accuracy: 1.0000\n",
      "Batch number: 038, Training: Loss: 0.0482, Accuracy: 1.0000\n",
      "Batch number: 039, Training: Loss: 0.0686, Accuracy: 0.9167\n",
      "Batch number: 040, Training: Loss: 0.1406, Accuracy: 0.9167\n",
      "Batch number: 041, Training: Loss: 0.1340, Accuracy: 0.9167\n",
      "Batch number: 042, Training: Loss: 0.3727, Accuracy: 0.8333\n",
      "Batch number: 043, Training: Loss: 0.0826, Accuracy: 0.9583\n",
      "Batch number: 044, Training: Loss: 0.3375, Accuracy: 0.8750\n",
      "Batch number: 045, Training: Loss: 0.1386, Accuracy: 0.8750\n",
      "Batch number: 046, Training: Loss: 0.1088, Accuracy: 0.9583\n",
      "Batch number: 047, Training: Loss: 0.0814, Accuracy: 0.9583\n",
      "Batch number: 048, Training: Loss: 0.0747, Accuracy: 1.0000\n",
      "Batch number: 049, Training: Loss: 0.0573, Accuracy: 0.9583\n",
      "Batch number: 050, Training: Loss: 0.0651, Accuracy: 1.0000\n",
      "Batch number: 051, Training: Loss: 0.1971, Accuracy: 0.9167\n",
      "Batch number: 052, Training: Loss: 0.1878, Accuracy: 0.9167\n",
      "Batch number: 053, Training: Loss: 0.1229, Accuracy: 0.9167\n",
      "Batch number: 054, Training: Loss: 0.1058, Accuracy: 0.9583\n",
      "Batch number: 055, Training: Loss: 0.0968, Accuracy: 0.9583\n",
      "Batch number: 056, Training: Loss: 0.0846, Accuracy: 0.9167\n",
      "Batch number: 057, Training: Loss: 0.1740, Accuracy: 0.9583\n",
      "Batch number: 058, Training: Loss: 0.1213, Accuracy: 0.9167\n",
      "Batch number: 059, Training: Loss: 0.1050, Accuracy: 0.9583\n",
      "Batch number: 060, Training: Loss: 0.2106, Accuracy: 0.9583\n",
      "Batch number: 061, Training: Loss: 0.1094, Accuracy: 0.9583\n",
      "Batch number: 062, Training: Loss: 0.2065, Accuracy: 0.9167\n",
      "Batch number: 063, Training: Loss: 0.1858, Accuracy: 0.9167\n",
      "Batch number: 064, Training: Loss: 0.1841, Accuracy: 0.9167\n",
      "Batch number: 065, Training: Loss: 0.1474, Accuracy: 0.9167\n",
      "Batch number: 066, Training: Loss: 0.1156, Accuracy: 0.9167\n",
      "Batch number: 067, Training: Loss: 0.0452, Accuracy: 1.0000\n",
      "Batch number: 068, Training: Loss: 0.1885, Accuracy: 0.8750\n",
      "Batch number: 069, Training: Loss: 0.1424, Accuracy: 0.9583\n",
      "Batch number: 070, Training: Loss: 0.2111, Accuracy: 0.9167\n",
      "Batch number: 071, Training: Loss: 0.1470, Accuracy: 0.9583\n",
      "Batch number: 072, Training: Loss: 0.1006, Accuracy: 1.0000\n",
      "Batch number: 073, Training: Loss: 0.0424, Accuracy: 1.0000\n",
      "Batch number: 074, Training: Loss: 0.1895, Accuracy: 0.9167\n",
      "Batch number: 075, Training: Loss: 0.0519, Accuracy: 1.0000\n",
      "Batch number: 076, Training: Loss: 0.0521, Accuracy: 0.9583\n",
      "Batch number: 077, Training: Loss: 0.3530, Accuracy: 0.8750\n",
      "Batch number: 078, Training: Loss: 0.0371, Accuracy: 1.0000\n",
      "Batch number: 079, Training: Loss: 0.0612, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number: 080, Training: Loss: 0.1557, Accuracy: 0.9583\n",
      "Batch number: 081, Training: Loss: 0.2591, Accuracy: 0.9167\n",
      "Batch number: 082, Training: Loss: 0.2868, Accuracy: 0.8750\n",
      "Batch number: 083, Training: Loss: 0.1651, Accuracy: 0.9167\n",
      "Batch number: 084, Training: Loss: 0.1013, Accuracy: 0.9583\n",
      "Batch number: 085, Training: Loss: 0.0606, Accuracy: 0.9583\n",
      "Batch number: 086, Training: Loss: 0.0580, Accuracy: 1.0000\n",
      "Batch number: 087, Training: Loss: 0.1135, Accuracy: 0.9167\n",
      "Batch number: 088, Training: Loss: 0.0753, Accuracy: 0.9583\n",
      "Batch number: 089, Training: Loss: 0.2304, Accuracy: 0.8750\n",
      "Batch number: 090, Training: Loss: 0.0115, Accuracy: 1.0000\n",
      "Batch number: 091, Training: Loss: 0.1843, Accuracy: 0.9167\n",
      "Batch number: 092, Training: Loss: 0.1852, Accuracy: 0.9167\n",
      "Batch number: 093, Training: Loss: 0.0647, Accuracy: 0.9583\n",
      "Batch number: 094, Training: Loss: 0.0306, Accuracy: 1.0000\n",
      "Batch number: 095, Training: Loss: 0.4230, Accuracy: 0.8333\n",
      "Batch number: 096, Training: Loss: 0.1568, Accuracy: 0.9167\n",
      "Batch number: 097, Training: Loss: 0.0561, Accuracy: 1.0000\n",
      "Batch number: 098, Training: Loss: 0.1840, Accuracy: 0.9167\n",
      "Batch number: 099, Training: Loss: 0.1423, Accuracy: 0.9583\n",
      "Batch number: 100, Training: Loss: 0.1444, Accuracy: 0.9583\n",
      "Batch number: 101, Training: Loss: 0.1414, Accuracy: 0.9167\n",
      "Batch number: 102, Training: Loss: 0.0804, Accuracy: 1.0000\n",
      "Batch number: 103, Training: Loss: 0.2716, Accuracy: 0.9167\n",
      "Batch number: 104, Training: Loss: 0.0677, Accuracy: 0.9583\n",
      "Batch number: 105, Training: Loss: 0.2085, Accuracy: 0.9583\n",
      "Batch number: 106, Training: Loss: 0.0950, Accuracy: 1.0000\n",
      "Batch number: 107, Training: Loss: 0.0666, Accuracy: 1.0000\n",
      "Batch number: 108, Training: Loss: 0.1520, Accuracy: 0.9167\n",
      "Batch number: 109, Training: Loss: 0.2355, Accuracy: 0.9167\n",
      "Batch number: 110, Training: Loss: 0.0774, Accuracy: 1.0000\n",
      "Batch number: 111, Training: Loss: 0.0802, Accuracy: 0.9583\n",
      "Batch number: 112, Training: Loss: 0.0389, Accuracy: 1.0000\n",
      "Batch number: 113, Training: Loss: 0.1974, Accuracy: 0.9167\n",
      "Batch number: 114, Training: Loss: 0.2031, Accuracy: 0.9583\n",
      "Batch number: 115, Training: Loss: 0.1983, Accuracy: 0.9167\n",
      "Batch number: 116, Training: Loss: 0.3744, Accuracy: 0.8750\n",
      "Batch number: 117, Training: Loss: 0.3321, Accuracy: 0.7917\n",
      "Batch number: 118, Training: Loss: 0.0671, Accuracy: 1.0000\n",
      "Batch number: 119, Training: Loss: 0.1288, Accuracy: 0.9167\n",
      "Batch number: 120, Training: Loss: 0.1480, Accuracy: 0.9167\n",
      "Batch number: 121, Training: Loss: 0.0894, Accuracy: 0.9583\n",
      "Batch number: 122, Training: Loss: 0.0890, Accuracy: 0.9583\n",
      "Batch number: 123, Training: Loss: 0.1246, Accuracy: 0.9583\n",
      "Batch number: 124, Training: Loss: 0.0186, Accuracy: 1.0000\n",
      "Batch number: 125, Training: Loss: 0.2120, Accuracy: 0.9583\n",
      "Batch number: 126, Training: Loss: 0.0813, Accuracy: 1.0000\n",
      "Batch number: 127, Training: Loss: 0.0712, Accuracy: 0.9583\n",
      "Batch number: 128, Training: Loss: 0.2075, Accuracy: 0.9167\n",
      "Batch number: 129, Training: Loss: 0.4282, Accuracy: 0.8750\n",
      "Batch number: 130, Training: Loss: 0.0942, Accuracy: 0.9167\n",
      "Batch number: 131, Training: Loss: 0.0240, Accuracy: 1.0000\n",
      "Batch number: 132, Training: Loss: 0.0735, Accuracy: 1.0000\n",
      "Batch number: 133, Training: Loss: 0.0538, Accuracy: 0.9167\n",
      "Batch number: 134, Training: Loss: 0.0856, Accuracy: 0.9583\n",
      "Batch number: 135, Training: Loss: 0.1242, Accuracy: 0.9583\n",
      "Batch number: 136, Training: Loss: 0.0372, Accuracy: 0.9583\n",
      "Batch number: 137, Training: Loss: 0.1739, Accuracy: 0.8333\n",
      "Batch number: 138, Training: Loss: 0.3184, Accuracy: 0.8333\n",
      "Batch number: 139, Training: Loss: 0.1976, Accuracy: 0.8333\n",
      "Batch number: 140, Training: Loss: 0.1937, Accuracy: 0.9583\n",
      "Batch number: 141, Training: Loss: 0.0948, Accuracy: 1.0000\n",
      "Batch number: 142, Training: Loss: 0.0891, Accuracy: 0.9583\n",
      "Batch number: 143, Training: Loss: 0.1907, Accuracy: 0.9583\n",
      "Batch number: 144, Training: Loss: 0.0872, Accuracy: 1.0000\n",
      "Batch number: 145, Training: Loss: 0.0864, Accuracy: 0.9583\n",
      "Batch number: 146, Training: Loss: 0.1640, Accuracy: 0.8750\n",
      "Batch number: 147, Training: Loss: 0.0623, Accuracy: 0.9583\n",
      "Batch number: 148, Training: Loss: 0.1722, Accuracy: 0.9583\n",
      "Batch number: 149, Training: Loss: 0.0793, Accuracy: 1.0000\n",
      "Batch number: 150, Training: Loss: 0.0441, Accuracy: 1.0000\n",
      "Batch number: 151, Training: Loss: 0.1376, Accuracy: 0.9583\n",
      "Batch number: 152, Training: Loss: 0.2583, Accuracy: 0.8750\n",
      "Batch number: 153, Training: Loss: 0.0484, Accuracy: 0.9583\n",
      "Batch number: 154, Training: Loss: 0.1228, Accuracy: 0.9167\n",
      "Batch number: 155, Training: Loss: 0.0554, Accuracy: 0.9583\n",
      "Batch number: 156, Training: Loss: 0.1722, Accuracy: 0.9583\n",
      "Batch number: 157, Training: Loss: 0.0599, Accuracy: 0.9583\n",
      "Batch number: 158, Training: Loss: 0.0043, Accuracy: 1.0000\n",
      "Batch number: 159, Training: Loss: 0.1885, Accuracy: 0.9583\n",
      "Batch number: 160, Training: Loss: 0.8021, Accuracy: 0.7500\n",
      "Batch number: 161, Training: Loss: 0.0156, Accuracy: 1.0000\n",
      "Batch number: 162, Training: Loss: 0.4225, Accuracy: 0.7917\n",
      "Batch number: 163, Training: Loss: 0.0805, Accuracy: 0.9583\n",
      "Batch number: 164, Training: Loss: 0.1870, Accuracy: 0.8750\n",
      "Batch number: 165, Training: Loss: 0.1303, Accuracy: 0.9167\n",
      "Batch number: 166, Training: Loss: 0.0997, Accuracy: 0.9583\n",
      "Batch number: 167, Training: Loss: 0.1237, Accuracy: 0.9167\n",
      "Batch number: 168, Training: Loss: 0.1003, Accuracy: 0.9583\n",
      "Batch number: 169, Training: Loss: 0.0481, Accuracy: 1.0000\n",
      "Batch number: 170, Training: Loss: 0.0397, Accuracy: 1.0000\n",
      "Batch number: 171, Training: Loss: 0.2472, Accuracy: 0.8750\n",
      "Batch number: 172, Training: Loss: 0.0311, Accuracy: 1.0000\n",
      "Batch number: 173, Training: Loss: 0.0833, Accuracy: 1.0000\n",
      "Batch number: 174, Training: Loss: 0.1599, Accuracy: 0.9583\n",
      "Batch number: 175, Training: Loss: 0.2177, Accuracy: 0.8750\n",
      "Batch number: 176, Training: Loss: 0.2781, Accuracy: 0.9167\n",
      "Batch number: 177, Training: Loss: 0.1196, Accuracy: 0.9583\n",
      "Batch number: 178, Training: Loss: 0.2469, Accuracy: 0.8750\n",
      "Batch number: 179, Training: Loss: 0.0970, Accuracy: 1.0000\n",
      "Batch number: 180, Training: Loss: 0.0244, Accuracy: 1.0000\n",
      "Batch number: 181, Training: Loss: 0.0502, Accuracy: 0.9583\n",
      "Batch number: 182, Training: Loss: 0.1101, Accuracy: 0.9583\n",
      "Batch number: 183, Training: Loss: 0.4825, Accuracy: 0.9167\n",
      "Batch number: 184, Training: Loss: 0.0164, Accuracy: 1.0000\n",
      "Batch number: 185, Training: Loss: 0.0826, Accuracy: 0.9583\n",
      "Batch number: 186, Training: Loss: 0.1521, Accuracy: 0.9167\n",
      "Batch number: 187, Training: Loss: 0.5267, Accuracy: 0.8333\n",
      "Batch number: 188, Training: Loss: 0.1164, Accuracy: 0.9583\n",
      "Batch number: 189, Training: Loss: 0.0356, Accuracy: 1.0000\n",
      "Batch number: 190, Training: Loss: 0.0353, Accuracy: 1.0000\n",
      "Batch number: 191, Training: Loss: 0.0363, Accuracy: 1.0000\n",
      "Epoch: 13/150\n",
      "Batch number: 000, Training: Loss: 0.0520, Accuracy: 1.0000\n",
      "Batch number: 001, Training: Loss: 0.1280, Accuracy: 0.9167\n",
      "Batch number: 002, Training: Loss: 0.1252, Accuracy: 1.0000\n",
      "Batch number: 003, Training: Loss: 0.0536, Accuracy: 1.0000\n",
      "Batch number: 004, Training: Loss: 0.0263, Accuracy: 1.0000\n",
      "Batch number: 005, Training: Loss: 0.1097, Accuracy: 0.9583\n",
      "Batch number: 006, Training: Loss: 0.1455, Accuracy: 0.9583\n",
      "Batch number: 007, Training: Loss: 0.1155, Accuracy: 0.9167\n",
      "Batch number: 008, Training: Loss: 0.0473, Accuracy: 1.0000\n",
      "Batch number: 009, Training: Loss: 0.0467, Accuracy: 1.0000\n",
      "Batch number: 010, Training: Loss: 0.1114, Accuracy: 0.9583\n",
      "Batch number: 011, Training: Loss: 0.3509, Accuracy: 0.8333\n",
      "Batch number: 012, Training: Loss: 0.0890, Accuracy: 0.9583\n",
      "Batch number: 013, Training: Loss: 0.1115, Accuracy: 0.9583\n",
      "Batch number: 014, Training: Loss: 0.0375, Accuracy: 1.0000\n",
      "Batch number: 015, Training: Loss: 0.6130, Accuracy: 0.7500\n",
      "Batch number: 016, Training: Loss: 0.1476, Accuracy: 0.9583\n",
      "Batch number: 017, Training: Loss: 0.3805, Accuracy: 0.9167\n",
      "Batch number: 018, Training: Loss: 0.0945, Accuracy: 0.9583\n",
      "Batch number: 019, Training: Loss: 0.0979, Accuracy: 0.9583\n",
      "Batch number: 020, Training: Loss: 0.1739, Accuracy: 0.9583\n",
      "Batch number: 021, Training: Loss: 0.2063, Accuracy: 0.9167\n",
      "Batch number: 022, Training: Loss: 0.0876, Accuracy: 0.9583\n",
      "Batch number: 023, Training: Loss: 0.1474, Accuracy: 1.0000\n",
      "Batch number: 024, Training: Loss: 0.0681, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number: 025, Training: Loss: 0.1559, Accuracy: 0.9583\n",
      "Batch number: 026, Training: Loss: 0.0701, Accuracy: 0.9583\n",
      "Batch number: 027, Training: Loss: 0.0744, Accuracy: 1.0000\n",
      "Batch number: 028, Training: Loss: 0.1023, Accuracy: 0.9583\n",
      "Batch number: 029, Training: Loss: 0.2203, Accuracy: 0.9167\n",
      "Batch number: 030, Training: Loss: 0.2240, Accuracy: 0.8750\n",
      "Batch number: 031, Training: Loss: 0.2327, Accuracy: 0.9167\n",
      "Batch number: 032, Training: Loss: 0.2079, Accuracy: 0.9583\n",
      "Batch number: 033, Training: Loss: 0.1291, Accuracy: 0.9583\n",
      "Batch number: 034, Training: Loss: 0.1522, Accuracy: 0.9583\n",
      "Batch number: 035, Training: Loss: 0.1393, Accuracy: 0.9583\n",
      "Batch number: 036, Training: Loss: 0.1371, Accuracy: 0.9583\n",
      "Batch number: 037, Training: Loss: 0.1027, Accuracy: 0.9583\n",
      "Batch number: 038, Training: Loss: 0.1509, Accuracy: 0.9167\n",
      "Batch number: 039, Training: Loss: 0.0364, Accuracy: 1.0000\n",
      "Batch number: 040, Training: Loss: 0.2479, Accuracy: 0.8750\n",
      "Batch number: 041, Training: Loss: 0.1585, Accuracy: 0.9583\n",
      "Batch number: 042, Training: Loss: 0.1139, Accuracy: 0.9583\n",
      "Batch number: 043, Training: Loss: 0.0774, Accuracy: 0.9583\n",
      "Batch number: 044, Training: Loss: 0.0826, Accuracy: 0.9583\n",
      "Batch number: 045, Training: Loss: 0.0303, Accuracy: 1.0000\n",
      "Batch number: 046, Training: Loss: 0.0625, Accuracy: 0.9583\n",
      "Batch number: 047, Training: Loss: 0.0872, Accuracy: 0.9583\n",
      "Batch number: 048, Training: Loss: 0.0982, Accuracy: 0.9583\n",
      "Batch number: 049, Training: Loss: 0.0567, Accuracy: 0.9583\n",
      "Batch number: 050, Training: Loss: 0.0381, Accuracy: 1.0000\n",
      "Batch number: 051, Training: Loss: 0.1382, Accuracy: 0.9167\n",
      "Batch number: 052, Training: Loss: 0.4991, Accuracy: 0.9583\n",
      "Batch number: 053, Training: Loss: 0.0568, Accuracy: 1.0000\n",
      "Batch number: 054, Training: Loss: 0.0084, Accuracy: 1.0000\n",
      "Batch number: 055, Training: Loss: 0.2513, Accuracy: 0.9167\n",
      "Batch number: 056, Training: Loss: 0.8850, Accuracy: 0.8333\n",
      "Batch number: 057, Training: Loss: 0.1581, Accuracy: 0.9167\n",
      "Batch number: 058, Training: Loss: 0.0378, Accuracy: 1.0000\n",
      "Batch number: 059, Training: Loss: 0.1115, Accuracy: 0.9583\n",
      "Batch number: 060, Training: Loss: 0.1548, Accuracy: 1.0000\n",
      "Batch number: 061, Training: Loss: 0.1348, Accuracy: 0.9167\n",
      "Batch number: 062, Training: Loss: 0.3149, Accuracy: 0.9167\n",
      "Batch number: 063, Training: Loss: 0.0130, Accuracy: 1.0000\n",
      "Batch number: 064, Training: Loss: 0.1579, Accuracy: 0.9583\n",
      "Batch number: 065, Training: Loss: 0.0499, Accuracy: 1.0000\n",
      "Batch number: 066, Training: Loss: 0.0969, Accuracy: 0.9583\n",
      "Batch number: 067, Training: Loss: 0.1555, Accuracy: 0.9167\n",
      "Batch number: 068, Training: Loss: 0.0916, Accuracy: 0.9583\n",
      "Batch number: 069, Training: Loss: 0.0878, Accuracy: 0.9583\n",
      "Batch number: 070, Training: Loss: 0.1091, Accuracy: 0.9583\n",
      "Batch number: 071, Training: Loss: 0.0167, Accuracy: 1.0000\n",
      "Batch number: 072, Training: Loss: 0.1804, Accuracy: 0.9167\n",
      "Batch number: 073, Training: Loss: 0.0575, Accuracy: 0.9583\n",
      "Batch number: 074, Training: Loss: 0.0752, Accuracy: 0.9583\n",
      "Batch number: 075, Training: Loss: 0.1542, Accuracy: 0.9167\n",
      "Batch number: 076, Training: Loss: 0.1272, Accuracy: 0.9583\n",
      "Batch number: 077, Training: Loss: 0.0817, Accuracy: 1.0000\n",
      "Batch number: 078, Training: Loss: 0.0473, Accuracy: 1.0000\n",
      "Batch number: 079, Training: Loss: 0.1321, Accuracy: 0.9167\n",
      "Batch number: 080, Training: Loss: 0.2883, Accuracy: 0.8750\n",
      "Batch number: 081, Training: Loss: 0.1349, Accuracy: 0.9583\n",
      "Batch number: 082, Training: Loss: 0.0276, Accuracy: 1.0000\n",
      "Batch number: 083, Training: Loss: 0.0666, Accuracy: 1.0000\n",
      "Batch number: 084, Training: Loss: 0.1205, Accuracy: 0.9167\n",
      "Batch number: 085, Training: Loss: 0.0736, Accuracy: 0.9583\n",
      "Batch number: 086, Training: Loss: 0.1749, Accuracy: 0.9167\n",
      "Batch number: 087, Training: Loss: 0.1028, Accuracy: 0.9583\n",
      "Batch number: 088, Training: Loss: 0.1527, Accuracy: 0.9167\n",
      "Batch number: 089, Training: Loss: 0.2933, Accuracy: 0.8750\n",
      "Batch number: 090, Training: Loss: 0.1956, Accuracy: 0.8750\n",
      "Batch number: 091, Training: Loss: 0.0546, Accuracy: 0.9583\n",
      "Batch number: 092, Training: Loss: 0.2788, Accuracy: 0.8333\n",
      "Batch number: 093, Training: Loss: 0.1168, Accuracy: 0.9583\n",
      "Batch number: 094, Training: Loss: 0.2602, Accuracy: 0.8750\n",
      "Batch number: 095, Training: Loss: 0.1847, Accuracy: 0.9583\n",
      "Batch number: 096, Training: Loss: 0.0860, Accuracy: 0.9583\n",
      "Batch number: 097, Training: Loss: 0.0477, Accuracy: 1.0000\n",
      "Batch number: 098, Training: Loss: 0.1458, Accuracy: 0.9583\n",
      "Batch number: 099, Training: Loss: 0.2869, Accuracy: 0.9167\n",
      "Batch number: 100, Training: Loss: 0.1852, Accuracy: 0.9583\n",
      "Batch number: 101, Training: Loss: 0.1854, Accuracy: 0.9583\n",
      "Batch number: 102, Training: Loss: 0.1968, Accuracy: 0.9583\n",
      "Batch number: 103, Training: Loss: 0.0671, Accuracy: 1.0000\n",
      "Batch number: 104, Training: Loss: 0.3190, Accuracy: 0.7917\n",
      "Batch number: 105, Training: Loss: 0.2542, Accuracy: 0.8750\n",
      "Batch number: 106, Training: Loss: 0.0870, Accuracy: 1.0000\n",
      "Batch number: 107, Training: Loss: 0.0689, Accuracy: 1.0000\n",
      "Batch number: 108, Training: Loss: 0.2530, Accuracy: 0.8750\n",
      "Batch number: 109, Training: Loss: 0.0488, Accuracy: 0.9583\n",
      "Batch number: 110, Training: Loss: 0.3587, Accuracy: 0.7083\n",
      "Batch number: 111, Training: Loss: 0.0704, Accuracy: 0.9583\n",
      "Batch number: 112, Training: Loss: 0.1692, Accuracy: 0.9167\n",
      "Batch number: 113, Training: Loss: 0.0782, Accuracy: 0.9583\n",
      "Batch number: 114, Training: Loss: 0.0533, Accuracy: 1.0000\n",
      "Batch number: 115, Training: Loss: 0.2686, Accuracy: 0.8750\n",
      "Batch number: 116, Training: Loss: 0.3423, Accuracy: 0.8750\n",
      "Batch number: 117, Training: Loss: 0.0646, Accuracy: 1.0000\n",
      "Batch number: 118, Training: Loss: 0.1181, Accuracy: 0.9583\n",
      "Batch number: 119, Training: Loss: 0.0823, Accuracy: 0.9583\n",
      "Batch number: 120, Training: Loss: 0.0610, Accuracy: 0.9583\n",
      "Batch number: 121, Training: Loss: 0.1472, Accuracy: 0.9167\n",
      "Batch number: 122, Training: Loss: 0.1861, Accuracy: 0.9167\n",
      "Batch number: 123, Training: Loss: 0.0774, Accuracy: 1.0000\n",
      "Batch number: 124, Training: Loss: 0.1007, Accuracy: 0.9167\n",
      "Batch number: 125, Training: Loss: 0.0509, Accuracy: 1.0000\n",
      "Batch number: 126, Training: Loss: 0.2097, Accuracy: 0.8750\n",
      "Batch number: 127, Training: Loss: 0.1042, Accuracy: 0.9583\n",
      "Batch number: 128, Training: Loss: 0.0905, Accuracy: 0.9583\n",
      "Batch number: 129, Training: Loss: 0.2558, Accuracy: 0.9167\n",
      "Batch number: 130, Training: Loss: 0.1309, Accuracy: 0.9583\n",
      "Batch number: 131, Training: Loss: 0.1705, Accuracy: 0.8750\n",
      "Batch number: 132, Training: Loss: 0.1228, Accuracy: 0.8750\n",
      "Batch number: 133, Training: Loss: 0.0268, Accuracy: 1.0000\n",
      "Batch number: 134, Training: Loss: 0.1620, Accuracy: 0.9167\n",
      "Batch number: 135, Training: Loss: 0.1104, Accuracy: 0.9583\n",
      "Batch number: 136, Training: Loss: 0.1288, Accuracy: 0.9583\n",
      "Batch number: 137, Training: Loss: 0.0441, Accuracy: 1.0000\n",
      "Batch number: 138, Training: Loss: 0.1040, Accuracy: 1.0000\n",
      "Batch number: 139, Training: Loss: 0.0389, Accuracy: 1.0000\n",
      "Batch number: 140, Training: Loss: 0.0641, Accuracy: 0.9583\n",
      "Batch number: 141, Training: Loss: 0.1584, Accuracy: 0.9583\n",
      "Batch number: 142, Training: Loss: 0.1213, Accuracy: 0.9167\n",
      "Batch number: 143, Training: Loss: 0.0777, Accuracy: 0.9583\n",
      "Batch number: 144, Training: Loss: 0.1709, Accuracy: 0.9583\n",
      "Batch number: 145, Training: Loss: 0.1077, Accuracy: 1.0000\n",
      "Batch number: 146, Training: Loss: 0.1541, Accuracy: 0.9167\n",
      "Batch number: 147, Training: Loss: 0.1045, Accuracy: 1.0000\n",
      "Batch number: 148, Training: Loss: 0.0079, Accuracy: 1.0000\n",
      "Batch number: 149, Training: Loss: 0.1845, Accuracy: 0.9583\n",
      "Batch number: 150, Training: Loss: 0.1257, Accuracy: 0.9167\n",
      "Batch number: 151, Training: Loss: 0.0693, Accuracy: 0.9583\n",
      "Batch number: 152, Training: Loss: 0.3783, Accuracy: 0.9167\n",
      "Batch number: 153, Training: Loss: 0.0473, Accuracy: 0.9583\n",
      "Batch number: 154, Training: Loss: 0.0441, Accuracy: 1.0000\n",
      "Batch number: 155, Training: Loss: 0.0846, Accuracy: 0.9583\n",
      "Batch number: 156, Training: Loss: 0.0697, Accuracy: 1.0000\n",
      "Batch number: 157, Training: Loss: 0.1717, Accuracy: 0.9167\n",
      "Batch number: 158, Training: Loss: 0.1619, Accuracy: 0.9583\n",
      "Batch number: 159, Training: Loss: 0.1148, Accuracy: 0.9167\n",
      "Batch number: 160, Training: Loss: 0.2236, Accuracy: 0.9167\n",
      "Batch number: 161, Training: Loss: 0.2605, Accuracy: 0.9167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number: 162, Training: Loss: 0.0816, Accuracy: 0.9583\n",
      "Batch number: 163, Training: Loss: 0.1181, Accuracy: 0.9167\n",
      "Batch number: 164, Training: Loss: 0.1109, Accuracy: 0.9167\n",
      "Batch number: 165, Training: Loss: 0.3153, Accuracy: 0.8750\n",
      "Batch number: 166, Training: Loss: 0.0550, Accuracy: 1.0000\n",
      "Batch number: 167, Training: Loss: 0.0710, Accuracy: 1.0000\n",
      "Batch number: 168, Training: Loss: 0.0562, Accuracy: 0.9583\n",
      "Batch number: 169, Training: Loss: 0.1685, Accuracy: 0.9167\n",
      "Batch number: 170, Training: Loss: 0.0841, Accuracy: 0.9583\n",
      "Batch number: 171, Training: Loss: 0.1692, Accuracy: 0.9167\n",
      "Batch number: 172, Training: Loss: 0.0700, Accuracy: 0.9583\n",
      "Batch number: 173, Training: Loss: 0.2958, Accuracy: 0.8333\n",
      "Batch number: 174, Training: Loss: 0.1587, Accuracy: 0.9583\n",
      "Batch number: 175, Training: Loss: 0.1288, Accuracy: 0.9167\n",
      "Batch number: 176, Training: Loss: 0.1845, Accuracy: 0.9167\n",
      "Batch number: 177, Training: Loss: 0.1192, Accuracy: 0.9583\n",
      "Batch number: 178, Training: Loss: 0.0343, Accuracy: 1.0000\n",
      "Batch number: 179, Training: Loss: 0.0871, Accuracy: 1.0000\n",
      "Batch number: 180, Training: Loss: 0.2517, Accuracy: 0.9167\n",
      "Batch number: 181, Training: Loss: 0.1009, Accuracy: 0.9583\n",
      "Batch number: 182, Training: Loss: 0.0155, Accuracy: 1.0000\n",
      "Batch number: 183, Training: Loss: 0.0854, Accuracy: 1.0000\n",
      "Batch number: 184, Training: Loss: 0.0666, Accuracy: 1.0000\n",
      "Batch number: 185, Training: Loss: 0.1607, Accuracy: 0.8750\n",
      "Batch number: 186, Training: Loss: 0.1272, Accuracy: 0.9583\n",
      "Batch number: 187, Training: Loss: 0.0780, Accuracy: 1.0000\n",
      "Batch number: 188, Training: Loss: 0.1802, Accuracy: 0.9167\n",
      "Batch number: 189, Training: Loss: 0.0221, Accuracy: 1.0000\n",
      "Batch number: 190, Training: Loss: 0.1929, Accuracy: 0.9167\n",
      "Batch number: 191, Training: Loss: 0.0935, Accuracy: 0.9474\n",
      "Epoch: 14/150\n",
      "Batch number: 000, Training: Loss: 0.2304, Accuracy: 0.9167\n",
      "Batch number: 001, Training: Loss: 0.0968, Accuracy: 0.9583\n",
      "Batch number: 002, Training: Loss: 0.1361, Accuracy: 0.9583\n",
      "Batch number: 003, Training: Loss: 0.0878, Accuracy: 1.0000\n",
      "Batch number: 004, Training: Loss: 0.0956, Accuracy: 0.9583\n",
      "Batch number: 005, Training: Loss: 0.2387, Accuracy: 0.9167\n",
      "Batch number: 006, Training: Loss: 0.0524, Accuracy: 1.0000\n",
      "Batch number: 007, Training: Loss: 0.0955, Accuracy: 0.9583\n",
      "Batch number: 008, Training: Loss: 0.1803, Accuracy: 0.9167\n",
      "Batch number: 009, Training: Loss: 0.1013, Accuracy: 0.9167\n",
      "Batch number: 010, Training: Loss: 0.0636, Accuracy: 1.0000\n",
      "Batch number: 011, Training: Loss: 0.1931, Accuracy: 0.9167\n",
      "Batch number: 012, Training: Loss: 0.2579, Accuracy: 0.8750\n",
      "Batch number: 013, Training: Loss: 0.1452, Accuracy: 0.9167\n",
      "Batch number: 014, Training: Loss: 0.0457, Accuracy: 1.0000\n",
      "Batch number: 015, Training: Loss: 0.0558, Accuracy: 0.9583\n",
      "Batch number: 016, Training: Loss: 0.1684, Accuracy: 0.8750\n",
      "Batch number: 017, Training: Loss: 0.0809, Accuracy: 0.9167\n",
      "Batch number: 018, Training: Loss: 0.1434, Accuracy: 0.9167\n",
      "Batch number: 019, Training: Loss: 0.2516, Accuracy: 0.9167\n",
      "Batch number: 020, Training: Loss: 0.0197, Accuracy: 1.0000\n",
      "Batch number: 021, Training: Loss: 0.0765, Accuracy: 1.0000\n",
      "Batch number: 022, Training: Loss: 0.3288, Accuracy: 0.9583\n",
      "Batch number: 023, Training: Loss: 0.0957, Accuracy: 0.9583\n",
      "Batch number: 024, Training: Loss: 0.0945, Accuracy: 0.9583\n",
      "Batch number: 025, Training: Loss: 0.1187, Accuracy: 0.9583\n",
      "Batch number: 026, Training: Loss: 0.0195, Accuracy: 1.0000\n",
      "Batch number: 027, Training: Loss: 0.1137, Accuracy: 0.9583\n",
      "Batch number: 028, Training: Loss: 0.1902, Accuracy: 0.9167\n",
      "Batch number: 029, Training: Loss: 0.0091, Accuracy: 1.0000\n",
      "Batch number: 030, Training: Loss: 0.0758, Accuracy: 0.9583\n",
      "Batch number: 031, Training: Loss: 0.0741, Accuracy: 1.0000\n",
      "Batch number: 032, Training: Loss: 0.4608, Accuracy: 0.8750\n",
      "Batch number: 033, Training: Loss: 0.1156, Accuracy: 0.9583\n",
      "Batch number: 034, Training: Loss: 0.3122, Accuracy: 0.8333\n",
      "Batch number: 035, Training: Loss: 0.0709, Accuracy: 1.0000\n",
      "Batch number: 036, Training: Loss: 0.1635, Accuracy: 0.8750\n",
      "Batch number: 037, Training: Loss: 0.0576, Accuracy: 1.0000\n",
      "Batch number: 038, Training: Loss: 0.0709, Accuracy: 1.0000\n",
      "Batch number: 039, Training: Loss: 0.0905, Accuracy: 1.0000\n",
      "Batch number: 040, Training: Loss: 0.1753, Accuracy: 0.9167\n",
      "Batch number: 041, Training: Loss: 0.0983, Accuracy: 0.9583\n",
      "Batch number: 042, Training: Loss: 0.0868, Accuracy: 0.9583\n",
      "Batch number: 043, Training: Loss: 0.2224, Accuracy: 0.9167\n",
      "Batch number: 044, Training: Loss: 0.0889, Accuracy: 0.9583\n",
      "Batch number: 045, Training: Loss: 0.1286, Accuracy: 0.9167\n",
      "Batch number: 046, Training: Loss: 0.0965, Accuracy: 0.9583\n",
      "Batch number: 047, Training: Loss: 0.0239, Accuracy: 1.0000\n",
      "Batch number: 048, Training: Loss: 0.1475, Accuracy: 0.9583\n",
      "Batch number: 049, Training: Loss: 0.0903, Accuracy: 0.9583\n",
      "Batch number: 050, Training: Loss: 0.0833, Accuracy: 0.9583\n",
      "Batch number: 051, Training: Loss: 0.0926, Accuracy: 0.9167\n",
      "Batch number: 052, Training: Loss: 0.1540, Accuracy: 0.8750\n",
      "Batch number: 053, Training: Loss: 0.0385, Accuracy: 1.0000\n",
      "Batch number: 054, Training: Loss: 0.0477, Accuracy: 1.0000\n",
      "Batch number: 055, Training: Loss: 0.3571, Accuracy: 0.8750\n",
      "Batch number: 056, Training: Loss: 0.1513, Accuracy: 0.9583\n",
      "Batch number: 057, Training: Loss: 0.2426, Accuracy: 0.8750\n",
      "Batch number: 058, Training: Loss: 0.0475, Accuracy: 1.0000\n",
      "Batch number: 059, Training: Loss: 0.0813, Accuracy: 0.9583\n",
      "Batch number: 060, Training: Loss: 0.0414, Accuracy: 1.0000\n",
      "Batch number: 061, Training: Loss: 0.0447, Accuracy: 0.9583\n",
      "Batch number: 062, Training: Loss: 0.1469, Accuracy: 0.9583\n",
      "Batch number: 063, Training: Loss: 0.1870, Accuracy: 0.9583\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "training_set = MyDataset(X, y, ds_trans)\n",
    "training_generator = DataLoader(training_set, **params)\n",
    "epochs = 150\n",
    "losses = []\n",
    "accs = []\n",
    "for epoch in range(epochs):\n",
    "    epoch_start = time.time()\n",
    "    print(\"Epoch: {}/{}\".format(epoch+1, epochs))\n",
    "     \n",
    "    # Set to training mode\n",
    "    resnet.train()\n",
    "     \n",
    "    # Loss and Accuracy within the epoch\n",
    "    train_loss = 0.0\n",
    "    train_acc = 0.0\n",
    "     \n",
    "    valid_loss = 0.0\n",
    "    valid_acc = 0.0\n",
    " \n",
    "    for i, (inputs, labels) in enumerate(training_generator):\n",
    " \n",
    "        inputs = inputs.cuda()\n",
    "        labels = labels.cuda()\n",
    "         \n",
    "        # Clean existing gradients\n",
    "        optimizer.zero_grad()\n",
    "         \n",
    "        # Forward pass - compute outputs on input data using the model\n",
    "        outputs = resnet(inputs)\n",
    "#         print(outputs)\n",
    "#         print(torch.max(labels, 1)[1])\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs.float(), labels)\n",
    "         \n",
    "        # Backpropagate the gradients\n",
    "        loss.backward()\n",
    "         \n",
    "        # Update the parameters\n",
    "        optimizer.step()\n",
    "         \n",
    "        # Compute the total loss for the batch and add it to train_loss\n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "         \n",
    "        # Compute the accuracy\n",
    "        ret, predictions = torch.max(outputs.data, 1)\n",
    "        correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
    "         \n",
    "        # Convert correct_counts to float and then compute the mean\n",
    "        acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
    "         \n",
    "        # Compute total accuracy in the whole batch and add to train_acc\n",
    "        train_acc += acc.item() * inputs.size(0)\n",
    "        losses.append(loss.item()) \n",
    "        accs.append(acc.item())\n",
    "        print(\"Batch number: {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}\".format(\n",
    "            i, loss.item(), acc.item())\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 287\n",
    "test_params = {\n",
    "    'batch_size': 16,\n",
    "    'shuffle': False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (15, 7)\n",
    "plt.plot(list(range(len(losses))), losses, label=\"loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (15, 7)\n",
    "l = [np.mean(losses[i*steps:(i+1)*steps]) for i in range(epochs)]\n",
    "len(l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(list(range(len(l))), l, label=\"l\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (15, 7)\n",
    "plt.plot(list(range(len(losses))), accs, label=\"accs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (15, 7)\n",
    "a = [np.mean(accs[i*steps:(i+1)*steps]) for i in range(epochs)]\n",
    "len(a)\n",
    "plt.plot(list(range(len(a))), a, label=\"a\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('./hw4/sample_submission.csv')\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.array(submission.loc[:, 'image_id'])\n",
    "Y_test = np.zeros(X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_trans_test = transforms.Compose([\n",
    "#     torchvision.transforms.Scale(256),\n",
    "                    torchvision.transforms.CenterCrop(224),\n",
    "                    torchvision.transforms.ToTensor()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "test_set = MyDataset(X_test, Y_test, ds_trans_test, datadir='./hw4/test_kaggle')\n",
    "test_generator = DataLoader(test_set, **test_params)\n",
    "res = []\n",
    "for i, (inputs, tt) in enumerate(test_generator):\n",
    "    inputs = inputs.cuda()\n",
    "    outputs = resnet(inputs)\n",
    "    res.append(F.softmax(outputs, dim=1).cpu().data.numpy().argmax(axis=1))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = np.concatenate(res)\n",
    "# res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(res) / res.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['label'] = res\n",
    "submission.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('./hw4/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
